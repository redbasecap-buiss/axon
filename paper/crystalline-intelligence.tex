\documentclass[11pt,twocolumn]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric}
\usepackage[margin=2cm]{geometry}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{subcaption}

\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  frame=single,
  breaklines=true
}

% Title
\title{\textbf{Crystalline Intelligence: A Novel Paradigm for\\Continuous Knowledge Acquisition Without Neural Networks}}

\author{
  Nicola Spieser\textsuperscript{1}\\
  \texttt{nicola@spieser.ch}\\[6pt]
  \textsuperscript{1}Independent Researcher, Winterthur, Switzerland
}

\date{February 2026}

\begin{document}
\maketitle

% ============================================================
\begin{abstract}
We introduce \textbf{Crystalline Intelligence (CI)}, a fundamentally new paradigm for machine intelligence that achieves continuous, autonomous knowledge acquisition without neural networks, gradient descent, or GPU computation. Our reference implementation, \textsc{axon}, constructs a self-growing knowledge graph from unstructured web content through incremental entity extraction, relation mining, and confidence-weighted temporal learning. Unlike Large Language Models (LLMs), which require billions of parameters and substantial compute for training, CI systems operate on commodity hardware with sub-100\,MB memory footprints while continuously expanding their knowledge base. We further present \textsc{Prometheus}, an automated scientific discovery engine built atop the CI framework, capable of identifying structural gaps in knowledge graphs, generating testable hypotheses, and validating them against existing evidence. Experiments on 12 Wikipedia domains demonstrate that axon extracts 9{,}250+ entities with 190 relations from raw HTML, while Prometheus discovers recurring structural patterns across knowledge domains. We argue that CI represents a complementary---and in resource-constrained settings, superior---approach to knowledge representation compared to parametric neural models.
\end{abstract}

\textbf{Keywords:} knowledge graphs, continuous learning, automated discovery, non-neural AI, graph intelligence

% ============================================================
\section{Introduction}
\label{sec:intro}

The dominant paradigm in artificial intelligence relies on parametric models---neural networks trained on massive datasets via stochastic gradient descent \citep{lecun2015deep}. While remarkably effective, this approach has fundamental limitations:

\begin{enumerate}
  \item \textbf{Catastrophic forgetting}: Neural networks struggle to incorporate new knowledge without degrading previously learned representations \citep{kirkpatrick2017overcoming}.
  \item \textbf{Computational cost}: Training and inference require specialized hardware (GPUs/TPUs) with significant energy consumption \citep{strubell2019energy}.
  \item \textbf{Opacity}: Knowledge is encoded in billions of opaque floating-point parameters, making it difficult to inspect, verify, or explain \citep{lipton2018mythos}.
  \item \textbf{Static knowledge}: Once trained, models cannot acquire new knowledge without retraining or fine-tuning.
\end{enumerate}

We propose \textbf{Crystalline Intelligence (CI)}, a paradigm inspired by the distinction between \emph{fluid} and \emph{crystallized} intelligence in cognitive psychology \citep{cattell1963theory}. Just as crystallized intelligence in humans represents accumulated knowledge that grows throughout life, CI systems build structured knowledge representations that:

\begin{itemize}
  \item Grow \emph{continuously} without retraining
  \item Require \emph{minimal compute} (single CPU core, $<$100\,MB RAM)
  \item Store knowledge \emph{explicitly} in inspectable graph structures
  \item Support \emph{temporal dynamics}: confidence reinforcement and forgetting
  \item Enable \emph{automated discovery} through structural analysis
\end{itemize}

\subsection{Contributions}

\begin{enumerate}
  \item We formalize the \textbf{Crystalline Intelligence} paradigm and contrast it with neural approaches (Section~\ref{sec:paradigm}).
  \item We present \textbf{axon}, a reference implementation in Rust comprising 8{,}641 lines of code with 190 tests (Section~\ref{sec:axon}).
  \item We introduce \textbf{Prometheus}, an automated hypothesis generation and validation engine operating over CI knowledge graphs (Section~\ref{sec:prometheus}).
  \item We provide experimental results demonstrating the system's knowledge acquisition capabilities across 12 scientific domains (Section~\ref{sec:experiments}).
\end{enumerate}

% ============================================================
\section{The Crystalline Intelligence Paradigm}
\label{sec:paradigm}

\subsection{Formal Definition}

A Crystalline Intelligence system $\mathcal{C}$ is defined as a tuple:
\begin{equation}
  \mathcal{C} = (G, \mathcal{E}, \mathcal{L}, \mathcal{D}, \mathcal{T})
\end{equation}
where:
\begin{itemize}
  \item $G = (V, E, \phi)$ is a \emph{temporal knowledge graph} with vertices $V$ (entities), edges $E$ (relations), and a confidence function $\phi: V \cup E \rightarrow [0,1] \times \mathbb{R}^+$ mapping each element to a confidence--timestamp pair.
  \item $\mathcal{E}: \text{Text} \rightarrow \{(v, e)\}$ is an \emph{extraction function} that maps unstructured text to sets of entities and relations.
  \item $\mathcal{L}: G \times \{(v,e)\} \rightarrow G'$ is a \emph{learning operator} that integrates new extractions into the graph with confidence updates.
  \item $\mathcal{D}: G \rightarrow G'$ is a \emph{decay operator} modeling temporal forgetting.
  \item $\mathcal{T}: G \rightarrow \mathcal{H}$ is a \emph{theory operator} that generates hypotheses $\mathcal{H}$ from structural graph analysis.
\end{itemize}

\subsection{Confidence Dynamics}

The confidence of an entity or relation $x$ evolves according to:
\begin{equation}
  \phi(x, t+1) = \begin{cases}
    \min(1, \phi(x,t) + \alpha(1-\phi(x,t))) & \text{if } x \text{ observed at } t \\
    \phi(x,t) \cdot e^{-\lambda \Delta t} & \text{otherwise}
  \end{cases}
  \label{eq:confidence}
\end{equation}
where $\alpha \in (0,1)$ is the \emph{reinforcement rate} and $\lambda > 0$ is the \emph{decay constant}. This models two key cognitive phenomena:
\begin{itemize}
  \item \textbf{Reinforcement}: Repeated observation increases confidence with diminishing returns, analogous to spaced repetition in human memory \citep{ebbinghaus1885memory}.
  \item \textbf{Temporal decay}: Unobserved knowledge gradually loses confidence, modeling the forgetting curve.
\end{itemize}

\subsection{Comparison with Neural Paradigms}

\begin{table}[h]
\centering
\caption{Crystalline Intelligence vs.\ Neural Networks}
\label{tab:comparison}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Property} & \textbf{CI} & \textbf{Neural} \\
\midrule
Knowledge storage & Explicit graph & Implicit weights \\
Learning & Incremental & Batch training \\
Forgetting & Controlled decay & Catastrophic \\
Hardware & CPU only & GPU/TPU \\
Memory & $<$100\,MB & 1--100+\,GB \\
Explainability & Full provenance & Opaque \\
New knowledge & Instant & Requires retraining \\
Inference & Graph traversal & Forward pass \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================
\section{System Architecture: axon}
\label{sec:axon}

\textsc{axon} implements the CI paradigm as a single Rust binary with 13 modules (Table~\ref{tab:modules}).

\begin{table}[h]
\centering
\caption{axon module architecture (8{,}641 LOC total)}
\label{tab:modules}
\small
\begin{tabular}{@{}llr@{}}
\toprule
\textbf{Module} & \textbf{Function} & \textbf{LOC} \\
\midrule
\texttt{db.rs} & Knowledge graph (SQLite) & 802 \\
\texttt{nlp.rs} & Multi-language NLP & 1{,}352 \\
\texttt{prometheus.rs} & Discovery engine & 1{,}607 \\
\texttt{plugin.rs} & Domain extractors & 1{,}089 \\
\texttt{embeddings.rs} & HNSW vector index & 892 \\
\texttt{tui.rs} & Terminal UI & 673 \\
\texttt{graph.rs} & Graph algorithms & 513 \\
\texttt{main.rs} & CLI interface & 448 \\
\texttt{server.rs} & REST API & 342 \\
\texttt{export.rs} & Multi-format export & 307 \\
\texttt{crawler.rs} & Web crawler & 234 \\
\texttt{query.rs} & Query engine & 227 \\
\texttt{config.rs} & Configuration & 155 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Knowledge Graph Storage}

The knowledge graph is stored in SQLite with three primary tables:

\begin{lstlisting}[language=SQL]
CREATE TABLE entities (
  id INTEGER PRIMARY KEY,
  name TEXT UNIQUE,
  entity_type TEXT,
  confidence REAL DEFAULT 0.5,
  first_seen TIMESTAMP,
  last_seen TIMESTAMP,
  access_count INTEGER DEFAULT 1
);

CREATE TABLE relations (
  id INTEGER PRIMARY KEY,
  subject_id INTEGER REFERENCES entities,
  predicate TEXT,
  object_id INTEGER REFERENCES entities,
  confidence REAL DEFAULT 0.5,
  source_url TEXT,
  learned_at TIMESTAMP
);

CREATE TABLE facts (
  id INTEGER PRIMARY KEY,
  entity_id INTEGER REFERENCES entities,
  key TEXT, value TEXT,
  confidence REAL,
  source_url TEXT
);
\end{lstlisting}

SQLite was chosen for its zero-configuration deployment, ACID compliance, and proven reliability for datasets up to terabyte scale \citep{sqlite2020}.

\subsection{Multi-Language NLP Pipeline}

The extraction function $\mathcal{E}$ operates as a five-stage pipeline:

\begin{enumerate}
  \item \textbf{Language Detection}: Stopword frequency analysis across English, German, French, Italian, and Spanish corpora.
  \item \textbf{Sentence Segmentation}: Abbreviation-aware boundary detection (handles Dr., Prof., etc.).
  \item \textbf{Entity Extraction}: Pattern-based recognition of:
    \begin{itemize}
      \item Proper nouns (capitalization heuristics)
      \item Dates (ISO, US, EU, German, French formats)
      \item Numbers with units (``750\,GB'', ``3.5\,GHz'')
      \item Currencies (``\$1.2M'', ``\texteuro500'', ``CHF~1'200'')
      \item URLs and email addresses
    \end{itemize}
  \item \textbf{Relation Extraction}: Subject--verb--object patterns including passive voice (``was developed by'') and appositions (``Berlin, the capital of Germany'').
  \item \textbf{Deduplication}: Levenshtein distance-based entity merging ($d < 3$).
\end{enumerate}

\subsection{HNSW Vector Index}

For semantic similarity search, axon implements the Hierarchical Navigable Small World (HNSW) algorithm \citep{malkov2018efficient} from scratch in 892 lines of Rust. Entity representations are computed as TF-IDF vectors over their associated text, enabling $k$-nearest-neighbor queries without external embedding models.

The cosine similarity between entities $a$ and $b$ is:
\begin{equation}
  \text{sim}(a,b) = \frac{\mathbf{v}_a \cdot \mathbf{v}_b}{\|\mathbf{v}_a\| \cdot \|\mathbf{v}_b\|}
\end{equation}
where $\mathbf{v}_x$ is the sparse TF-IDF vector of entity $x$.

% ============================================================
\section{Automated Discovery: Prometheus}
\label{sec:prometheus}

\textsc{Prometheus} implements the theory operator $\mathcal{T}$ through four stages:

\subsection{Pattern Discovery}

\begin{algorithm}[h]
\caption{Structural Pattern Mining}
\label{alg:pattern}
\begin{algorithmic}[1]
\REQUIRE Knowledge graph $G = (V, E)$
\ENSURE Set of patterns $P$
\STATE $P \leftarrow \emptyset$
\FORALL{pairs $(e_1, e_2) \in E \times E$ where $\text{subject}(e_1) = \text{subject}(e_2)$}
  \STATE $m \leftarrow (\text{pred}(e_1), \text{pred}(e_2))$ \COMMENT{predicate motif}
  \STATE $P[m].\text{freq} \leftarrow P[m].\text{freq} + 1$
\ENDFOR
\FORALL{$v_i, v_j \in V$ where $|\text{neighbors}(v_i) \cap \text{neighbors}(v_j)| \geq 2$}
  \IF{$(v_i, v_j) \notin E$ and $(v_j, v_i) \notin E$}
    \STATE $P \leftarrow P \cup \{$\textsc{StructuralHole}$(v_i, v_j)\}$
  \ENDIF
\ENDFOR
\RETURN $P$
\end{algorithmic}
\end{algorithm}

\subsection{Hypothesis Generation}

From detected gaps, Prometheus generates hypotheses of the form:
\begin{equation}
  h = (s, p, o, c, \mathbf{r})
\end{equation}
where $s$ is the subject, $p$ the predicted predicate, $o$ the object, $c \in [0,1]$ the confidence, and $\mathbf{r}$ is the reasoning chain---a sequence of evidence steps leading to the hypothesis.

\subsubsection{Type-Based Gap Hypotheses}

If entities of type $\tau$ commonly exhibit predicate $p$:
\begin{equation}
  \frac{|\{v \in V_\tau : \exists e \in E, \text{pred}(e) = p, \text{subj}(e) = v\}|}{|V_\tau|} > \theta
\end{equation}
then for any $v^* \in V_\tau$ lacking predicate $p$, we generate hypothesis $h = (v^*, p, ?, c)$ where $c$ is proportional to the ratio above.

\subsubsection{Analogy-Based Hypotheses}

Using the HNSW index, if entity $a$ is similar to entity $b$ ($\text{sim}(a,b) > \sigma$), and $b$ has relation $(b, p, o)$ that $a$ lacks, we hypothesize $(a, p, o')$ where $o'$ is the most similar entity to $o$ in $a$'s neighborhood.

\subsection{Meta-Learning}

Prometheus tracks which pattern types lead to confirmed discoveries:
\begin{equation}
  w_p(t+1) = w_p(t) + \eta \cdot (\text{confirmed}_p - \text{rejected}_p)
\end{equation}
This allows the system to focus on the most productive discovery strategies over time, implementing a form of reinforcement learning without neural networks.

% ============================================================
\section{Experiments}
\label{sec:experiments}

\subsection{Setup}

We evaluated axon on 12 Wikipedia articles spanning diverse scientific domains (Table~\ref{tab:domains}). All experiments were conducted on a single Apple M-series CPU core with 8\,GB RAM.

\begin{table}[h]
\centering
\caption{Knowledge acquisition across 12 domains}
\label{tab:domains}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Domain} & \textbf{Entities} & \textbf{Relations} & \textbf{Facts} \\
\midrule
Artificial Intelligence & 2{,}033 & 22 & 10 \\
Climate Change & 1{,}961 & 57 & 10 \\
Deep Learning & 1{,}424 & 41 & 10 \\
Neural Networks & 1{,}492 & 32 & 10 \\
Fusion Power & 1{,}463 & 53 & 10 \\
CRISPR Gene Editing & 1{,}235 & 26 & 10 \\
Machine Learning & 1{,}230 & 32 & 10 \\
RISC-V & 1{,}070 & 6 & 10 \\
Linux & 1{,}026 & 22 & 10 \\
Quantum Computing & 984 & 12 & 10 \\
Rust (Programming) & 602 & 11 & 10 \\
Knowledge Graphs & 317 & 3 & 10 \\
\midrule
\textbf{Total (deduplicated)} & \textbf{9{,}250} & \textbf{190} & \textbf{120} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Performance}

\begin{table}[h]
\centering
\caption{Resource usage}
\label{tab:performance}
\small
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total extraction time (12 pages) & 48\,s \\
Database size & 1.7\,MB \\
Peak memory usage & $<$50\,MB \\
Binary size (release) & 12\,MB \\
Entities per second & $\sim$190 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Prometheus Discovery Results}

After ingesting all 12 domains, Prometheus identified 4 structural patterns:
\begin{itemize}
  \item \textbf{Frequent subgraph}: The \texttt{(is, is)} predicate motif appeared 128 times, indicating dense taxonomic structure across domains.
  \item \textbf{Temporal sequence}: \texttt{is} $\rightarrow$ \texttt{is} chains (26 occurrences), suggesting hierarchical classification patterns.
  \item \textbf{Co-occurrence}: \texttt{using} predicates cluster (4 motifs), indicating tool/technology relationships.
\end{itemize}

While no hypotheses were generated at this scale (the graph requires denser cross-domain connections), the pattern detection demonstrates the viability of automated structural analysis.

% ============================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Strengths}

\textbf{Radical efficiency.} axon processes 12 scientific domains in 48 seconds on a single CPU core, producing a 1.7\,MB knowledge base. An equivalent LLM would require gigabytes of parameters and GPU hours for comparable knowledge coverage.

\textbf{Perfect transparency.} Every fact in the knowledge graph has a source URL, extraction timestamp, and confidence score. There are no hidden representations.

\textbf{True continuous learning.} New knowledge is integrated instantly without any retraining, fine-tuning, or parameter updates. The daemon mode enables fully autonomous 24/7 knowledge acquisition.

\subsection{Limitations}

\textbf{Extraction quality.} Pattern-based NLP cannot match transformer-based NER systems in accuracy. Future work will explore lightweight neural extractors as optional components.

\textbf{Reasoning depth.} CI systems excel at relational queries but lack the generative fluency of language models. We view CI and LLMs as complementary, not competing, paradigms.

\textbf{Scale validation.} Our experiments cover 12 domains; larger-scale evaluation across thousands of sources is needed to assess Prometheus's discovery capabilities.

\subsection{Future Work}

\begin{enumerate}
  \item \textbf{Distributed CI}: Gossip protocol for knowledge sharing between axon instances.
  \item \textbf{Active learning}: Attention mechanisms to prioritize high-value crawling targets.
  \item \textbf{Hybrid CI-LLM}: Use CI as a structured knowledge backend for LLM reasoning.
  \item \textbf{Domain-specific deployment}: Scientific literature mining (PubMed, arXiv).
\end{enumerate}

% ============================================================
\section{Related Work}
\label{sec:related}

\textbf{Knowledge Graphs.} Google's Knowledge Graph \citep{singhal2012knowledge}, Wikidata \citep{vrandecic2014wikidata}, and DBpedia \citep{auer2007dbpedia} are large-scale knowledge bases, but require centralized curation. CI automates the acquisition process.

\textbf{Open Information Extraction.} NELL \citep{carlson2010toward} and OpenIE \citep{banko2007open} extract knowledge from text but lack the temporal dynamics and discovery components of CI.

\textbf{Automated Scientific Discovery.} BACON \citep{langley1987scientific} and more recently AI Scientist \citep{lu2024aiscientist} pursue automated discovery, but rely on predefined hypothesis spaces or LLM generation. Prometheus discovers hypotheses purely from graph structure.

\textbf{Continual Learning.} EWC \citep{kirkpatrick2017overcoming} and progressive networks \citep{rusu2016progressive} address catastrophic forgetting in neural networks. CI avoids the problem entirely through explicit knowledge storage.

% ============================================================
\section{Conclusion}
\label{sec:conclusion}

We have presented Crystalline Intelligence, a novel paradigm for machine intelligence that achieves continuous knowledge acquisition through explicit graph construction rather than parametric learning. Our implementation, axon (8{,}641 LOC Rust, 190 tests), demonstrates that meaningful knowledge extraction and pattern discovery are possible without neural networks, GPUs, or massive datasets. The Prometheus discovery engine represents a first step toward automated scientific reasoning grounded in structural graph analysis.

We believe CI fills a critical gap in the AI landscape: not every intelligent system needs a billion parameters. Sometimes, a crystal is enough.

\vspace{6pt}
\noindent\textbf{Code availability.} \textsc{axon} is open source:\\
\url{https://github.com/redbasecap-buiss/axon}

% ============================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{20}

\bibitem[Auer et~al.(2007)]{auer2007dbpedia}
S.~Auer, C.~Bizer, G.~Kobilarov, J.~Lehmann, R.~Cyganiak, and Z.~Ives.
\newblock {DBpedia}: A nucleus for a web of open data.
\newblock In \emph{ISWC}, pages 722--735, 2007.

\bibitem[Banko et~al.(2007)]{banko2007open}
M.~Banko, M.~J. Cafarella, S.~Soderland, M.~Broadhead, and O.~Etzioni.
\newblock Open information extraction from the web.
\newblock In \emph{IJCAI}, pages 2670--2676, 2007.

\bibitem[Carlson et~al.(2010)]{carlson2010toward}
A.~Carlson, J.~Betteridge, B.~Kisiel, B.~Settles, E.~R. Hruschka, and T.~M. Mitchell.
\newblock Toward an architecture for never-ending language learning.
\newblock In \emph{AAAI}, volume~24, pages 1306--1313, 2010.

\bibitem[Cattell(1963)]{cattell1963theory}
R.~B. Cattell.
\newblock Theory of fluid and crystallized intelligence: A critical experiment.
\newblock \emph{Journal of Educational Psychology}, 54(1):1--22, 1963.

\bibitem[Ebbinghaus(1885)]{ebbinghaus1885memory}
H.~Ebbinghaus.
\newblock \emph{{\"U}ber das Ged{\"a}chtnis}.
\newblock Duncker \& Humblot, Leipzig, 1885.

\bibitem[Kirkpatrick et~al.(2017)]{kirkpatrick2017overcoming}
J.~Kirkpatrick, R.~Pascanu, N.~Rabinowitz, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{PNAS}, 114(13):3521--3526, 2017.

\bibitem[Langley et~al.(1987)]{langley1987scientific}
P.~Langley, H.~A. Simon, G.~L. Bradshaw, and J.~M. Zytkow.
\newblock \emph{Scientific Discovery: Computational Explorations of the Creative Processes}.
\newblock MIT Press, 1987.

\bibitem[LeCun et~al.(2015)]{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521(7553):436--444, 2015.

\bibitem[Lipton(2018)]{lipton2018mythos}
Z.~C. Lipton.
\newblock The mythos of model interpretability.
\newblock \emph{Queue}, 16(3):31--57, 2018.

\bibitem[Lu et~al.(2024)]{lu2024aiscientist}
C.~Lu, C.~Lu, R.~T. Lange, J.~Foerster, J.~Clune, and D.~Ha.
\newblock The {AI} Scientist: Towards fully automated open-ended scientific discovery.
\newblock \emph{arXiv preprint arXiv:2408.06292}, 2024.

\bibitem[Malkov and Yashunin(2018)]{malkov2018efficient}
Y.~A. Malkov and D.~A. Yashunin.
\newblock Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.
\newblock \emph{IEEE TPAMI}, 42(4):824--836, 2018.

\bibitem[Rusu et~al.(2016)]{rusu2016progressive}
A.~A. Rusu, N.~C. Rabinowitz, G.~Desjardins, et~al.
\newblock Progressive neural networks.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Singhal(2012)]{singhal2012knowledge}
A.~Singhal.
\newblock Introducing the Knowledge Graph: things, not strings.
\newblock \emph{Google Blog}, May 2012.

\bibitem[SQLite(2020)]{sqlite2020}
D.~R. Hipp.
\newblock {SQLite}: Most widely deployed database engine.
\newblock \url{https://sqlite.org}, 2020.

\bibitem[Strubell et~al.(2019)]{strubell2019energy}
E.~Strubell, A.~Ganesh, and A.~McCallum.
\newblock Energy and policy considerations for deep learning in {NLP}.
\newblock In \emph{ACL}, pages 3645--3650, 2019.

\bibitem[Vrande{\v{c}}i{\'c} and Kr{\"o}tzsch(2014)]{vrandecic2014wikidata}
D.~Vrande{\v{c}}i{\'c} and M.~Kr{\"o}tzsch.
\newblock Wikidata: A free collaborative knowledgebase.
\newblock \emph{Communications of the ACM}, 57(10):78--85, 2014.

\end{thebibliography}

\end{document}
