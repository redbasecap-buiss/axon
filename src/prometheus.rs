#![allow(dead_code, clippy::type_complexity)]
//! PROMETHEUS — Automated Scientific Discovery Engine
//!
//! Pattern discovery, gap detection, hypothesis generation, validation,
//! and meta-learning over the axon knowledge graph.

use chrono::{Timelike, Utc};
use rusqlite::{params, Connection, Result};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};

use crate::db::Brain;

// ---------------------------------------------------------------------------
// Noise filtering
// ---------------------------------------------------------------------------

/// Entity types to exclude from discovery (low signal).
const NOISE_TYPES: &[&str] = &[
    "phrase",
    "source",
    "url",
    "relative_date",
    "number_unit",
    "date",
    "year",
    "currency",
    "email",
    "compound_noun",
];

/// Predicates too generic to drive discovery.
const GENERIC_PREDICATES: &[&str] = &[
    "is", "are", "was", "were", "has", "have", "had", "be", "been", "do", "does", "did",
];

/// High-value entity types for focused discovery.
const HIGH_VALUE_TYPES: &[&str] = &[
    "person",
    "organization",
    "place",
    "concept",
    "technology",
    "company",
    "product",
    "event",
];

/// Minimum confidence threshold for meaningful hypothesis confirmation.
const CONFIRMATION_THRESHOLD: f64 = 0.7;
/// Auto-reject threshold.
const REJECTION_THRESHOLD: f64 = 0.15;

/// Stopword entity names that sneak through NLP extraction.
const NOISE_NAMES: &[&str] = &[
    "however",
    "this",
    "that",
    "what",
    "which",
    "these",
    "those",
    "where",
    "when",
    "there",
    "here",
    "also",
    "other",
    "such",
    "some",
    "many",
    "most",
    "more",
    "very",
    "just",
    "only",
    "even",
    "still",
    "already",
    "often",
    "never",
    "because",
    "although",
    "therefore",
    "moreover",
    "furthermore",
    "nevertheless",
    "nonetheless",
    "whereas",
    "meanwhile",
    "otherwise",
    "accordingly",
    "consequently",
    "hence",
    "thus",
    "thereby",
    "pages",
    "adding",
    "besides",
    "journal",
    "pdf",
    "time",
    "see",
    "examples",
    "new",
    "used",
    "using",
    "based",
    "known",
    "called",
    "named",
    "including",
    "currently",
    "recently",
    "today",
    "later",
    "isbn",
    "doi",
    "vol",
    "chapter",
    "section",
    "figure",
    "table",
    "appendix",
    "note",
    "notes",
    "references",
    "bibliography",
    "list",
    "index",
    "contents",
    "abstract",
    "introduction",
    "conclusion",
    "discussion",
    "results",
    "methods",
    "analysis",
    "review",
    "surveys",
    "commentary",
    "proceedings",
    "during",
    "resting",
    "within",
    "between",
    "through",
    "about",
    "above",
    "below",
    "after",
    "before",
    "early",
    "modern",
    "several",
    "various",
    "certain",
    "another",
    "following",
    "previous",
    "former",
    "latter",
    "both",
    "each",
    "every",
    "first",
    "second",
    "third",
    "last",
    "next",
    "major",
    "minor",
    "resources",
    "papers",
    "encounters",
    "bits",
    "college",
    "nature",
    "online",
    "links",
    "related",
    "archives",
    "works",
    "press",
    "images",
    "media",
    "events",
    "topics",
    "articles",
    "reports",
    "documents",
    "files",
    "records",
    "entries",
    "items",
    "details",
    "options",
    "updates",
    "features",
    "tools",
    "services",
    "further",
    "reading",
    "external",
    "official",
    "website",
    "webpage",
    "homepage",
    "galleries",
    "collections",
    "publications",
    "editions",
    "instead",
    "similarly",
    "finally",
    "rather",
    "simply",
    "perhaps",
    "certainly",
    "probably",
    "clearly",
    "exactly",
    "indeed",
    "sometimes",
    "always",
    "almost",
    "around",
    "toward",
    "towards",
    "across",
    "along",
    "among",
    "against",
    "unless",
    "until",
    "while",
    "since",
    "despite",
    "however",
    "future",
    "current",
    "letters",
    "foundations",
    "number",
    "internet",
    "program",
    "meanwhile",
    "originally",
    "reportedly",
    "resting",
    "buried",
    "published",
    "culture",
    "cultures",
    "charter",
    "sport",
    "delivered",
    "shy",
];

pub fn is_noise_type(t: &str) -> bool {
    NOISE_TYPES.contains(&t)
}

fn is_generic_predicate(p: &str) -> bool {
    GENERIC_PREDICATES.contains(&p)
}

/// Symmetric predicates: A pred B ⟺ B pred A (no directionality).
fn is_symmetric_predicate(p: &str) -> bool {
    const SYMMETRIC: &[&str] = &[
        "contemporary_of",
        "collaborated_with",
        "partner_of",
        "related_to",
        "related_concept",
        "similar_to",
        "associated_with",
        "allied_with",
        "co-authored_with",
        "co-discovered_with",
        "rival_of",
        "neighbor_of",
        "sibling_of",
        "colleague_of",
        "co-founded_with",
        "located_near",
    ];
    SYMMETRIC.contains(&p)
}

/// Check if two entity types are compatible for hypothesis generation
/// (e.g., person↔person, concept↔technology, organization↔company).
fn types_compatible(a: &str, b: &str) -> bool {
    if a == b {
        return true;
    }
    let compatible_pairs: &[(&str, &str)] = &[
        ("concept", "technology"),
        ("organization", "company"),
        ("person", "organization"),
        ("person", "company"),
        ("place", "organization"),
        ("concept", "event"),
        ("technology", "product"),
        ("organization", "product"),
    ];
    compatible_pairs
        .iter()
        .any(|(x, y)| (a == *x && b == *y) || (a == *y && b == *x))
}

/// Infer a more specific predicate for a hypothesized relation based on entity types
/// and optional shared-neighbor context. Returns a predicate string.
/// `shared_neighbor_types` is a map from entity_type → count of shared neighbors of that type.
fn infer_predicate(
    a_type: &str,
    b_type: &str,
    shared_neighbor_types: Option<&HashMap<String, usize>>,
) -> &'static str {
    // If we have shared-neighbor evidence, use it for person-person refinement
    if a_type == "person" && b_type == "person" {
        if let Some(snt) = shared_neighbor_types {
            if snt.get("organization").copied().unwrap_or(0) > 0 {
                return "colleagues_at";
            }
            if snt.get("concept").copied().unwrap_or(0) >= 2 {
                return "co_researchers";
            }
            if snt.get("event").copied().unwrap_or(0) > 0 {
                return "co_participants";
            }
            if snt.get("technology").copied().unwrap_or(0) > 0 {
                return "co_researchers";
            }
        }
    }

    // Type-pair defaults
    match (a_type, b_type) {
        ("person", "person") => "contemporary_of",
        ("concept", "concept") => "related_concept",
        ("organization", "organization") => "partner_of",
        ("place", "place") => "associated_with",
        ("person", "organization") | ("organization", "person") => "affiliated_with",
        ("person", "concept") | ("concept", "person") => "contributed_to",
        ("person", "place") | ("place", "person") => "active_in",
        ("person", "technology") | ("technology", "person") => "works_on",
        ("organization", "concept") | ("concept", "organization") => "relevant_to",
        ("organization", "place") | ("place", "organization") => "based_in",
        ("technology", "concept") | ("concept", "technology") => "relevant_to",
        _ => "related_to",
    }
}

/// Verb-heavy words that signal a sentence fragment rather than a real entity name.
const FRAGMENT_VERBS: &[&str] = &[
    "would",
    "could",
    "should",
    "became",
    "become",
    "produced",
    "succeeded",
    "incurred",
    "controlled",
    "founded",
    "built",
    "wrote",
    "studied",
];

/// Check if an entity name looks like noise (stopword, too short, numeric, etc.)
pub fn is_noise_name(name: &str) -> bool {
    let lower = name.to_lowercase();
    if NOISE_NAMES.contains(&lower.as_str()) {
        return true;
    }
    // Single word under 3 chars
    if !name.contains(' ') && name.len() < 3 {
        return true;
    }
    // Mostly non-alphabetic (URLs, references, etc.)
    let alpha_count = name.chars().filter(|c| c.is_alphabetic()).count();
    let total = name.len().max(1);
    if alpha_count < total / 2 {
        return true;
    }
    // Starts with common fragment indicators
    let lower_trimmed = lower.trim();
    if lower_trimmed.starts_with("pdf ")
        || lower_trimmed.starts_with("the original")
        || lower_trimmed.starts_with("archived")
        || lower_trimmed.starts_with("because ")
        || lower_trimmed.starts_with("although ")
        || lower_trimmed.starts_with("therefore ")
        || lower_trimmed.starts_with("whereas ")
        || lower_trimmed.starts_with("furthermore ")
        || lower_trimmed.starts_with("moreover ")
        || lower_trimmed.ends_with(" because")
        || lower_trimmed.ends_with(" although")
    {
        return true;
    }
    // Long multi-word names that look like sentence fragments (>6 words)
    let word_count = lower_trimmed.split_whitespace().count();
    if word_count > 6 {
        return true;
    }
    // Names starting with adverbs/conjunctions — NLP extraction errors (e.g. "Eventually Pierre", "Conversely West Berlin")
    if word_count >= 2 {
        let first_word = lower_trimmed.split_whitespace().next().unwrap_or("");
        let adverb_starts = [
            "eventually",
            "conversely",
            "subsequently",
            "additionally",
            "alternatively",
            "approximately",
            "essentially",
            "historically",
            "immediately",
            "increasingly",
            "particularly",
            "primarily",
            "significantly",
            "specifically",
            "traditionally",
            "ultimately",
            "apparently",
            "presumably",
            "supposedly",
            "meanwhile",
            "similarly",
            "likewise",
            "initially",
            "originally",
            "typically",
            "generally",
            "basically",
            "naturally",
            "notably",
            "merely",
            "largely",
            "partly",
            "partly",
        ];
        if adverb_starts.contains(&first_word) {
            return true;
        }
        // Adjective prefixes that indicate NLP extraction noise (e.g. "Current French", "First Delivered", "Enlightened Europe")
        let adjective_starts = [
            "current",
            "first",
            "second",
            "third",
            "last",
            "next",
            "final",
            "original",
            "former",
            "latter",
            "new",
            "old",
            "modern",
            "ancient",
            "early",
            "late",
            "great",
            "greater",
            "lesser",
            "upper",
            "lower",
            "inner",
            "outer",
            "enlightened",
            "allied",
            "total",
            "main",
            "major",
            "minor",
            "entire",
            "brief",
            "shy",
        ];
        if adjective_starts.contains(&first_word) {
            // Exceptions: well-known multi-word proper nouns
            let full = lower_trimmed;
            let is_known_proper = full.starts_with("first world war")
                || full.starts_with("great wall")
                || full.starts_with("great barrier")
                || full.starts_with("new york")
                || full.starts_with("new zealand")
                || full.starts_with("new england")
                || full.starts_with("new jersey")
                || full.starts_with("new south wales")
                || full.starts_with("great britain")
                || full.starts_with("allied powers")
                || full.starts_with("first french")
                || full.starts_with("first world")
                || full.starts_with("old testament")
                || full.starts_with("ancient egypt")
                || full.starts_with("ancient rome")
                || full.starts_with("ancient greece")
                || full.starts_with("lower saxony")
                || full.starts_with("upper austria");
            if !is_known_proper {
                return true;
            }
        }
    }
    // Names starting with gerunds/verbs — NLP extraction errors (e.g. "Using Hilbert", "Subscribe Soviet")
    if word_count >= 2 {
        let first_word = lower_trimmed.split_whitespace().next().unwrap_or("");
        let verb_starts = [
            "using",
            "including",
            "according",
            "following",
            "resulting",
            "subscribe",
            "devastated",
            "introducing",
            "presenting",
            "featuring",
            "announcing",
            "celebrating",
            "exploring",
            "examining",
            "investigating",
            "analyzing",
            "discovering",
            "revealing",
            "surviving",
            "conquering",
            "defeating",
            "invading",
            "occupying",
            "liberating",
            "capturing",
            "completing",
            "completions",
            "completeness",
            "containing",
            "considering",
            "regarding",
            "concerning",
            "involving",
            "requiring",
            "providing",
            "offering",
            "describing",
            "explaining",
            "demonstrating",
            "establishing",
            "developing",
            "producing",
            "officially",
            "formerly",
            "previously",
            "originally",
            "subsequently",
            "newly",
            "recently",
            "creating",
            "building",
            "making",
            "taking",
            "giving",
            "getting",
            "having",
            "being",
            "doing",
            "going",
            "coming",
            "keeping",
            "leaving",
            "putting",
            "running",
            "setting",
            "turning",
            "working",
            "playing",
            "moving",
            "living",
            "starting",
            "beginning",
            "opening",
            "closing",
            "ending",
            "finishing",
            "applying",
            "measuring",
            "calculating",
            "computing",
            "processing",
            "representing",
            "connecting",
            "combining",
            "comparing",
            "assuming",
        ];
        if verb_starts.contains(&first_word) {
            return true;
        }
    }
    // Names containing verbs typical of sentence fragments
    if word_count > 3 {
        let words: Vec<&str> = lower_trimmed.split_whitespace().collect();
        let verb_count = words.iter().filter(|w| FRAGMENT_VERBS.contains(w)).count();
        if verb_count >= 1 {
            return true;
        }
    }
    // Citation-like patterns: contains parentheses with years or publisher info
    if (lower_trimmed.contains("(") && lower_trimmed.contains(")"))
        || lower_trimmed.contains(" pp ")
        || lower_trimmed.contains("pp.")
    {
        return true;
    }
    // Starts with lowercase (likely a sentence fragment, not a proper entity)
    if let Some(first) = name.chars().next() {
        if first.is_lowercase() && word_count > 2 {
            return true;
        }
    }
    // Possessive forms as standalone entities (e.g. "Newton's", "Switzerland's") — noise
    if (lower_trimmed.ends_with("'s") || lower_trimmed.ends_with("'s")) && word_count <= 2 {
        return true;
    }
    // Entities ending with common suffix noise
    if lower_trimmed.ends_with(" et al") || lower_trimmed.ends_with(" et al.") {
        return true;
    }
    // Concatenated country/place lists (e.g. "Britain Prussia Russia Saxony", "France Spain Italy")
    if word_count >= 3 {
        let countries = [
            "britain",
            "france",
            "germany",
            "spain",
            "italy",
            "russia",
            "prussia",
            "austria",
            "hungary",
            "poland",
            "sweden",
            "norway",
            "denmark",
            "portugal",
            "netherlands",
            "belgium",
            "switzerland",
            "england",
            "scotland",
            "ireland",
            "saxony",
            "bavaria",
            "bohemia",
            "moravia",
            "ottoman",
            "byzantine",
            "persia",
            "china",
            "japan",
            "india",
            "egypt",
            "greece",
            "rome",
            "turkey",
        ];
        let words: Vec<&str> = lower_trimmed.split_whitespace().collect();
        let country_count = words.iter().filter(|w| countries.contains(w)).count();
        if country_count >= 2 && country_count as f64 / word_count as f64 >= 0.5 {
            return true;
        }
    }
    // Entities that are just adjectives/demonyms (e.g. "German-speaking", "British", "Jewish")
    let demonym_suffixes = ["ish", "ian", "ese", "ean", "speaking"];
    if word_count == 1
        && demonym_suffixes.iter().any(|s| lower_trimmed.ends_with(s))
        && lower_trimmed.len() < 20
    {
        return true;
    }
    // Entities that look like duplicated words (e.g. "Zurich Airport Zurich Airport")
    if word_count >= 4 {
        let words: Vec<&str> = lower_trimmed.split_whitespace().collect();
        let half = words.len() / 2;
        if words[..half] == words[half..half * 2] {
            return true;
        }
    }
    // Capitalized generic words that aren't real entities
    let generic_caps = [
        "history",
        "theory",
        "revolution",
        "state",
        "prize",
        "meanwhile",
        "buried",
        "research",
        "finally",
        "published",
        "development",
        "example",
        "general",
        "original",
        "national",
        "international",
        "government",
        "society",
        "university",
        "institute",
        "foundation",
        "academy",
        "department",
        "commentary",
        "surveys",
        "critique",
        "conflict",
        "sport",
        "civilization",
        "tradition",
        "empire",
        "kingdom",
        "dynasty",
        "republic",
        "province",
        "strom",
        "like",
        "daughter",
        "airport",
        "cathedral",
        "city",
        "county",
        "church",
        "reformed",
        "orthodox",
        "catholic",
        "association",
        "mathematical",
        "henry",
        "phillips",
        "admiral",
        "director",
        "minimum",
        "maximum",
        "define",
        "storms",
        "formulas",
        "candidates",
        "prejudice",
        "birthplace",
        "novels",
        "slaughter",
        "inferno",
        "scotsman",
        "century",
        "decades",
        "period",
        "chapter",
        "volume",
        "edition",
        "series",
        "region",
        "area",
        "system",
        "process",
        "method",
        "approach",
        "model",
        "structure",
        "function",
        "principle",
        "concept",
        "element",
        "factor",
        "feature",
        "aspect",
        "issue",
        "problem",
        "solution",
        "result",
        "effect",
        "impact",
        "role",
        "type",
        "form",
        "kind",
        "level",
        "degree",
        "range",
        "rate",
        "size",
        "number",
        "amount",
        "value",
        "point",
        "source",
        "basis",
        "terms",
        "means",
        "end",
        "part",
        "side",
        "case",
        "fact",
        "idea",
        "view",
        "sense",
    ];
    if word_count == 1 && generic_caps.contains(&lower_trimmed) {
        return true;
    }
    // Multi-word names ending with generic nouns that aren't real entities
    let trailing_generic = [
        "resources",
        "papers",
        "encounters",
        "bits",
        "links",
        "archives",
        "works",
        "images",
        "media",
        "articles",
        "reports",
        "documents",
        "files",
        "records",
        "entries",
        "items",
        "details",
        "options",
        "updates",
        "features",
        "tools",
        "services",
        "publications",
        "editions",
        "galleries",
        "collections",
        // Generic nouns that indicate NLP extraction noise when trailing
        "lecture",
        "lectures",
        "kinship",
        "child",
        "children",
        "down",
        "up",
        "out",
        "off",
        "away",
        "back",
        "forward",
        "together",
        "apart",
        "inside",
        "outside",
        "above",
        "below",
        "across",
        "through",
        "along",
        "behind",
        "movement",
        "movements",
        "rebellion",
        "uprising",
        "revolt",
        "campaign",
        "campaigns",
        "controversy",
        "conspiracy",
        "hypothesis",
        "thesis",
        "dissertation",
        "manuscript",
        "manuscripts",
        "transcript",
        "transcripts",
        "memorial",
        "memorials",
        "monument",
        "monuments",
        "testimony",
        "analysis",
        "overview",
        "summary",
        "introduction",
        "conclusion",
        "discussion",
        "comparison",
        "reference",
        "references",
        "bibliography",
        "footnote",
        "footnotes",
        "appendix",
        "index",
        "glossary",
        "catalog",
        "catalogue",
        "inventory",
        "directory",
        "handbook",
        "manual",
        "guide",
        "tutorial",
        "textbook",
    ];
    if word_count >= 2 {
        let last_word = lower_trimmed.split_whitespace().last().unwrap_or("");
        if trailing_generic.contains(&last_word) {
            return true;
        }
    }
    // Names ending with "Journal", "During", "Resting", "Commentary" etc. — citation/fragment noise
    let trailing_noise = [
        "journal",
        "during",
        "resting",
        "commentary",
        "surveys",
        "proceedings",
        "magazine",
        "review",
        "bulletin",
        "newsletter",
        "gazette",
        "digest",
        "quarterly",
        "annual",
        "monthly",
        "weekly",
        // Source/website names that get concatenated with entities
        "sciencedaily",
        "wikipedia",
        "britannica",
        "jstor",
        "springer",
        "wiley",
        "elsevier",
        "arxiv",
        "doi",
        "isbn",
        "issn",
        "oclc",
        "pmid",
    ];
    if word_count >= 2 {
        let last_word = lower_trimmed.split_whitespace().last().unwrap_or("");
        if trailing_noise.contains(&last_word) {
            return true;
        }
    }
    // Names containing "Journal" anywhere (journal titles extracted as entities)
    if lower_trimmed.contains("journal") {
        return true;
    }
    // Names containing academic publication patterns
    if lower_trimmed.contains("monthly notices") || lower_trimmed.contains("physical review") {
        return true;
    }
    // Multi-word names containing known noise words in non-first position (concatenation artifacts)
    if word_count >= 3 {
        let words: Vec<&str> = lower_trimmed.split_whitespace().collect();
        let concat_noise = [
            "examples",
            "encounters",
            "bits",
            "papers",
            "college",
            "online",
            "links",
            "archives",
            "images",
            "media",
            "articles",
            "reports",
            "documents",
            "files",
            "records",
            "president",
            "admiral",
            "director",
        ];
        let noise_count = words.iter().filter(|w| concat_noise.contains(w)).count();
        if noise_count >= 1 {
            return true;
        }
    }
    // Names containing 4-digit years (citation/reference fragments like "Ramanujan Journal 1997 1")
    let year_re = lower_trimmed.split_whitespace().any(|w| {
        w.len() == 4 && w.chars().all(|c| c.is_ascii_digit()) && w.starts_with(['1', '2'])
    });
    if year_re && word_count >= 3 {
        return true;
    }
    // Roman numeral suffixes without real content (e.g. "Michael V", "Morgan I")
    let roman_numerals = [
        "i", "ii", "iii", "iv", "v", "vi", "vii", "viii", "ix", "x", "xi", "xii",
    ];
    if word_count == 2 {
        let words: Vec<&str> = lower_trimmed.split_whitespace().collect();
        if roman_numerals.contains(&words[1]) && words[0].len() <= 10 {
            // But allow real names like "Henry VIII" — only filter if first word isn't a common first name
            // For now, be conservative and keep this as a soft signal
            // combined with other noise indicators
        }
    }
    // "Like" suffix patterns (e.g. "German Like")
    if word_count >= 2 && lower_trimmed.ends_with(" like") {
        return true;
    }
    // Names that are mostly numbers with some text (e.g. "1997 1", "Vol 23")
    let digit_count = lower_trimmed.chars().filter(|c| c.is_ascii_digit()).count();
    if digit_count > 0 && digit_count as f64 / lower_trimmed.len().max(1) as f64 > 0.4 {
        return true;
    }
    // Names starting with "Sir" followed by a single letter (e.g. "Sir I")
    if lower_trimmed.starts_with("sir ") && word_count <= 2 {
        return true;
    }
    // Names containing single-letter words interspersed (citation fragments like "Symanzik K Schrödinger")
    if word_count >= 3 {
        let words: Vec<&str> = lower_trimmed.split_whitespace().collect();
        let single_letter_count = words.iter().filter(|w| w.len() == 1).count();
        if single_letter_count >= 1 && single_letter_count as f64 / word_count as f64 >= 0.25 {
            return true;
        }
    }
    // Names that are just titles/honorifics
    let title_only = ["sir", "mr", "mrs", "ms", "dr", "prof", "lord", "lady"];
    if word_count == 1 && title_only.contains(&lower_trimmed) {
        return true;
    }
    // Concatenated entity detection: names that look like 2+ entities mashed together.
    // Pattern: "Einstein Boris Podolsky", "Bohr Bush", "Baltimore Maryland Archimedes"
    // Heuristic: 3+ words where a middle word is a common first name suggests concatenation.
    if word_count >= 3 {
        let words: Vec<&str> = lower_trimmed.split_whitespace().collect();
        // Check if any non-first, non-last word is a common first name (suggests entity boundary)
        let first_names = [
            "albert",
            "isaac",
            "niels",
            "werner",
            "max",
            "erwin",
            "paul",
            "carl",
            "karl",
            "ernst",
            "ludwig",
            "fritz",
            "otto",
            "hans",
            "kurt",
            "richard",
            "robert",
            "james",
            "john",
            "william",
            "charles",
            "george",
            "edward",
            "henry",
            "thomas",
            "david",
            "michael",
            "joseph",
            "alexander",
            "leonhard",
            "bernhard",
            "hermann",
            "heinrich",
            "johann",
            "gottfried",
            "wilhelm",
            "friedrich",
            "georg",
            "peter",
            "martin",
            "franz",
            "boris",
            "nikolai",
            "sergei",
            "andrei",
            "vladimir",
            "ivan",
            "dmitri",
            "alan",
            "arthur",
            "francis",
            "stephen",
            "felix",
            "emmy",
            "marie",
            "ada",
            "sophia",
            "anna",
            "margaret",
            "mary",
            "elizabeth",
            "benjamin",
            "daniel",
            "donald",
            "jacques",
            "pierre",
            "jean",
            "louis",
            "antoine",
            "nicolas",
            "henri",
            "rené",
            "giuseppe",
            "giovanni",
            "antonio",
            "marco",
            "andreas",
            "rafael",
            "enrico",
            "lise",
            "emile",
            "auguste",
        ];
        let boundary_hits = words[1..words.len() - 1]
            .iter()
            .filter(|w| first_names.contains(w))
            .count();
        if boundary_hits >= 1 {
            return true;
        }
    }
    // Two-word names where both words are known famous surnames (concatenation like "Bohr Bush", "Newton Gauss")
    if word_count == 2 {
        let words: Vec<&str> = lower_trimmed.split_whitespace().collect();
        let famous_surnames = [
            "einstein",
            "newton",
            "bohr",
            "heisenberg",
            "euler",
            "gauss",
            "hilbert",
            "riemann",
            "lagrange",
            "laplace",
            "fermat",
            "pascal",
            "leibniz",
            "kepler",
            "copernicus",
            "galileo",
            "darwin",
            "maxwell",
            "faraday",
            "planck",
            "dirac",
            "schrödinger",
            "pauli",
            "feynman",
            "hawking",
            "curie",
            "lovelace",
            "turing",
            "archimedes",
            "euclid",
            "ptolemy",
            "pythagoras",
            "bush",
            "poldolsky",
            "podolsky",
            "rosen",
            "bell",
            "born",
            "fermi",
            "oppenheimer",
            "rutherford",
            "tesla",
            "edison",
            "marconi",
            "hertz",
            "boltzmann",
            "carnot",
            "kelvin",
            "joule",
            "ampere",
            "volta",
            "ohm",
            "coulomb",
            "fresnel",
            "huygens",
            "hooke",
            "watt",
            "bernoulli",
            "fourier",
            "cauchy",
            "abel",
            "galois",
            "cantor",
            "dedekind",
            "weierstrass",
            "poincare",
            "noether",
            "ramanujan",
            "hardy",
            "littlewood",
            "gödel",
            "church",
            "von neumann",
        ];
        if famous_surnames.contains(&words[0]) && famous_surnames.contains(&words[1]) {
            return true;
        }
    }
    // Concatenated place+person or person+place: 4+ words where multiple known places appear
    // alongside other entity types (e.g. "Dhar Nema Mauritania", "Pacific Ocean Commerce")
    if word_count >= 4 {
        let words: Vec<&str> = lower_trimmed.split_whitespace().collect();
        let known_places = [
            "maryland",
            "baltimore",
            "california",
            "virginia",
            "carolina",
            "massachusetts",
            "pennsylvania",
            "connecticut",
            "washington",
            "florida",
            "georgia",
            "oregon",
            "mauritania",
            "patagonia",
            "rawson",
            "gulf",
            "ocean",
            "pacific",
            "atlantic",
            "commerce",
            "tutorials",
            "monastery",
        ];
        let place_hits = words.iter().filter(|w| known_places.contains(w)).count();
        let non_place = word_count - place_hits;
        // If we have places mixed with other words in a 4+ word name, it's concatenation
        if place_hits >= 1 && non_place >= 2 && !lower_trimmed.contains("university") {
            // But allow genuine multi-word place names — only flag if there's a person-like word too
            let has_person_word = words.iter().any(|w| first_names_extended().contains(w));
            if has_person_word {
                return true;
            }
        }
    }
    // Names containing source/website indicators anywhere (e.g. "Comet ScienceDaily")
    let source_indicators = [
        "sciencedaily",
        "phys.org",
        "livescience",
        "newscientist",
        "nationalgeographic",
        "smithsonian",
        "bbc",
    ];
    if source_indicators.iter().any(|s| lower_trimmed.contains(s)) {
        return true;
    }
    // Two-word names where both words are all-caps abbreviations or very short generic words
    // (e.g. "Two Child", "Women Kinship")
    if word_count == 2 {
        let words: Vec<&str> = lower_trimmed.split_whitespace().collect();
        let generic_first = [
            "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten", "many", "some",
            "few", "several", "most", "all", "each", "every", "women", "men", "people", "human",
            "humans", "comet", "planet",
        ];
        let generic_second = [
            "child", "children", "kinship", "down", "up", "theory", "policy", "problem", "paradox",
            "question", "answer", "rule", "law", "bride", "groom", "lecture", "speech", "talk",
        ];
        if generic_first.contains(&words[0]) && generic_second.contains(&words[1]) {
            return true;
        }
    }
    false
}

/// Extended first names list for concatenation detection (called rarely, not perf-critical).
fn first_names_extended() -> HashSet<&'static str> {
    [
        "albert",
        "isaac",
        "niels",
        "werner",
        "max",
        "erwin",
        "paul",
        "carl",
        "karl",
        "ernst",
        "ludwig",
        "fritz",
        "otto",
        "hans",
        "kurt",
        "richard",
        "robert",
        "james",
        "john",
        "william",
        "charles",
        "george",
        "edward",
        "henry",
        "thomas",
        "david",
        "michael",
        "joseph",
        "alexander",
        "leonhard",
        "bernhard",
        "hermann",
        "heinrich",
        "johann",
        "gottfried",
        "wilhelm",
        "friedrich",
        "georg",
        "peter",
        "martin",
        "franz",
        "boris",
        "nikolai",
        "sergei",
        "andrei",
        "vladimir",
        "ivan",
        "dmitri",
        "alan",
        "arthur",
        "francis",
        "stephen",
        "felix",
        "emmy",
        "marie",
        "ada",
        "sophia",
        "anna",
        "margaret",
        "mary",
        "elizabeth",
        "benjamin",
        "daniel",
        "donald",
        "jacques",
        "pierre",
        "jean",
        "louis",
        "antoine",
        "nicolas",
        "henri",
        "rené",
        "giuseppe",
        "giovanni",
        "antonio",
        "marco",
        "andreas",
        "rafael",
        "enrico",
        "lise",
        "emile",
        "auguste",
        "diodorus",
        "archimedes",
        "euclid",
        "ptolemy",
        "heisenberg",
        "einstein",
        "bohr",
        "newton",
        "euler",
        "gauss",
        "hilbert",
        "riemann",
        "lagrange",
        "laplace",
    ]
    .into_iter()
    .collect()
}

/// Filter entity IDs to only meaningful ones (non-noise type, reasonable name, quality).
fn meaningful_ids(brain: &Brain) -> Result<HashSet<i64>> {
    let entities = brain.all_entities()?;
    Ok(entities
        .iter()
        .filter(|e| {
            !is_noise_type(&e.entity_type)
                && e.name.len() <= 80
                && e.name.len() >= 2
                && !is_noise_name(&e.name)
        })
        .map(|e| e.id)
        .collect())
}

// ---------------------------------------------------------------------------
// Data structures
// ---------------------------------------------------------------------------

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum HypothesisStatus {
    Proposed,
    Testing,
    Confirmed,
    Rejected,
}

impl HypothesisStatus {
    pub fn as_str(&self) -> &'static str {
        match self {
            Self::Proposed => "proposed",
            Self::Testing => "testing",
            Self::Confirmed => "confirmed",
            Self::Rejected => "rejected",
        }
    }

    pub fn from_str(s: &str) -> Self {
        match s {
            "testing" => Self::Testing,
            "confirmed" => Self::Confirmed,
            "rejected" => Self::Rejected,
            _ => Self::Proposed,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Hypothesis {
    pub id: i64,
    pub subject: String,
    pub predicate: String,
    pub object: String,
    pub confidence: f64,
    pub evidence_for: Vec<String>,
    pub evidence_against: Vec<String>,
    pub reasoning_chain: Vec<String>,
    pub status: HypothesisStatus,
    pub discovered_at: String,
    pub pattern_source: String,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum PatternType {
    CoOccurrence,
    StructuralHole,
    TypeGap,
    Analogy,
    TemporalSequence,
    FrequentSubgraph,
}

impl PatternType {
    pub fn as_str(&self) -> &'static str {
        match self {
            Self::CoOccurrence => "co_occurrence",
            Self::StructuralHole => "structural_hole",
            Self::TypeGap => "type_gap",
            Self::Analogy => "analogy",
            Self::TemporalSequence => "temporal_sequence",
            Self::FrequentSubgraph => "frequent_subgraph",
        }
    }

    pub fn from_str(s: &str) -> Self {
        match s {
            "co_occurrence" => Self::CoOccurrence,
            "structural_hole" => Self::StructuralHole,
            "type_gap" => Self::TypeGap,
            "analogy" => Self::Analogy,
            "temporal_sequence" => Self::TemporalSequence,
            "frequent_subgraph" => Self::FrequentSubgraph,
            _ => Self::CoOccurrence,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Pattern {
    pub id: i64,
    pub pattern_type: PatternType,
    pub entities_involved: Vec<String>,
    pub frequency: i64,
    pub last_seen: String,
    pub description: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Discovery {
    pub id: i64,
    pub hypothesis_id: i64,
    pub confirmed_at: String,
    pub evidence_sources: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PatternWeight {
    pub pattern_type: String,
    pub confirmations: i64,
    pub rejections: i64,
    pub weight: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DiscoveryReport {
    pub patterns_found: Vec<Pattern>,
    pub hypotheses_generated: Vec<Hypothesis>,
    pub gaps_detected: usize,
    pub summary: String,
}

// ---------------------------------------------------------------------------
// Schema initialisation
// ---------------------------------------------------------------------------

pub fn init_prometheus_schema(conn: &Connection) -> Result<()> {
    conn.execute_batch(
        "
        CREATE TABLE IF NOT EXISTS hypotheses (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            subject TEXT NOT NULL,
            predicate TEXT NOT NULL,
            object TEXT NOT NULL,
            confidence REAL NOT NULL DEFAULT 0.5,
            evidence_for TEXT NOT NULL DEFAULT '[]',
            evidence_against TEXT NOT NULL DEFAULT '[]',
            reasoning_chain TEXT NOT NULL DEFAULT '[]',
            status TEXT NOT NULL DEFAULT 'proposed',
            discovered_at TEXT NOT NULL,
            pattern_source TEXT NOT NULL DEFAULT ''
        );
        CREATE TABLE IF NOT EXISTS patterns (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            pattern_type TEXT NOT NULL,
            entities_involved TEXT NOT NULL DEFAULT '[]',
            frequency INTEGER NOT NULL DEFAULT 1,
            last_seen TEXT NOT NULL,
            description TEXT NOT NULL DEFAULT ''
        );
        CREATE TABLE IF NOT EXISTS discoveries (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            hypothesis_id INTEGER NOT NULL,
            confirmed_at TEXT NOT NULL,
            evidence_sources TEXT NOT NULL DEFAULT '[]',
            FOREIGN KEY(hypothesis_id) REFERENCES hypotheses(id)
        );
        CREATE TABLE IF NOT EXISTS pattern_weights (
            pattern_type TEXT PRIMARY KEY,
            confirmations INTEGER NOT NULL DEFAULT 0,
            rejections INTEGER NOT NULL DEFAULT 0,
            weight REAL NOT NULL DEFAULT 1.0
        );
        CREATE INDEX IF NOT EXISTS idx_hypotheses_status ON hypotheses(status);
        CREATE INDEX IF NOT EXISTS idx_patterns_type ON patterns(pattern_type);
        ",
    )
}

// ---------------------------------------------------------------------------
// Prometheus engine
// ---------------------------------------------------------------------------

pub struct Prometheus<'a> {
    brain: &'a Brain,
}

impl<'a> Prometheus<'a> {
    pub fn new(brain: &'a Brain) -> Result<Self> {
        brain.with_conn(init_prometheus_schema)?;
        Ok(Self { brain })
    }

    // -----------------------------------------------------------------------
    // Pattern Discovery
    // -----------------------------------------------------------------------

    /// Find entities that share a source URL but have no direct relation.
    /// In sparse graphs, co-extraction from the same page is strong evidence of relatedness.
    pub fn find_source_co_occurrences(&self) -> Result<Vec<Pattern>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Group entities by source_url
        let mut source_entities: HashMap<String, HashSet<i64>> = HashMap::new();
        for r in &relations {
            if !r.source_url.is_empty() {
                if meaningful.contains(&r.subject_id) {
                    source_entities
                        .entry(r.source_url.clone())
                        .or_default()
                        .insert(r.subject_id);
                }
                if meaningful.contains(&r.object_id) {
                    source_entities
                        .entry(r.source_url.clone())
                        .or_default()
                        .insert(r.object_id);
                }
            }
        }

        // Build direct-connection set
        let mut connected: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            connected.insert(key);
        }

        // Find pairs sharing ≥2 sources but not directly connected
        let mut pair_sources: HashMap<(i64, i64), usize> = HashMap::new();
        for entities in source_entities.values() {
            // Skip sources with too many entities (O(n²) pairs)
            if entities.len() > 50 {
                continue;
            }
            let ids: Vec<i64> = entities.iter().copied().collect();
            for i in 0..ids.len() {
                for j in (i + 1)..ids.len() {
                    let key = if ids[i] < ids[j] {
                        (ids[i], ids[j])
                    } else {
                        (ids[j], ids[i])
                    };
                    if !connected.contains(&key) {
                        *pair_sources.entry(key).or_insert(0) += 1;
                    }
                }
            }
        }

        let mut patterns = Vec::new();
        for ((a, b), count) in &pair_sources {
            if *count >= 2 {
                let a_name = self.entity_name(*a)?;
                let b_name = self.entity_name(*b)?;
                patterns.push(Pattern {
                    id: 0,
                    pattern_type: PatternType::CoOccurrence,
                    entities_involved: vec![a_name.clone(), b_name.clone()],
                    frequency: *count as i64,
                    last_seen: now_str(),
                    description: format!(
                        "{} and {} co-occur in {} source(s) but lack direct relation",
                        a_name, b_name, count
                    ),
                });
            }
        }
        patterns.sort_by(|a, b| b.frequency.cmp(&a.frequency));
        patterns.truncate(50);
        Ok(patterns)
    }

    /// Find co-occurring entity pairs using Jaccard similarity.
    /// Better than raw count for sparse graphs — normalizes by neighbourhood size.
    pub fn find_co_occurrences(&self, min_shared: usize) -> Result<Vec<Pattern>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;
        let mut neighbours: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            if meaningful.contains(&r.subject_id) && meaningful.contains(&r.object_id) {
                neighbours
                    .entry(r.subject_id)
                    .or_default()
                    .insert(r.object_id);
                neighbours
                    .entry(r.object_id)
                    .or_default()
                    .insert(r.subject_id);
            }
        }
        // Only consider entities with ≥2 neighbours (otherwise can't share any)
        let ids: Vec<i64> = neighbours
            .iter()
            .filter(|(_, nb)| nb.len() >= min_shared.max(2))
            .map(|(&id, _)| id)
            .collect();
        // Cap at 2000 highest-degree entities to avoid O(n²) blowup
        let mut ids_sorted: Vec<(i64, usize)> =
            ids.iter().map(|&id| (id, neighbours[&id].len())).collect();
        ids_sorted.sort_by(|a, b| b.1.cmp(&a.1));
        ids_sorted.truncate(2000);
        let ids: Vec<i64> = ids_sorted.into_iter().map(|(id, _)| id).collect();
        // Use inverted index: for each neighbour, list which candidates have it
        let mut inv: HashMap<i64, Vec<usize>> = HashMap::new();
        for (idx, &id) in ids.iter().enumerate() {
            for &nb in &neighbours[&id] {
                inv.entry(nb).or_default().push(idx);
            }
        }
        // Count shared neighbours via inverted index (avoids full O(n²))
        let mut pair_shared: HashMap<(usize, usize), usize> = HashMap::new();
        for posting in inv.values() {
            if posting.len() < 2 || posting.len() > 200 {
                continue; // skip hubs to avoid quadratic blowup within posting lists
            }
            for i in 0..posting.len() {
                for j in (i + 1)..posting.len() {
                    let key = (posting[i].min(posting[j]), posting[i].max(posting[j]));
                    *pair_shared.entry(key).or_insert(0) += 1;
                }
            }
        }
        let mut patterns = Vec::new();
        for ((i, j), shared) in &pair_shared {
            if *shared >= min_shared {
                let a = ids[*i];
                let b = ids[*j];
                let na = &neighbours[&a];
                let nb = &neighbours[&b];
                let union_size = na.union(nb).count();
                let jaccard = if union_size > 0 {
                    *shared as f64 / union_size as f64
                } else {
                    0.0
                };
                let a_name = self.entity_name(a)?;
                let b_name = self.entity_name(b)?;
                patterns.push(Pattern {
                    id: 0,
                    pattern_type: PatternType::CoOccurrence,
                    entities_involved: vec![a_name.clone(), b_name.clone()],
                    frequency: *shared as i64,
                    last_seen: now_str(),
                    description: format!(
                        "{} and {} share {} neighbours (Jaccard: {:.2})",
                        a_name, b_name, shared, jaccard
                    ),
                });
            }
        }
        // Sort by frequency descending for better results
        patterns.sort_by(|a, b| b.frequency.cmp(&a.frequency));
        patterns.truncate(100);
        Ok(patterns)
    }

    /// Find co-occurring entity pairs using Pointwise Mutual Information (PMI).
    /// PMI measures how much more likely two entities are to co-occur (share a source)
    /// than expected by chance. Better than Jaccard for sparse graphs because it
    /// accounts for entity frequency — rare entities co-occurring is more informative
    /// than common ones. PMI = log2(P(a,b) / (P(a) * P(b)))
    pub fn find_pmi_co_occurrences(&self, min_pmi: f64) -> Result<Vec<Pattern>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Build entity → set of source URLs
        let mut entity_sources: HashMap<i64, HashSet<String>> = HashMap::new();
        let mut all_sources: HashSet<String> = HashSet::new();
        for r in &relations {
            if !r.source_url.is_empty() {
                all_sources.insert(r.source_url.clone());
                if meaningful.contains(&r.subject_id) {
                    entity_sources
                        .entry(r.subject_id)
                        .or_default()
                        .insert(r.source_url.clone());
                }
                if meaningful.contains(&r.object_id) {
                    entity_sources
                        .entry(r.object_id)
                        .or_default()
                        .insert(r.source_url.clone());
                }
            }
        }

        let n = all_sources.len() as f64;
        if n < 2.0 {
            return Ok(vec![]);
        }

        // Build direct-connection set
        let mut connected: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            connected.insert(key);
        }

        // Hub penalty: entities appearing in too many sources produce noisy PMI.
        // Cap at 30% of all sources — beyond that, co-occurrence is expected, not surprising.
        let hub_threshold = (n * 0.3).max(5.0) as usize;

        // Only consider entities appearing in ≥2 sources (reduces noise + O(n²) cost)
        let mut candidates: Vec<(i64, &HashSet<String>)> = entity_sources
            .iter()
            .filter(|(_, sources)| sources.len() >= 2 && sources.len() <= hub_threshold)
            .map(|(&id, sources)| (id, sources))
            .collect();
        // Cap at 1000 entities with most sources to avoid O(n²) blowup
        candidates.sort_by(|a, b| b.1.len().cmp(&a.1.len()));
        candidates.truncate(1000);

        // Build inverted index: source_url → list of candidate indices
        let mut source_to_cands: HashMap<&str, Vec<usize>> = HashMap::new();
        for (idx, &(_, sources)) in candidates.iter().enumerate() {
            for src in sources {
                source_to_cands.entry(src.as_str()).or_default().push(idx);
            }
        }

        // Count co-occurrences via inverted index (avoids O(n²) full scan)
        let mut pair_joint: HashMap<(usize, usize), usize> = HashMap::new();
        for posting in source_to_cands.values() {
            if posting.len() < 2 || posting.len() > 200 {
                continue;
            }
            for i in 0..posting.len() {
                for j in (i + 1)..posting.len() {
                    let key = (posting[i].min(posting[j]), posting[i].max(posting[j]));
                    *pair_joint.entry(key).or_insert(0) += 1;
                }
            }
        }

        let mut patterns = Vec::new();
        for ((i, j), joint_count) in &pair_joint {
            let (a, sa) = candidates[*i];
            let (b, sb) = candidates[*j];
            let key = if a < b { (a, b) } else { (b, a) };
            if connected.contains(&key) {
                continue;
            }
            let joint = *joint_count as f64;
            let p_ab = joint / n;
            let p_a = sa.len() as f64 / n;
            let p_b = sb.len() as f64 / n;
            let pmi = (p_ab / (p_a * p_b)).log2();
            if pmi >= min_pmi {
                let a_name = self.entity_name(a)?;
                let b_name = self.entity_name(b)?;
                patterns.push(Pattern {
                    id: 0,
                    pattern_type: PatternType::CoOccurrence,
                    entities_involved: vec![a_name.clone(), b_name.clone()],
                    frequency: joint as i64,
                    last_seen: now_str(),
                    description: format!(
                        "{} and {} have PMI={:.2} ({} shared sources) — statistically surprising co-occurrence",
                        a_name, b_name, pmi, joint as i64
                    ),
                });
            }
        }
        patterns.sort_by(|a, b| b.frequency.cmp(&a.frequency));
        patterns.truncate(30);
        Ok(patterns)
    }

    /// Find entity clusters: groups of entities connected by the same predicate type.
    /// Useful for discovering thematic clusters in sparse graphs.
    pub fn find_entity_clusters(&self) -> Result<Vec<Pattern>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;
        // Group entities by predicate they participate in
        let mut pred_entities: HashMap<String, HashSet<i64>> = HashMap::new();
        for r in &relations {
            if meaningful.contains(&r.subject_id) {
                pred_entities
                    .entry(r.predicate.clone())
                    .or_default()
                    .insert(r.subject_id);
            }
            if meaningful.contains(&r.object_id) {
                pred_entities
                    .entry(r.predicate.clone())
                    .or_default()
                    .insert(r.object_id);
            }
        }
        let mut patterns = Vec::new();
        for (pred, entities) in &pred_entities {
            if entities.len() >= 3 && !is_generic_predicate(pred) {
                let names: Vec<String> = entities
                    .iter()
                    .take(5)
                    .filter_map(|&id| self.entity_name(id).ok())
                    .collect();
                patterns.push(Pattern {
                    id: 0,
                    pattern_type: PatternType::CoOccurrence,
                    entities_involved: names,
                    frequency: entities.len() as i64,
                    last_seen: now_str(),
                    description: format!(
                        "Predicate '{}' connects {} entities — thematic cluster",
                        pred,
                        entities.len()
                    ),
                });
            }
        }
        patterns.sort_by(|a, b| b.frequency.cmp(&a.frequency));
        Ok(patterns)
    }

    /// Find knowledge frontiers: entity types that appear in many entities
    /// but have disproportionately few relations (growth potential).
    pub fn find_knowledge_frontiers(&self) -> Result<Vec<(String, usize, f64, String)>> {
        let density = crate::graph::knowledge_density(self.brain)?;
        let mut frontiers: Vec<(String, usize, f64, String)> = Vec::new();
        for (etype, (count, avg_rels)) in &density {
            if is_noise_type(etype) || *count < 2 {
                continue;
            }
            // Low density = high frontier potential
            if *avg_rels < 2.0 {
                let reason = if *avg_rels < 0.5 {
                    format!(
                        "CRITICAL: {} '{}' entities, only {:.1} avg relations — near-zero connectivity",
                        count, etype, avg_rels
                    )
                } else {
                    format!(
                        "{} '{}' entities with {:.1} avg relations — underexplored",
                        count, etype, avg_rels
                    )
                };
                frontiers.push((etype.clone(), *count, *avg_rels, reason));
            }
        }
        frontiers.sort_by(|a, b| a.2.partial_cmp(&b.2).unwrap_or(std::cmp::Ordering::Equal));
        Ok(frontiers)
    }

    /// Find frequent subgraph patterns — recurring predicate patterns around entities.
    pub fn find_frequent_subgraphs(&self, min_freq: usize) -> Result<Vec<Pattern>> {
        let relations = self.brain.all_relations()?;
        // Count predicate pair motifs: (pred_out, pred_in) around a common entity
        let mut motifs: HashMap<(String, String), Vec<(i64, i64, i64)>> = HashMap::new();
        let mut outgoing: HashMap<i64, Vec<(String, i64)>> = HashMap::new();
        for r in &relations {
            outgoing
                .entry(r.subject_id)
                .or_default()
                .push((r.predicate.clone(), r.object_id));
        }
        // For each entity with 2+ outgoing edges, pair predicates
        for (eid, edges) in &outgoing {
            for i in 0..edges.len() {
                for j in (i + 1)..edges.len() {
                    let key = if edges[i].0 <= edges[j].0 {
                        (edges[i].0.clone(), edges[j].0.clone())
                    } else {
                        (edges[j].0.clone(), edges[i].0.clone())
                    };
                    motifs
                        .entry(key)
                        .or_default()
                        .push((*eid, edges[i].1, edges[j].1));
                }
            }
        }
        let mut patterns = Vec::new();
        for ((p1, p2), instances) in &motifs {
            if instances.len() >= min_freq && !is_generic_predicate(p1) && !is_generic_predicate(p2)
            {
                let example_names: Vec<String> = instances
                    .iter()
                    .take(3)
                    .filter_map(|(e, _, _)| self.entity_name(*e).ok())
                    .collect();
                patterns.push(Pattern {
                    id: 0,
                    pattern_type: PatternType::FrequentSubgraph,
                    entities_involved: example_names,
                    frequency: instances.len() as i64,
                    last_seen: now_str(),
                    description: format!(
                        "Motif ({}, {}) appears {} times",
                        p1,
                        p2,
                        instances.len()
                    ),
                });
            }
        }
        Ok(patterns)
    }

    /// Find temporal patterns — predicates that often appear in sequence by learned_at.
    pub fn find_temporal_patterns(&self, min_freq: usize) -> Result<Vec<Pattern>> {
        let relations = self.brain.all_relations()?;
        // Group by subject, sort by learned_at, look at consecutive predicate pairs
        let mut by_subject: HashMap<i64, Vec<(String, String)>> = HashMap::new();
        for r in &relations {
            by_subject
                .entry(r.subject_id)
                .or_default()
                .push((r.predicate.clone(), r.learned_at.to_string()));
        }
        let mut seq_count: HashMap<(String, String), i64> = HashMap::new();
        for (_sid, mut preds) in by_subject {
            preds.sort_by(|a, b| a.1.cmp(&b.1));
            for w in preds.windows(2) {
                *seq_count
                    .entry((w[0].0.clone(), w[1].0.clone()))
                    .or_insert(0) += 1;
            }
        }
        let mut patterns = Vec::new();
        for ((p1, p2), count) in &seq_count {
            if *count >= min_freq as i64 && !is_generic_predicate(p1) && !is_generic_predicate(p2) {
                patterns.push(Pattern {
                    id: 0,
                    pattern_type: PatternType::TemporalSequence,
                    entities_involved: vec![p1.clone(), p2.clone()],
                    frequency: *count,
                    last_seen: now_str(),
                    description: format!(
                        "Predicate '{}' often followed by '{}' ({} times)",
                        p1, p2, count
                    ),
                });
            }
        }
        Ok(patterns)
    }

    /// Find predicate chain patterns: A→p1→B→p2→C suggests a compound relation A→(p1∘p2)→C.
    /// E.g., "Einstein" →born_in→ "Germany" →located_in→ "Europe" suggests "Einstein" associated_with "Europe".
    /// Returns patterns with the chain predicates and involved entities.
    pub fn find_predicate_chains(&self, min_freq: usize) -> Result<Vec<Pattern>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Build outgoing edges: entity_id → [(predicate, target_id)]
        let mut outgoing: HashMap<i64, Vec<(String, i64)>> = HashMap::new();
        for r in &relations {
            if meaningful.contains(&r.subject_id)
                && meaningful.contains(&r.object_id)
                && !is_generic_predicate(&r.predicate)
            {
                outgoing
                    .entry(r.subject_id)
                    .or_default()
                    .push((r.predicate.clone(), r.object_id));
            }
        }

        // Count predicate chain motifs: (p1, p2) → frequency
        let mut chain_count: HashMap<(String, String), Vec<(i64, i64, i64)>> = HashMap::new();
        for (&a, edges_a) in &outgoing {
            for (p1, b) in edges_a {
                if let Some(edges_b) = outgoing.get(b) {
                    for (p2, c) in edges_b {
                        if *c != a && p1 != p2 {
                            let key = (p1.clone(), p2.clone());
                            chain_count.entry(key).or_default().push((a, *b, *c));
                        }
                    }
                }
            }
        }

        let mut patterns = Vec::new();
        for ((p1, p2), instances) in &chain_count {
            if instances.len() >= min_freq {
                let example_names: Vec<String> = instances
                    .iter()
                    .take(3)
                    .filter_map(|(a, b, c)| {
                        let an = self.entity_name(*a).ok()?;
                        let bn = self.entity_name(*b).ok()?;
                        let cn = self.entity_name(*c).ok()?;
                        Some(format!("{}→{}→{}", an, bn, cn))
                    })
                    .collect();
                patterns.push(Pattern {
                    id: 0,
                    pattern_type: PatternType::FrequentSubgraph,
                    entities_involved: example_names,
                    frequency: instances.len() as i64,
                    last_seen: now_str(),
                    description: format!(
                        "Chain pattern ({} → {}) appears {} times — transitive relation candidate",
                        p1,
                        p2,
                        instances.len()
                    ),
                });
            }
        }
        patterns.sort_by(|a, b| b.frequency.cmp(&a.frequency));
        patterns.truncate(30);
        Ok(patterns)
    }

    /// Generate hypotheses from predicate chains: if A→p1→B→p2→C occurs frequently,
    /// propose that A is transitively related to C.
    pub fn generate_hypotheses_from_chains(&self) -> Result<Vec<Hypothesis>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        let mut outgoing: HashMap<i64, Vec<(String, i64)>> = HashMap::new();
        for r in &relations {
            if meaningful.contains(&r.subject_id)
                && meaningful.contains(&r.object_id)
                && !is_generic_predicate(&r.predicate)
            {
                outgoing
                    .entry(r.subject_id)
                    .or_default()
                    .push((r.predicate.clone(), r.object_id));
            }
        }

        // Build direct connection set
        let mut connected: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            connected.insert(key);
        }

        // Transitive predicate pairs that suggest compound relations
        let transitive_chains: &[(&str, &str, &str)] = &[
            ("born_in", "located_in", "associated_with"),
            ("located_in", "located_in", "located_in"),
            ("part_of", "part_of", "part_of"),
            ("member_of", "part_of", "associated_with"),
            ("works_at", "located_in", "associated_with"),
            ("founded_by", "born_in", "associated_with"),
            ("created_by", "member_of", "associated_with"),
            ("headquartered_in", "located_in", "operates_in"),
        ];

        let mut hypotheses = Vec::new();
        for (&a, edges_a) in &outgoing {
            for (p1, b) in edges_a {
                if let Some(edges_b) = outgoing.get(b) {
                    for (p2, c) in edges_b {
                        if *c == a {
                            continue;
                        }
                        let key = if a < *c { (a, *c) } else { (*c, a) };
                        if connected.contains(&key) {
                            continue;
                        }
                        // Check if this chain matches a known transitive pattern
                        for &(cp1, cp2, new_pred) in transitive_chains {
                            if p1 == cp1 && p2 == cp2 {
                                let a_name = self.entity_name(a)?;
                                let b_name = self.entity_name(*b)?;
                                let c_name = self.entity_name(*c)?;
                                hypotheses.push(Hypothesis {
                                    id: 0,
                                    subject: a_name.clone(),
                                    predicate: new_pred.to_string(),
                                    object: c_name.clone(),
                                    confidence: 0.5,
                                    evidence_for: vec![format!(
                                        "Chain: {} →{}→ {} →{}→ {}",
                                        a_name, p1, b_name, p2, c_name
                                    )],
                                    evidence_against: vec![],
                                    reasoning_chain: vec![
                                        format!("{} {} {}", a_name, p1, b_name),
                                        format!("{} {} {}", b_name, p2, c_name),
                                        format!(
                                            "Transitive chain ({} → {}) implies {} {} {}",
                                            p1, p2, a_name, new_pred, c_name
                                        ),
                                    ],
                                    status: HypothesisStatus::Proposed,
                                    discovered_at: now_str(),
                                    pattern_source: "predicate_chain".to_string(),
                                });
                                if hypotheses.len() >= 50 {
                                    return Ok(hypotheses);
                                }
                            }
                        }
                    }
                }
            }
        }
        Ok(hypotheses)
    }

    /// Find statistical anomalies — predicates that are surprisingly absent for certain entities.
    pub fn find_anomalies(&self) -> Result<Vec<Pattern>> {
        // For each entity_type, collect which predicates are common (>50%)
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut type_entities: HashMap<String, Vec<i64>> = HashMap::new();
        for e in &entities {
            if e.entity_type != "source" {
                type_entities
                    .entry(e.entity_type.clone())
                    .or_default()
                    .push(e.id);
            }
        }
        let mut entity_preds: HashMap<i64, HashSet<String>> = HashMap::new();
        for r in &relations {
            entity_preds
                .entry(r.subject_id)
                .or_default()
                .insert(r.predicate.clone());
        }
        let mut patterns = Vec::new();
        for (etype, eids) in &type_entities {
            if eids.len() < 3 {
                continue;
            }
            // Count predicate prevalence
            let mut pred_count: HashMap<String, usize> = HashMap::new();
            for eid in eids {
                if let Some(preds) = entity_preds.get(eid) {
                    for p in preds {
                        *pred_count.entry(p.clone()).or_insert(0) += 1;
                    }
                }
            }
            let threshold = (eids.len() as f64 * 0.5).ceil() as usize;
            for (pred, count) in &pred_count {
                if *count >= threshold {
                    // Find entities missing this predicate
                    for eid in eids {
                        let has = entity_preds.get(eid).is_some_and(|s| s.contains(pred));
                        if !has {
                            let name = self.entity_name(*eid)?;
                            patterns.push(Pattern {
                                id: 0,
                                pattern_type: PatternType::TypeGap,
                                entities_involved: vec![name.clone(), pred.clone(), etype.clone()],
                                frequency: 1,
                                last_seen: now_str(),
                                description: format!(
                                    "Entity '{}' of type '{}' lacks predicate '{}' which {}/{} peers have",
                                    name, etype, pred, count, eids.len()
                                ),
                            });
                            if patterns.len() >= 200 {
                                return Ok(patterns);
                            }
                        }
                    }
                }
            }
        }
        Ok(patterns)
    }

    // -----------------------------------------------------------------------
    // Gap Detection
    // -----------------------------------------------------------------------

    /// Find structural holes: A→B, A→C, B→D, C→D but B↛C.
    /// Filters to meaningful entity types only.
    pub fn find_structural_holes(&self) -> Result<Vec<(String, String)>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            // Include edge if at least one endpoint is meaningful
            if meaningful.contains(&r.subject_id) || meaningful.contains(&r.object_id) {
                adj.entry(r.subject_id).or_default().insert(r.object_id);
                adj.entry(r.object_id).or_default().insert(r.subject_id);
            }
        }
        let mut holes = Vec::new();
        let mut seen: HashSet<(i64, i64)> = HashSet::new();
        for (&a, a_nb) in &adj {
            // Skip high-degree nodes to avoid O(d²) blowup
            if a_nb.len() > 50 {
                continue;
            }
            let a_list: Vec<i64> = a_nb.iter().copied().collect();
            for i in 0..a_list.len() {
                for j in (i + 1)..a_list.len() {
                    let b = a_list[i];
                    let c = a_list[j];
                    let b_connected_c = adj.get(&b).is_some_and(|s| s.contains(&c));
                    if !b_connected_c {
                        // Check if B and C share another neighbour besides A
                        let b_nb = adj.get(&b).cloned().unwrap_or_default();
                        let c_nb = adj.get(&c).cloned().unwrap_or_default();
                        let shared: usize = b_nb.intersection(&c_nb).filter(|&&x| x != a).count();
                        if shared > 0 {
                            let key = if b < c { (b, c) } else { (c, b) };
                            if seen.insert(key) {
                                let b_name = self.entity_name(b)?;
                                let c_name = self.entity_name(c)?;
                                holes.push((b_name, c_name));
                            }
                        }
                    }
                }
            }
            if holes.len() >= 200 {
                break;
            }
        }
        Ok(holes)
    }

    /// Type-based gaps: entities of type X that lack a predicate Y which most peers have.
    pub fn find_type_gaps(&self) -> Result<Vec<(String, String, String)>> {
        let patterns = self.find_anomalies()?;
        let mut gaps = Vec::new();
        for p in patterns {
            if p.pattern_type == PatternType::TypeGap && p.entities_involved.len() >= 3 {
                gaps.push((
                    p.entities_involved[0].clone(), // entity
                    p.entities_involved[1].clone(), // predicate
                    p.entities_involved[2].clone(), // type
                ));
            }
        }
        Ok(gaps)
    }

    /// Analogy detection: "A is to B as C is to ?" — entities with parallel relation structures.
    pub fn find_analogies(&self) -> Result<Vec<(String, String, String, String, String)>> {
        let relations = self.brain.all_relations()?;
        // Group relations by predicate
        let mut by_pred: HashMap<String, Vec<(i64, i64)>> = HashMap::new();
        for r in &relations {
            by_pred
                .entry(r.predicate.clone())
                .or_default()
                .push((r.subject_id, r.object_id));
        }
        // For each entity pair sharing ≥2 predicates, look for analogy gaps
        let mut pair_preds: HashMap<(i64, i64), HashSet<String>> = HashMap::new();
        for (pred, pairs) in &by_pred {
            for &(s, o) in pairs {
                pair_preds.entry((s, o)).or_default().insert(pred.clone());
            }
        }
        let mut analogies = Vec::new();
        let mut pairs: Vec<((i64, i64), HashSet<String>)> = pair_preds
            .into_iter()
            .filter(|(_, preds)| preds.len() >= 2)
            .collect();
        // Sort by predicate count descending and cap to avoid O(n²) blowup
        pairs.sort_by(|a, b| b.1.len().cmp(&a.1.len()));
        pairs.truncate(500);
        for i in 0..pairs.len() {
            for j in (i + 1)..pairs.len() {
                let ((a, b), preds_ab) = &pairs[i];
                let ((c, d), preds_cd) = &pairs[j];
                if a == c || b == d || a == d || b == c {
                    continue;
                }
                let shared: HashSet<&String> = preds_ab.intersection(preds_cd).collect();
                let only_ab: Vec<&String> = preds_ab.difference(preds_cd).collect();
                let only_cd: Vec<&String> = preds_cd.difference(preds_ab).collect();
                if shared.len() >= 2 && (!only_ab.is_empty() || !only_cd.is_empty()) {
                    let a_name = self.entity_name(*a)?;
                    let b_name = self.entity_name(*b)?;
                    let c_name = self.entity_name(*c)?;
                    let d_name = self.entity_name(*d)?;
                    let missing = if !only_ab.is_empty() {
                        format!("{} may also {} {}", c_name, only_ab[0], d_name)
                    } else {
                        format!("{} may also {} {}", a_name, only_cd[0], b_name)
                    };
                    analogies.push((a_name, b_name, c_name, d_name, missing));
                    if analogies.len() >= 50 {
                        return Ok(analogies);
                    }
                }
            }
        }
        Ok(analogies)
    }

    // -----------------------------------------------------------------------
    // Hypothesis Engine
    // -----------------------------------------------------------------------

    /// Generate hypotheses from structural holes.
    pub fn generate_hypotheses_from_holes(&self) -> Result<Vec<Hypothesis>> {
        let holes = self.find_structural_holes()?;
        let mut hypotheses = Vec::new();
        for (b, c) in &holes {
            let h = Hypothesis {
                id: 0,
                subject: b.clone(),
                predicate: "related_to".to_string(),
                object: c.clone(),
                confidence: 0.4,
                evidence_for: vec![format!(
                    "Structural hole: {} and {} share common neighbours but are not directly connected",
                    b, c
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!("Found structural hole between {} and {}", b, c),
                    "Both entities share common neighbours".to_string(),
                    "Missing direct link suggests potential relation".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "structural_hole".to_string(),
            };
            hypotheses.push(h);
        }
        Ok(hypotheses)
    }

    /// Generate hypotheses from type-based gaps.
    /// Instead of generating unfalsifiable "object=?" hypotheses, we try to find
    /// a plausible object by looking at what other entities of the same type connect to
    /// via the missing predicate.
    pub fn generate_hypotheses_from_type_gaps(&self) -> Result<Vec<Hypothesis>> {
        let gaps = self.find_type_gaps()?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Build predicate→object mapping from existing relations for same-type entities
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();
        let mut type_pred_objects: HashMap<(String, String), Vec<(i64, String)>> = HashMap::new();
        for r in &relations {
            if meaningful.contains(&r.subject_id) && meaningful.contains(&r.object_id) {
                if let Some(&etype) = id_to_type.get(&r.subject_id) {
                    type_pred_objects
                        .entry((etype.to_string(), r.predicate.clone()))
                        .or_default()
                        .push((
                            r.object_id,
                            self.entity_name(r.object_id).unwrap_or_default(),
                        ));
                }
            }
        }

        let mut hypotheses = Vec::new();
        for (entity, predicate, etype) in gaps.iter().take(100) {
            // Find the most common object for this (type, predicate) pair
            let key = (etype.clone(), predicate.clone());
            if let Some(objects) = type_pred_objects.get(&key) {
                // Count object frequency
                let mut obj_freq: HashMap<&str, usize> = HashMap::new();
                for (_, name) in objects {
                    *obj_freq.entry(name.as_str()).or_insert(0) += 1;
                }
                // Pick the most common object (if it appears in >30% of cases)
                let total = objects.len();
                if let Some((&best_obj, &count)) = obj_freq.iter().max_by_key(|(_, &c)| c) {
                    if count as f64 / total as f64 > 0.3
                        && !best_obj.is_empty()
                        && best_obj != entity
                    {
                        hypotheses.push(Hypothesis {
                            id: 0,
                            subject: entity.clone(),
                            predicate: predicate.clone(),
                            object: best_obj.to_string(),
                            confidence: 0.45,
                            evidence_for: vec![format!(
                                "{}/{} '{}' entities with '{}' point to '{}'",
                                count, total, etype, predicate, best_obj
                            )],
                            evidence_against: vec![],
                            reasoning_chain: vec![
                                format!("Entity '{}' is of type '{}'", entity, etype),
                                format!(
                                    "Most '{}' entities have '{}' → '{}'",
                                    etype, predicate, best_obj
                                ),
                                format!("'{}' likely also {} '{}'", entity, predicate, best_obj),
                            ],
                            status: HypothesisStatus::Proposed,
                            discovered_at: now_str(),
                            pattern_source: "type_gap".to_string(),
                        });
                    }
                }
            }
        }
        Ok(hypotheses)
    }

    /// Generate hypotheses from shared-object patterns: if A→pred→X and B→pred→X,
    /// maybe A and B are related.
    /// Filters out generic predicates and caps per-entity generation to avoid hub explosion.
    pub fn generate_hypotheses_from_shared_objects(&self) -> Result<Vec<Hypothesis>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;
        // Group by (predicate, object_id) → list of subject_ids
        // Only require subjects to be meaningful (objects can be any type)
        // Skip generic predicates that create too many spurious connections
        let skip_preds: HashSet<&str> = [
            "contributed_to",
            "associated_with",
            "references",
            "related_concept",
            "relevant_to",
            "works_on",
            "related_to",
            "is",
            "has",
            "was",
            "includes",
            "contains",
            "involves",
            "features",
            "describes",
        ]
        .into_iter()
        .collect();

        // Specific predicates that imply a meaningful peer relationship
        let peer_predicates: HashMap<&str, &str> = [
            ("born_in", "share_birthplace"),
            ("located_in", "co_located_in"),
            ("member_of", "co_member_of"),
            ("founded", "co_founded"),
            ("studied_at", "co_studied_at"),
            ("works_at", "co_employed_at"),
            ("developed", "co_developed"),
            ("invented", "co_invented"),
            ("discovered", "co_discovered"),
            ("published_in", "co_published_in"),
            ("headquartered_in", "co_headquartered_in"),
        ]
        .into_iter()
        .collect();

        let mut groups: HashMap<(String, i64), Vec<i64>> = HashMap::new();
        for r in &relations {
            if meaningful.contains(&r.subject_id)
                && !skip_preds.contains(r.predicate.as_str())
                && r.confidence >= 0.5
            {
                groups
                    .entry((r.predicate.clone(), r.object_id))
                    .or_default()
                    .push(r.subject_id);
            }
        }
        let mut hypotheses = Vec::new();
        for ((pred, obj_id), subjects) in &groups {
            // Require 2-5 subjects sharing the same object (>5 is too generic)
            if subjects.len() < 2 || subjects.len() > 5 {
                continue;
            }
            let obj_name = self.entity_name(*obj_id)?;

            // Only use explicitly mapped peer predicates — unmapped ones had
            // a 3% confirmation rate and generate too much noise.
            let inferred_pred = match peer_predicates.get(pred.as_str()) {
                Some(&p) => p,
                None => continue, // skip unmapped predicates entirely
            };

            let base_conf = 0.60; // known peer relationship

            // Pre-fetch entity types for type compatibility filtering
            let subj_types: Vec<(i64, String)> = subjects
                .iter()
                .filter_map(|&sid| {
                    self.brain
                        .get_entity_by_id(sid)
                        .ok()
                        .flatten()
                        .map(|e| (sid, e.entity_type))
                })
                .collect();

            // For each pair of subjects, propose a relationship
            for i in 0..subj_types.len().min(4) {
                for j in (i + 1)..subj_types.len().min(4) {
                    let (sid_a, ref type_a) = subj_types[i];
                    let (sid_b, ref type_b) = subj_types[j];

                    // Type compatibility: both subjects should be the same type
                    // or a compatible pair (person-person, org-org, place-place)
                    // Mixed type pairs (e.g. person + concept) rarely confirm
                    if type_a != type_b {
                        let compatible_pairs: HashSet<(&str, &str)> =
                            [("person", "organization"), ("organization", "person")]
                                .into_iter()
                                .collect();
                        if !compatible_pairs.contains(&(type_a.as_str(), type_b.as_str())) {
                            continue;
                        }
                    }

                    let a = self.entity_name(sid_a)?;
                    let b = self.entity_name(sid_b)?;

                    // Skip if inferred predicate is too vague and confidence too low
                    if inferred_pred == "related_to" {
                        continue;
                    }

                    // Require at least 1 additional shared neighbor beyond the
                    // shared object — pure shared-object alone has a 3% confirm rate
                    let a_rels = self.brain.get_relations_for(sid_a)?;
                    let b_rels = self.brain.get_relations_for(sid_b)?;
                    let a_nbrs: HashSet<String> = a_rels
                        .iter()
                        .flat_map(|(sn, _, on, _)| [sn.clone(), on.clone()])
                        .collect();
                    let b_nbrs: HashSet<String> = b_rels
                        .iter()
                        .flat_map(|(sn, _, on, _)| [sn.clone(), on.clone()])
                        .collect();
                    let extra_shared = a_nbrs
                        .intersection(&b_nbrs)
                        .filter(|n| *n != &a && *n != &b && *n != &obj_name)
                        .count();
                    if extra_shared == 0 {
                        continue;
                    }

                    let boosted_conf = (base_conf + 0.05 * extra_shared.min(3) as f64).min(0.85);

                    hypotheses.push(Hypothesis {
                        id: 0,
                        subject: a.clone(),
                        predicate: inferred_pred.to_string(),
                        object: b.clone(),
                        confidence: boosted_conf,
                        evidence_for: vec![format!(
                            "Both {} and {} share '{}' relationship to '{}'",
                            a, b, pred, obj_name
                        )],
                        evidence_against: vec![],
                        reasoning_chain: vec![
                            format!("{} {} {}", a, pred, obj_name),
                            format!("{} {} {}", b, pred, obj_name),
                            format!(
                                "Shared '{}' to '{}' implies {} relationship",
                                pred, obj_name, inferred_pred
                            ),
                        ],
                        status: HypothesisStatus::Proposed,
                        discovered_at: now_str(),
                        pattern_source: "shared_object".to_string(),
                    });
                }
            }
        }
        Ok(hypotheses)
    }

    /// Generate hypotheses from source co-occurrence: entities extracted from the same
    /// source page likely have a semantic relationship.
    pub fn generate_hypotheses_from_source_co_occurrence(&self) -> Result<Vec<Hypothesis>> {
        let source_patterns = self.find_source_co_occurrences()?;
        let mut hypotheses = Vec::new();
        for p in source_patterns.iter().take(30) {
            if p.entities_involved.len() >= 2 {
                let a = &p.entities_involved[0];
                let b = &p.entities_involved[1];
                hypotheses.push(Hypothesis {
                    id: 0,
                    subject: a.clone(),
                    predicate: "related_to".to_string(),
                    object: b.clone(),
                    confidence: 0.35 + (p.frequency as f64 * 0.1).min(0.3),
                    evidence_for: vec![format!(
                        "Co-extracted from {} shared source(s)",
                        p.frequency
                    )],
                    evidence_against: vec![],
                    reasoning_chain: vec![
                        format!("{} and {} appear in the same source document(s)", a, b),
                        "Co-occurrence in source material suggests semantic relation".to_string(),
                    ],
                    status: HypothesisStatus::Proposed,
                    discovered_at: now_str(),
                    pattern_source: "source_co_occurrence".to_string(),
                });
            }
        }
        Ok(hypotheses)
    }

    /// Analyze predicate diversity — identify over-reliance on generic predicates.
    /// Returns (total_rels, generic_count, diverse_count, diversity_ratio).
    pub fn predicate_diversity(&self) -> Result<(usize, usize, usize, f64)> {
        let relations = self.brain.all_relations()?;
        let total = relations.len();
        let generic = relations
            .iter()
            .filter(|r| is_generic_predicate(&r.predicate))
            .count();
        let diverse = total - generic;
        let ratio = if total > 0 {
            diverse as f64 / total as f64
        } else {
            0.0
        };
        Ok((total, generic, diverse, ratio))
    }

    /// Generate hypotheses from analogy patterns (A:B :: C:? ).
    pub fn generate_hypotheses_from_analogies(&self) -> Result<Vec<Hypothesis>> {
        let analogies = self.find_analogies()?;
        let mut hypotheses = Vec::new();
        for (a, b, c, d, missing) in analogies.iter().take(20) {
            hypotheses.push(Hypothesis {
                id: 0,
                subject: c.clone(),
                predicate: "analogous_to".to_string(),
                object: format!("{} (via {}-{} analogy)", missing, a, b),
                confidence: 0.35,
                evidence_for: vec![format!(
                    "Analogy: {} is to {} as {} is to {} — {}",
                    a, b, c, d, missing
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} and {} share relation structure with {} and {}",
                        a, b, c, d
                    ),
                    format!("Gap suggests: {}", missing),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "analogy".to_string(),
            });
        }
        Ok(hypotheses)
    }

    /// Find island entities: meaningful entities with zero relations (knowledge gaps).
    pub fn find_island_entities(&self) -> Result<Vec<(String, String)>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }
        let mut islands = Vec::new();
        for e in &entities {
            if !is_noise_type(&e.entity_type) && !connected.contains(&e.id) {
                islands.push((e.name.clone(), e.entity_type.clone()));
            }
        }
        Ok(islands)
    }

    /// Check if a hypothesis contradicts known facts.
    pub fn check_contradiction(&self, hypothesis: &Hypothesis) -> Result<bool> {
        // Check if the exact opposite relation exists
        let subj = self.brain.get_entity_by_name(&hypothesis.subject)?;
        let obj = self.brain.get_entity_by_name(&hypothesis.object)?;
        if let (Some(s), Some(_o)) = (subj, obj) {
            let rels = self.brain.get_relations_for(s.id)?;
            for (sname, pred, oname, _conf) in &rels {
                // Check for contradiction predicates
                let contradicts = is_contradicting_predicate(&hypothesis.predicate, pred);
                if contradicts
                    && ((sname == &hypothesis.subject && oname == &hypothesis.object)
                        || (sname == &hypothesis.object && oname == &hypothesis.subject))
                {
                    return Ok(true);
                }
            }
            // Check singleton facts
            let facts = self.brain.get_facts_for(s.id)?;
            for f in &facts {
                if f.key == hypothesis.predicate && f.value != hypothesis.object && f.value != "?" {
                    return Ok(true);
                }
            }
        }
        Ok(false)
    }

    /// Score a hypothesis based on available evidence, community co-membership,
    /// and k-core depth.
    pub fn score_hypothesis(&self, hypothesis: &mut Hypothesis) -> Result<f64> {
        let mut score = 0.5_f64;
        // Boost for more evidence_for
        score += hypothesis.evidence_for.len() as f64 * 0.1;
        // Penalty for evidence_against
        score -= hypothesis.evidence_against.len() as f64 * 0.15;
        // Check for contradiction
        if self.check_contradiction(hypothesis)? {
            score -= 0.3;
            hypothesis
                .evidence_against
                .push("Contradicts existing knowledge".to_string());
        }

        // Note: community and k-core boosting is done in batch via
        // boost_hypotheses_with_graph_structure() to avoid recomputing per-hypothesis.

        // Predicate diversity bonus: hypotheses introducing rare predicates
        // get a boost to counteract graph homogeneity (e.g., contemporary_of dominance).
        let pred_count: i64 = self.brain.with_conn(|conn| {
            conn.query_row(
                "SELECT COUNT(*) FROM relations WHERE predicate = ?1",
                params![hypothesis.predicate],
                |row| row.get(0),
            )
        })?;
        let total_rels: i64 = self.brain.with_conn(|conn| {
            conn.query_row("SELECT COUNT(*) FROM relations", [], |row| row.get(0))
        })?;
        if total_rels > 0 {
            let pred_freq = pred_count as f64 / total_rels as f64;
            // Predicates appearing in <5% of relations get a small bonus
            if pred_freq < 0.05 {
                score += 0.05;
            } else if pred_freq > 0.35 {
                // Heavily saturated predicates (>35% of graph) get a stronger penalty
                // to force discovery of more diverse relationship types.
                score -= 0.08;
            } else if pred_freq > 0.25 {
                // Dominant predicates get a slight penalty to promote diversity
                score -= 0.03;
            }
        }

        // Entity degree signal: hypotheses linking well-connected entities
        // are more trustworthy than those linking peripheral ones.
        if hypothesis.object != "?" {
            if let (Some(s_ent), Some(o_ent)) = (
                self.brain.get_entity_by_name(&hypothesis.subject)?,
                self.brain.get_entity_by_name(&hypothesis.object)?,
            ) {
                let s_deg: i64 = self.brain.with_conn(|conn| {
                    conn.query_row(
                        "SELECT COUNT(*) FROM relations WHERE subject_id = ?1 OR object_id = ?1",
                        params![s_ent.id],
                        |row| row.get(0),
                    )
                })?;
                let o_deg: i64 = self.brain.with_conn(|conn| {
                    conn.query_row(
                        "SELECT COUNT(*) FROM relations WHERE subject_id = ?1 OR object_id = ?1",
                        params![o_ent.id],
                        |row| row.get(0),
                    )
                })?;
                let min_deg = s_deg.min(o_deg);
                if min_deg >= 5 {
                    // Both entities are well-connected — more reliable hypothesis
                    score += 0.08;
                } else if min_deg >= 2 {
                    score += 0.03;
                } else if min_deg == 0 {
                    // One entity is isolated — hypothesis is speculative
                    score -= 0.05;
                }

                // Shared-neighbor boost: entities with common neighbors are more
                // likely to be genuinely related (common-neighbors link predictor).
                let shared_neighbors: i64 = self.brain.with_conn(|conn| {
                    conn.query_row(
                        "SELECT COUNT(DISTINCT n.id) FROM (
                            SELECT object_id AS id FROM relations WHERE subject_id = ?1
                            UNION SELECT subject_id FROM relations WHERE object_id = ?1
                        ) n
                        INNER JOIN (
                            SELECT object_id AS id FROM relations WHERE subject_id = ?2
                            UNION SELECT subject_id FROM relations WHERE object_id = ?2
                        ) m ON n.id = m.id",
                        params![s_ent.id, o_ent.id],
                        |row| row.get(0),
                    )
                })?;
                if shared_neighbors >= 5 {
                    score += 0.12;
                    hypothesis.evidence_for.push(format!(
                        "{} shared neighbors between entities",
                        shared_neighbors
                    ));
                } else if shared_neighbors >= 2 {
                    score += 0.06;
                }

                // Entity recency momentum: recently-accessed entities are more
                // relevant. Entities with high access_count relative to their age
                // indicate active knowledge areas worth investing in.
                let max_access = s_ent.access_count.max(o_ent.access_count);
                let now = Utc::now().naive_utc();
                let newest_age_days = (now - s_ent.last_seen)
                    .num_days()
                    .min((now - o_ent.last_seen).num_days())
                    .max(1);
                let momentum = max_access as f64 / newest_age_days as f64;
                if momentum > 2.0 {
                    score += 0.06; // high-momentum entities
                } else if momentum > 0.5 {
                    score += 0.02;
                }
            }
        }

        // Source diversity boost: entities corroborated by multiple distinct
        // source URLs are more reliable — independent verification principle.
        if hypothesis.object != "?" {
            if let (Some(s_ent), Some(o_ent)) = (
                self.brain.get_entity_by_name(&hypothesis.subject)?,
                self.brain.get_entity_by_name(&hypothesis.object)?,
            ) {
                let source_count = |eid: i64| -> i64 {
                    self.brain
                        .with_conn(|conn| {
                            conn.query_row(
                                "SELECT COUNT(DISTINCT source_url) FROM relations
                                 WHERE (subject_id = ?1 OR object_id = ?1) AND source_url IS NOT NULL AND source_url != ''",
                                params![eid],
                                |row| row.get(0),
                            )
                        })
                        .unwrap_or(0)
                };
                let s_sources = source_count(s_ent.id);
                let o_sources = source_count(o_ent.id);
                let min_sources = s_sources.min(o_sources);
                if min_sources >= 5 {
                    score += 0.08; // strongly corroborated entities
                } else if min_sources >= 3 {
                    score += 0.04;
                }
            }
        }

        // Apply pattern weight
        let weight = self.get_pattern_weight(&hypothesis.pattern_source)?;
        score *= weight;
        score = score.clamp(0.0, 1.0);
        hypothesis.confidence = score;
        Ok(score)
    }

    // -----------------------------------------------------------------------
    // Persistence
    // -----------------------------------------------------------------------

    pub fn save_hypothesis(&self, h: &Hypothesis) -> Result<i64> {
        self.brain.with_conn(|conn| {
            conn.execute(
                "INSERT INTO hypotheses (subject, predicate, object, confidence, evidence_for, evidence_against, reasoning_chain, status, discovered_at, pattern_source)
                 VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8, ?9, ?10)",
                params![
                    h.subject,
                    h.predicate,
                    h.object,
                    h.confidence,
                    serde_json::to_string(&h.evidence_for).unwrap_or_default(),
                    serde_json::to_string(&h.evidence_against).unwrap_or_default(),
                    serde_json::to_string(&h.reasoning_chain).unwrap_or_default(),
                    h.status.as_str(),
                    h.discovered_at,
                    h.pattern_source,
                ],
            )?;
            Ok(conn.last_insert_rowid())
        })
    }

    pub fn save_pattern(&self, p: &Pattern) -> Result<i64> {
        self.brain.with_conn(|conn| {
            conn.execute(
                "INSERT INTO patterns (pattern_type, entities_involved, frequency, last_seen, description)
                 VALUES (?1, ?2, ?3, ?4, ?5)",
                params![
                    p.pattern_type.as_str(),
                    serde_json::to_string(&p.entities_involved).unwrap_or_default(),
                    p.frequency,
                    p.last_seen,
                    p.description,
                ],
            )?;
            Ok(conn.last_insert_rowid())
        })
    }

    pub fn get_hypothesis(&self, id: i64) -> Result<Option<Hypothesis>> {
        self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT id, subject, predicate, object, confidence, evidence_for, evidence_against, reasoning_chain, status, discovered_at, pattern_source
                 FROM hypotheses WHERE id = ?1",
            )?;
            let mut rows = stmt.query_map(params![id], |row| {
                Ok(parse_hypothesis_row(row))
            })?;
            match rows.next() {
                Some(Ok(h)) => Ok(Some(h)),
                _ => Ok(None),
            }
        })
    }

    pub fn list_hypotheses(&self, status: Option<HypothesisStatus>) -> Result<Vec<Hypothesis>> {
        self.brain.with_conn(|conn| {
            let (sql, param): (String, Option<String>) = match status {
                Some(s) => (
                    "SELECT id, subject, predicate, object, confidence, evidence_for, evidence_against, reasoning_chain, status, discovered_at, pattern_source FROM hypotheses WHERE status = ?1 ORDER BY confidence DESC".to_string(),
                    Some(s.as_str().to_string()),
                ),
                None => (
                    "SELECT id, subject, predicate, object, confidence, evidence_for, evidence_against, reasoning_chain, status, discovered_at, pattern_source FROM hypotheses ORDER BY confidence DESC".to_string(),
                    None,
                ),
            };
            let mut stmt = conn.prepare(&sql)?;
            let rows = if let Some(ref p) = param {
                stmt.query_map(params![p], |row| Ok(parse_hypothesis_row(row)))?
                    .collect::<Result<Vec<_>>>()?
            } else {
                stmt.query_map([], |row| Ok(parse_hypothesis_row(row)))?
                    .collect::<Result<Vec<_>>>()?
            };
            Ok(rows)
        })
    }

    pub fn update_hypothesis_status(&self, id: i64, status: HypothesisStatus) -> Result<()> {
        self.brain.with_conn(|conn| {
            conn.execute(
                "UPDATE hypotheses SET status = ?1 WHERE id = ?2",
                params![status.as_str(), id],
            )?;
            Ok(())
        })
    }

    pub fn save_discovery(&self, hypothesis_id: i64, evidence_sources: &[String]) -> Result<i64> {
        self.brain.with_conn(|conn| {
            conn.execute(
                "INSERT INTO discoveries (hypothesis_id, confirmed_at, evidence_sources) VALUES (?1, ?2, ?3)",
                params![
                    hypothesis_id,
                    now_str(),
                    serde_json::to_string(evidence_sources).unwrap_or_default(),
                ],
            )?;
            Ok(conn.last_insert_rowid())
        })
    }

    // -----------------------------------------------------------------------
    // Validation
    // -----------------------------------------------------------------------

    /// Validate a hypothesis against existing knowledge.
    pub fn validate_hypothesis(&self, h: &mut Hypothesis) -> Result<()> {
        // Strategies that already use neighbor overlap as their primary signal —
        // don't double-count shared neighbors for these.
        const NEIGHBOR_BASED_STRATEGIES: &[&str] = &[
            "community_bridge",
            "resource_allocation",
            "neighborhood_overlap",
            "jaccard",
            "adamic_adar",
            "triadic_closure",
            "near_miss",
            "semantic_fingerprint",
            "rising_star",
            "preferential_attachment",
        ];
        let skip_neighbor_boost = NEIGHBOR_BASED_STRATEGIES.contains(&h.pattern_source.as_str());

        // Track cumulative boost to apply diminishing returns — each successive
        // piece of evidence contributes less (prevents confidence inflation).
        let base_confidence = h.confidence;
        let mut total_boost: f64 = 0.0;
        let apply_boost = |total: &mut f64, amount: f64| -> f64 {
            // Diminishing returns: each boost is scaled by (1 - total_boost_so_far)
            // First boost applies fully, subsequent boosts contribute less
            let headroom = (1.0 - base_confidence - *total).max(0.0);
            let effective = (amount * (1.0 - *total * 0.5)).min(headroom);
            *total += effective;
            effective
        };

        // Search for supporting/contradicting evidence in the graph
        let subj = self.brain.search_entities(&h.subject)?;
        let _obj = if h.object != "?" {
            self.brain.search_entities(&h.object)?
        } else {
            vec![]
        };

        // Check if the relation already exists (confirms the hypothesis)
        let mut direct_evidence = false;
        for s in &subj {
            let rels = self.brain.get_relations_for(s.id)?;
            for (sname, pred, oname, conf) in &rels {
                if (pred == &h.predicate || predicates_similar(pred, &h.predicate))
                    && (h.object == "?" || oname == &h.object || sname == &h.object)
                {
                    h.evidence_for.push(format!(
                        "Found relation: {} {} {} (confidence: {:.2})",
                        sname, pred, oname, conf
                    ));
                    let boost = apply_boost(&mut total_boost, 0.2);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                    let _ = boost; // suppress unused warning
                    direct_evidence = true;
                }
            }
        }

        // Check for shared neighbors (indirect evidence of relatedness)
        // Skip this boost for strategies that already used neighbor overlap
        if h.object != "?" && !skip_neighbor_boost {
            if let (Some(s_ent), Some(o_ent)) = (
                self.brain.get_entity_by_name(&h.subject)?,
                self.brain.get_entity_by_name(&h.object)?,
            ) {
                let s_rels = self.brain.get_relations_for(s_ent.id)?;
                let o_rels = self.brain.get_relations_for(o_ent.id)?;
                let s_neighbors: HashSet<String> = s_rels
                    .iter()
                    .flat_map(|(sn, _, on, _)| [sn.clone(), on.clone()])
                    .collect();
                let o_neighbors: HashSet<String> = o_rels
                    .iter()
                    .flat_map(|(sn, _, on, _)| [sn.clone(), on.clone()])
                    .collect();
                let shared: Vec<&String> = s_neighbors
                    .intersection(&o_neighbors)
                    .filter(|n| *n != &h.subject && *n != &h.object)
                    .collect();
                if shared.len() >= 2 {
                    h.evidence_for.push(format!(
                        "Shared {} neighbors: {}",
                        shared.len(),
                        shared
                            .iter()
                            .take(3)
                            .map(|s| s.as_str())
                            .collect::<Vec<_>>()
                            .join(", ")
                    ));
                    let raw = 0.05 * shared.len().min(4) as f64; // cap at 4 neighbors
                    apply_boost(&mut total_boost, raw);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                }
            }
        }

        // For neighbor-based strategies without direct evidence, look for
        // independent corroboration: shared facts or co-occurrence in source URLs
        if skip_neighbor_boost && !direct_evidence && h.object != "?" {
            if let (Some(s_ent), Some(o_ent)) = (
                self.brain.get_entity_by_name(&h.subject)?,
                self.brain.get_entity_by_name(&h.object)?,
            ) {
                // Fact overlap: same key-value pairs across entities
                let s_facts = self.brain.get_facts_for(s_ent.id)?;
                let o_facts = self.brain.get_facts_for(o_ent.id)?;
                let s_keys: HashSet<&str> = s_facts.iter().map(|f| f.key.as_str()).collect();
                let o_keys: HashSet<&str> = o_facts.iter().map(|f| f.key.as_str()).collect();
                let shared_keys = s_keys.intersection(&o_keys).count();

                // Stronger signal: count matching key-value pairs (not just keys)
                let s_kv: HashSet<(&str, &str)> = s_facts
                    .iter()
                    .map(|f| (f.key.as_str(), f.value.as_str()))
                    .collect();
                let o_kv: HashSet<(&str, &str)> = o_facts
                    .iter()
                    .map(|f| (f.key.as_str(), f.value.as_str()))
                    .collect();
                let shared_kv = s_kv.intersection(&o_kv).count();

                if shared_kv >= 1 {
                    // Exact key-value match is strong evidence (same field, same nationality, etc.)
                    h.evidence_for
                        .push(format!("Shared {} exact fact key-value pairs", shared_kv));
                    apply_boost(&mut total_boost, 0.08 * shared_kv.min(3) as f64);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                } else if shared_keys >= 1 {
                    h.evidence_for
                        .push(format!("Shared {} fact keys across entities", shared_keys));
                    apply_boost(&mut total_boost, 0.05 * shared_keys.min(3) as f64);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                }

                // Source co-occurrence: both entities appear in the same source URL
                let s_sources: HashSet<String> = self
                    .brain
                    .get_source_urls_for(s_ent.id)?
                    .into_iter()
                    .collect();
                let o_sources: HashSet<String> = self
                    .brain
                    .get_source_urls_for(o_ent.id)?
                    .into_iter()
                    .collect();
                let shared_sources = s_sources.intersection(&o_sources).count();
                if shared_sources >= 1 {
                    h.evidence_for
                        .push(format!("Co-occur in {} source URL(s)", shared_sources));
                    apply_boost(&mut total_boost, 0.03 * shared_sources.min(3) as f64);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                }

                // Temporal proximity: entities first seen within 24h of each other
                // are likely related (extracted from the same crawl session / topic)
                {
                    let delta = (s_ent.first_seen - o_ent.first_seen)
                        .num_hours()
                        .unsigned_abs();
                    if delta <= 1 {
                        h.evidence_for
                            .push("Entities discovered within 1 hour of each other".to_string());
                        apply_boost(&mut total_boost, 0.08);
                        h.confidence = (base_confidence + total_boost).min(1.0);
                    } else if delta <= 24 {
                        h.evidence_for
                            .push("Entities discovered within 24 hours of each other".to_string());
                        apply_boost(&mut total_boost, 0.03);
                        h.confidence = (base_confidence + total_boost).min(1.0);
                    }
                }
            }
        }

        // Check for contradictions
        if self.check_contradiction(h)? {
            h.evidence_against
                .push("Contradicts existing knowledge".to_string());
            h.confidence = (h.confidence - 0.3).max(0.0);
        }

        // Path-distance evidence: if a short path (2-3 hops) exists between
        // subject and object, that's independent structural evidence of relatedness.
        // Uses bounded BFS (max 3 hops) for efficiency — avoids full graph traversal.
        if h.object != "?" && !direct_evidence {
            if let Ok(Some(path)) =
                crate::graph::shortest_path_bounded(self.brain, &h.subject, &h.object, 3)
            {
                let hops = path.len().saturating_sub(1);
                if hops == 2 {
                    h.evidence_for
                        .push("Connected via 2-hop path through graph".to_string());
                    apply_boost(&mut total_boost, 0.10);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                } else if hops == 3 {
                    h.evidence_for
                        .push("Connected via 3-hop path through graph".to_string());
                    apply_boost(&mut total_boost, 0.05);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                }
                // 1-hop means already directly connected — handled by direct evidence above
            }
        }

        // Type compatibility check: penalize hypotheses linking incompatible entity types
        if h.object != "?" {
            if let (Some(s_ent), Some(o_ent)) = (
                self.brain.get_entity_by_name(&h.subject)?,
                self.brain.get_entity_by_name(&h.object)?,
            ) {
                let incompatible =
                    is_type_incompatible(&s_ent.entity_type, &o_ent.entity_type, &h.predicate);
                if incompatible {
                    h.evidence_against.push(format!(
                        "Type mismatch: {} ({}) → {} → {} ({})",
                        h.subject, s_ent.entity_type, h.predicate, h.object, o_ent.entity_type
                    ));
                    h.confidence = (h.confidence - 0.2).max(0.0);
                }
            }
        }

        // Staleness penalty: entities not seen recently are less reliable subjects
        if h.object != "?" {
            if let (Some(s_ent), Some(o_ent)) = (
                self.brain.get_entity_by_name(&h.subject)?,
                self.brain.get_entity_by_name(&h.object)?,
            ) {
                let now = Utc::now().naive_utc();
                let s_age_days = (now - s_ent.last_seen).num_days();
                let o_age_days = (now - o_ent.last_seen).num_days();
                let max_age = s_age_days.max(o_age_days);
                if max_age > 90 {
                    h.evidence_against
                        .push(format!("Entity staleness: {}d since last seen", max_age));
                    h.confidence = (h.confidence - 0.08).max(0.0);
                } else if max_age > 30 {
                    h.confidence = (h.confidence - 0.03).max(0.0);
                }
            }
        }

        // Fragmentation-reduction bonus: hypotheses that would connect isolated
        // entities to the main graph get a confidence boost, incentivizing the
        // discovery engine to reduce the 89%+ fragmentation rate.
        if h.object != "?" {
            if let (Some(s_ent), Some(o_ent)) = (
                self.brain.get_entity_by_name(&h.subject)?,
                self.brain.get_entity_by_name(&h.object)?,
            ) {
                let s_rels = self.brain.get_relations_for(s_ent.id)?;
                let o_rels = self.brain.get_relations_for(o_ent.id)?;
                let s_isolated = s_rels.is_empty();
                let o_isolated = o_rels.is_empty();
                if s_isolated && o_isolated {
                    // Both isolated: connecting them creates a new component but
                    // reduces island count by 2 — moderate boost
                    h.evidence_for.push(
                        "Both entities are isolated — connection reduces fragmentation".to_string(),
                    );
                    apply_boost(&mut total_boost, 0.06);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                } else if s_isolated || o_isolated {
                    // One isolated: connecting to a non-isolated entity integrates
                    // it into the graph — stronger boost (reduces islands by 1)
                    h.evidence_for.push(
                        "Connects isolated entity to graph — reduces fragmentation".to_string(),
                    );
                    apply_boost(&mut total_boost, 0.10);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                }
            }
        }

        // Entity importance signal: highly accessed entities with rich facts
        // represent well-established knowledge; boost hypotheses involving them.
        if h.object != "?" {
            if let (Some(s_ent), Some(o_ent)) = (
                self.brain.get_entity_by_name(&h.subject)?,
                self.brain.get_entity_by_name(&h.object)?,
            ) {
                let combined_access = s_ent.access_count + o_ent.access_count;
                let s_facts = self.brain.get_facts_for(s_ent.id)?.len();
                let o_facts = self.brain.get_facts_for(o_ent.id)?.len();
                let fact_richness = s_facts + o_facts;
                // Only boost if entities are genuinely important (not random noise)
                if combined_access >= 8 && fact_richness >= 4 {
                    h.evidence_for.push(format!(
                        "High-importance entities: {} accesses, {} facts",
                        combined_access, fact_richness
                    ));
                    apply_boost(&mut total_boost, 0.07);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                } else if combined_access >= 4 && fact_richness >= 2 {
                    apply_boost(&mut total_boost, 0.03);
                    h.confidence = (base_confidence + total_boost).min(1.0);
                }
            }
        }

        // Cross-pattern corroboration: if other hypotheses from different
        // pattern sources proposed the same (subject, predicate, object) triple,
        // that's independent evidence — boost confidence proportional to the
        // number of corroborating sources.
        if h.object != "?" && h.id > 0 {
            let corroborating: usize = self
                .brain
                .with_conn(|conn| {
                    conn.query_row(
                        "SELECT COUNT(DISTINCT pattern_source) FROM hypotheses
                     WHERE subject = ?1 AND predicate = ?2 AND object = ?3
                     AND id != ?4 AND pattern_source != ?5",
                        params![h.subject, h.predicate, h.object, h.id, h.pattern_source],
                        |row| row.get(0),
                    )
                })
                .unwrap_or(0);
            if corroborating >= 2 {
                h.evidence_for.push(format!(
                    "Cross-pattern corroboration: {} independent pattern sources agree",
                    corroborating
                ));
                apply_boost(&mut total_boost, 0.12);
                h.confidence = (base_confidence + total_boost).min(1.0);
            } else if corroborating == 1 {
                h.evidence_for
                    .push("Cross-pattern corroboration: 1 independent source agrees".to_string());
                apply_boost(&mut total_boost, 0.06);
                h.confidence = (base_confidence + total_boost).min(1.0);
            }
        }

        // Predicate specificity: specific predicates (born_in, invented, founded)
        // are more reliable signals than vague ones (related_to, associated_with).
        // Penalize vague predicates to reduce false confirmations.
        {
            const VAGUE_PREDICATES: &[&str] = &[
                "related_to",
                "associated_with",
                "connected_to",
                "linked_to",
                "relevant_to",
                "contemporary_of",
            ];
            const SPECIFIC_PREDICATES: &[&str] = &[
                "born_in",
                "founded",
                "invented",
                "discovered",
                "created",
                "published",
                "located_in",
                "member_of",
                "headquartered_in",
                "studied_at",
                "works_at",
                "developed",
                "pioneered",
                "capital_of",
                "part_of",
            ];
            if VAGUE_PREDICATES.contains(&h.predicate.as_str()) {
                h.confidence = (h.confidence - 0.05).max(0.0);
            } else if SPECIFIC_PREDICATES.contains(&h.predicate.as_str()) {
                apply_boost(&mut total_boost, 0.04);
                h.confidence = (base_confidence + total_boost).min(1.0);
            }
        }

        // Predicate frequency signal: predicates that appear many times in the graph
        // are well-established patterns — boost hypotheses using them.
        // Rare predicates (< 3 occurrences) may be extraction noise — penalize slightly.
        {
            let pred_count: i64 = self
                .brain
                .with_conn(|conn| {
                    conn.query_row(
                        "SELECT COUNT(*) FROM relations WHERE predicate = ?1",
                        params![h.predicate],
                        |row| row.get(0),
                    )
                })
                .unwrap_or(0);
            if pred_count >= 50 {
                apply_boost(&mut total_boost, 0.05);
                h.confidence = (base_confidence + total_boost).min(1.0);
            } else if pred_count >= 10 {
                apply_boost(&mut total_boost, 0.02);
                h.confidence = (base_confidence + total_boost).min(1.0);
            } else if pred_count < 3 {
                h.evidence_against.push(format!(
                    "Rare predicate '{}' (only {} occurrences in graph)",
                    h.predicate, pred_count
                ));
                h.confidence = (h.confidence - 0.04).max(0.0);
            }
        }

        // Confirmation cascade: if confirmed hypotheses already exist for either
        // entity, this is evidence that the entity is well-connected and new
        // hypotheses involving it are more likely to be valid.
        if h.object != "?" {
            let cascade_count: i64 = self
                .brain
                .with_conn(|conn| {
                    conn.query_row(
                        "SELECT COUNT(*) FROM hypotheses
                         WHERE status = 'confirmed'
                         AND ((subject = ?1 OR object = ?1) OR (subject = ?2 OR object = ?2))
                         AND id != ?3",
                        params![h.subject, h.object, h.id],
                        |row| row.get(0),
                    )
                })
                .unwrap_or(0);
            if cascade_count >= 5 {
                h.evidence_for.push(format!(
                    "Confirmation cascade: {} confirmed hypotheses involve these entities",
                    cascade_count
                ));
                apply_boost(&mut total_boost, 0.08);
                h.confidence = (base_confidence + total_boost).min(1.0);
            } else if cascade_count >= 2 {
                h.evidence_for.push(format!(
                    "Confirmation cascade: {} confirmed hypotheses involve these entities",
                    cascade_count
                ));
                apply_boost(&mut total_boost, 0.04);
                h.confidence = (base_confidence + total_boost).min(1.0);
            }
        }

        // Semantic coherence gate: reject type-incompatible predicates
        // regardless of confidence score
        if h.object != "?" {
            if let (Some(ref se), Some(ref oe)) = (
                self.brain.get_entity_by_name(&h.subject)?,
                self.brain.get_entity_by_name(&h.object)?,
            ) {
                if !predicate_type_compatible(&h.predicate, &se.entity_type, &oe.entity_type) {
                    h.status = HypothesisStatus::Rejected;
                    h.evidence_against.push(format!(
                        "Type mismatch: {} ({}) vs {} ({})",
                        h.subject, se.entity_type, h.object, oe.entity_type
                    ));
                    return Ok(());
                }
            }
        }

        // Update status based on confidence (use defined thresholds)
        let was_confirmed = h.status == HypothesisStatus::Confirmed;
        if h.confidence >= CONFIRMATION_THRESHOLD {
            h.status = HypothesisStatus::Confirmed;
            // Record discovery if newly confirmed
            if !was_confirmed && h.id > 0 {
                let _ = self.save_discovery(h.id, &h.evidence_for);
            }
        } else if h.confidence <= REJECTION_THRESHOLD {
            h.status = HypothesisStatus::Rejected;
        } else {
            h.status = HypothesisStatus::Testing;
        }

        Ok(())
    }

    // -----------------------------------------------------------------------
    // Meta-learning
    // -----------------------------------------------------------------------

    pub fn get_pattern_weight(&self, pattern_type: &str) -> Result<f64> {
        self.brain.with_conn(|conn| {
            let result: std::result::Result<f64, _> = conn.query_row(
                "SELECT weight FROM pattern_weights WHERE pattern_type = ?1",
                params![pattern_type],
                |row| row.get(0),
            );
            Ok(result.unwrap_or(1.0))
        })
    }

    pub fn record_outcome(&self, pattern_type: &str, confirmed: bool) -> Result<()> {
        self.brain.with_conn(|conn| {
            // Update counts for historical tracking
            conn.execute(
                "INSERT INTO pattern_weights (pattern_type, confirmations, rejections, weight)
                 VALUES (?1, ?2, ?3, 1.0)
                 ON CONFLICT(pattern_type) DO UPDATE SET
                    confirmations = confirmations + ?2,
                    rejections = rejections + ?3",
                params![
                    pattern_type,
                    if confirmed { 1 } else { 0 },
                    if confirmed { 0 } else { 1 },
                ],
            )?;
            // Use EMA-blended weight: 80% historical ratio + 20% recent outcome.
            // This lets recovering strategies bounce back faster than pure cumulative.
            let (conf, rej): (i64, i64) = conn.query_row(
                "SELECT confirmations, rejections FROM pattern_weights WHERE pattern_type = ?1",
                params![pattern_type],
                |row| Ok((row.get(0)?, row.get(1)?)),
            )?;
            let total = conf + rej;
            let historical = if total > 0 {
                conf as f64 / total as f64
            } else {
                1.0
            };
            let recent = if confirmed { 1.0 } else { 0.0 };
            // Blend factor: with few samples trust historical ratio more,
            // as samples grow allow recent signal to nudge the weight.
            // recency_factor ranges from ~0.03 (3 samples) to ~0.10 (100+ samples)
            let recency_factor = (10.0 / (total as f64 + 30.0)).min(0.15);
            let ema_weight = historical * (1.0 - recency_factor) + recent * recency_factor;
            conn.execute(
                "UPDATE pattern_weights SET weight = ?1 WHERE pattern_type = ?2",
                params![ema_weight, pattern_type],
            )?;
            Ok(())
        })
    }

    pub fn get_pattern_weights(&self) -> Result<Vec<PatternWeight>> {
        self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT pattern_type, confirmations, rejections, weight FROM pattern_weights ORDER BY weight DESC",
            )?;
            let rows = stmt.query_map([], |row| {
                Ok(PatternWeight {
                    pattern_type: row.get(0)?,
                    confirmations: row.get(1)?,
                    rejections: row.get(2)?,
                    weight: row.get(3)?,
                })
            })?;
            rows.collect()
        })
    }

    /// Get calibrated initial confidence for a pattern source.
    /// Uses historical confirmation rate as Bayesian prior, with smoothing.
    /// New/unknown sources get 0.5 (neutral), proven sources get boosted,
    /// unreliable sources get suppressed.
    pub fn calibrated_confidence(&self, pattern_source: &str, base: f64) -> Result<f64> {
        let weight = self.get_pattern_weight(pattern_source)?;
        // Blend base confidence with historical weight using Bayesian update.
        // Weight of 1.0 = no data (neutral), so don't adjust.
        // Weight close to 0.0 = unreliable source, suppress.
        // Weight close to 1.0 = no data or very good.
        let weights = self.get_pattern_weights()?;
        let source_data = weights.iter().find(|w| w.pattern_type == pattern_source);
        let total_observations = source_data
            .map(|w| w.confirmations + w.rejections)
            .unwrap_or(0);

        if total_observations < 3 {
            // Too few data points — use base confidence
            return Ok(base);
        }
        // Bayesian blend: shift base toward observed weight.
        // Cap alpha at 0.5 and ensure a floor of base*0.4 so strategies
        // can recover from death spirals (validation can still boost them).
        let alpha = (total_observations as f64 / (total_observations as f64 + 20.0)).min(0.5);
        let calibrated = base * (1.0 - alpha) + weight * alpha;
        // Floor: never suppress below 40% of base confidence
        let floor = base * 0.4;
        Ok(calibrated.max(floor).clamp(0.05, 0.95))
    }

    /// Deduplicate hypotheses by entity pair: if multiple hypotheses exist about
    /// the same (subject, object) pair, keep only the highest-confidence one.
    /// Returns count of hypotheses removed.
    pub fn dedup_hypotheses_by_pair(&self) -> Result<usize> {
        let hyps = self.list_hypotheses(None)?;
        let mut by_pair: HashMap<(String, String), Vec<(i64, f64, String)>> = HashMap::new();

        for h in &hyps {
            // Normalize pair order
            let pair = if h.subject <= h.object {
                (h.subject.clone(), h.object.clone())
            } else {
                (h.object.clone(), h.subject.clone())
            };
            by_pair.entry(pair).or_default().push((
                h.id,
                h.confidence,
                h.status.as_str().to_string(),
            ));
        }

        let mut removed = 0usize;
        for (_pair, mut entries) in by_pair {
            if entries.len() < 2 {
                continue;
            }
            // Sort: confirmed first, then by confidence descending
            entries.sort_by(|a, b| {
                let status_ord = |s: &str| -> i32 {
                    match s {
                        "confirmed" => 0,
                        "testing" => 1,
                        "proposed" => 2,
                        _ => 3,
                    }
                };
                status_ord(&a.2)
                    .cmp(&status_ord(&b.2))
                    .then(b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal))
            });
            // Keep the best, remove the rest
            for &(id, _, _) in &entries[1..] {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM hypotheses WHERE id = ?1", params![id])?;
                    Ok(())
                })?;
                removed += 1;
            }
        }
        Ok(removed)
    }

    /// Remove symmetric duplicate hypotheses: for symmetric predicates like
    /// contemporary_of, "A pred B" and "B pred A" are equivalent. Keep the
    /// higher-confidence one and delete the other.
    pub fn dedup_symmetric_hypotheses(&self) -> Result<usize> {
        let hyps = self.list_hypotheses(None)?;
        let mut seen: HashMap<(String, String, String), (i64, f64, String)> = HashMap::new();
        let mut to_delete: Vec<i64> = Vec::new();

        for h in &hyps {
            if !is_symmetric_predicate(&h.predicate) || h.object == "?" {
                continue;
            }
            // Normalize: always store (min, max) order
            let key = if h.subject <= h.object {
                (h.subject.clone(), h.predicate.clone(), h.object.clone())
            } else {
                (h.object.clone(), h.predicate.clone(), h.subject.clone())
            };
            if let Some(existing) = seen.get(&key) {
                // Keep the one with higher confidence, or confirmed status
                let dominated = if h.status.as_str() == "confirmed" && existing.2 != "confirmed" {
                    existing.0
                } else if existing.2 == "confirmed" && h.status.as_str() != "confirmed" {
                    h.id
                } else if h.confidence > existing.1 {
                    existing.0
                } else {
                    h.id
                };
                to_delete.push(dominated);
                if dominated == existing.0 {
                    seen.insert(key, (h.id, h.confidence, h.status.as_str().to_string()));
                }
            } else {
                seen.insert(key, (h.id, h.confidence, h.status.as_str().to_string()));
            }
        }

        let removed = to_delete.len();
        for id in to_delete {
            self.brain.with_conn(|conn| {
                conn.execute("DELETE FROM hypotheses WHERE id = ?1", params![id])?;
                Ok(())
            })?;
        }
        Ok(removed)
    }

    /// Cross-strategy reinforcement: when multiple independent strategies have generated
    /// hypotheses about the same entity pair (subject, object), boost the highest-confidence
    /// one proportionally to the number of independent corroborations.
    /// This runs BEFORE pair deduplication so we can count distinct strategies.
    pub fn cross_strategy_reinforcement(&self) -> Result<usize> {
        let hyps = self.list_hypotheses(None)?;
        // Group by normalized entity pair
        let mut by_pair: HashMap<(String, String), Vec<(i64, f64, String, String)>> =
            HashMap::new();
        for h in &hyps {
            if h.object == "?" {
                continue;
            }
            let pair = if h.subject <= h.object {
                (h.subject.clone(), h.object.clone())
            } else {
                (h.object.clone(), h.subject.clone())
            };
            by_pair.entry(pair).or_default().push((
                h.id,
                h.confidence,
                h.pattern_source.clone(),
                h.status.as_str().to_string(),
            ));
        }

        let mut boosted = 0usize;
        for (_pair, entries) in &by_pair {
            // Count distinct strategies
            let strategies: HashSet<&str> = entries.iter().map(|e| e.2.as_str()).collect();
            if strategies.len() < 2 {
                continue;
            }
            // Find the best non-rejected entry
            let best = entries
                .iter()
                .filter(|e| e.3 != "rejected")
                .max_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));
            if let Some((id, conf, _, status)) = best {
                // Reinforcement: +0.08 per additional independent strategy (diminishing)
                let extra_strategies = strategies.len() - 1;
                let boost = 0.08 * (extra_strategies as f64).min(3.0);
                let new_conf = (conf + boost).min(1.0);
                if new_conf > *conf {
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "UPDATE hypotheses SET confidence = ?1 WHERE id = ?2",
                            params![new_conf, id],
                        )?;
                        Ok(())
                    })?;
                    // If boosted above threshold and still testing, confirm it
                    if new_conf >= CONFIRMATION_THRESHOLD && *status == "testing" {
                        self.update_hypothesis_status(*id, HypothesisStatus::Confirmed)?;
                        let evidence = vec![format!(
                            "Cross-strategy reinforcement: {} independent strategies agree ({})",
                            strategies.len(),
                            strategies.into_iter().collect::<Vec<_>>().join(", ")
                        )];
                        let _ = self.save_discovery(*id, &evidence);
                    }
                    boosted += 1;
                }
            }
        }
        Ok(boosted)
    }

    /// Track discovery velocity: count of new patterns, hypotheses, and confirmations
    /// per discovery run. Persists to a tracking table for trend analysis.
    pub fn track_discovery_velocity(
        &self,
        patterns: usize,
        hypotheses: usize,
        confirmed: usize,
        rejected: usize,
    ) -> Result<()> {
        self.brain.with_conn(|conn| {
            conn.execute_batch(
                "CREATE TABLE IF NOT EXISTS discovery_velocity (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    run_at TEXT NOT NULL,
                    patterns_found INTEGER NOT NULL,
                    hypotheses_generated INTEGER NOT NULL,
                    confirmed INTEGER NOT NULL,
                    rejected INTEGER NOT NULL
                );"
            )?;
            conn.execute(
                "INSERT INTO discovery_velocity (run_at, patterns_found, hypotheses_generated, confirmed, rejected) VALUES (datetime('now'), ?1, ?2, ?3, ?4)",
                params![patterns as i64, hypotheses as i64, confirmed as i64, rejected as i64],
            )?;
            Ok(())
        })
    }

    /// Get discovery velocity trend (recent runs).
    pub fn get_velocity_trend(&self, limit: usize) -> Result<Vec<(String, i64, i64, i64, i64)>> {
        self.brain.with_conn(|conn| {
            let exists: bool = conn.query_row(
                "SELECT COUNT(*) > 0 FROM sqlite_master WHERE type='table' AND name='discovery_velocity'",
                [],
                |row| row.get(0),
            )?;
            if !exists {
                return Ok(vec![]);
            }
            let mut stmt = conn.prepare(
                "SELECT run_at, patterns_found, hypotheses_generated, confirmed, rejected FROM discovery_velocity ORDER BY id DESC LIMIT ?1"
            )?;
            let rows = stmt.query_map(params![limit as i64], |row| {
                Ok((row.get::<_, String>(0)?, row.get::<_, i64>(1)?, row.get::<_, i64>(2)?, row.get::<_, i64>(3)?, row.get::<_, i64>(4)?))
            })?;
            rows.collect::<Result<Vec<_>>>()
        })
    }

    /// Strategy effectiveness report: shows which discovery strategies are working.
    /// Returns Vec of (strategy_name, confirmations, rejections, weight, recommendation).
    pub fn strategy_effectiveness(&self) -> Result<Vec<(String, i64, i64, f64, String)>> {
        let weights = self.get_pattern_weights()?;
        let mut report = Vec::new();
        for w in &weights {
            let total = w.confirmations + w.rejections;
            let rec = if total < 10 {
                "insufficient data".to_string()
            } else if w.weight >= 0.9 {
                "excellent — keep".to_string()
            } else if w.weight >= 0.7 {
                "good".to_string()
            } else if w.weight >= 0.3 {
                "mediocre — review".to_string()
            } else if w.weight >= 0.1 {
                "poor — consider disabling".to_string()
            } else {
                "failing — disable".to_string()
            };
            report.push((
                w.pattern_type.clone(),
                w.confirmations,
                w.rejections,
                w.weight,
                rec,
            ));
        }
        report.sort_by(|a, b| b.3.partial_cmp(&a.3).unwrap_or(std::cmp::Ordering::Equal));
        Ok(report)
    }

    /// Strategy ROI analysis: computes confirmations-per-hypothesis-generated ratio
    /// and identifies strategies with diminishing returns (high volume, low confirmation).
    /// Returns Vec of (strategy, total_generated, confirmed, rejected, roi, recommendation).
    pub fn strategy_roi(&self) -> Result<Vec<(String, i64, i64, i64, f64, String)>> {
        let weights = self.get_pattern_weights()?;
        // Count total hypotheses per strategy
        let strategy_counts: HashMap<String, (i64, i64, i64)> = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT pattern_source,
                        COUNT(*),
                        SUM(CASE WHEN status = 'confirmed' THEN 1 ELSE 0 END),
                        SUM(CASE WHEN status = 'rejected' THEN 1 ELSE 0 END)
                 FROM hypotheses GROUP BY pattern_source",
            )?;
            let rows = stmt.query_map([], |row| {
                Ok((
                    row.get::<_, String>(0)?,
                    row.get::<_, i64>(1)?,
                    row.get::<_, i64>(2)?,
                    row.get::<_, i64>(3)?,
                ))
            })?;
            let mut map = HashMap::new();
            for row in rows {
                let (src, total, conf, rej) = row?;
                map.insert(src, (total, conf, rej));
            }
            Ok(map)
        })?;

        let mut report = Vec::new();
        for w in &weights {
            let (total, confirmed, rejected) = strategy_counts
                .get(&w.pattern_type)
                .copied()
                .unwrap_or((0, 0, 0));
            let roi = if total > 0 {
                confirmed as f64 / total as f64
            } else {
                0.0
            };
            let rec = if total < 5 {
                "needs more data".to_string()
            } else if roi >= 0.5 {
                "excellent ROI — increase exploration".to_string()
            } else if roi >= 0.2 {
                "good ROI".to_string()
            } else if roi >= 0.05 {
                "low ROI — consider tightening thresholds".to_string()
            } else if total > 50 && roi < 0.02 {
                "diminishing returns — reduce volume or disable".to_string()
            } else {
                "poor ROI — review strategy logic".to_string()
            };
            report.push((w.pattern_type.clone(), total, confirmed, rejected, roi, rec));
        }
        report.sort_by(|a, b| b.4.partial_cmp(&a.4).unwrap_or(std::cmp::Ordering::Equal));
        Ok(report)
    }

    /// Adaptive strategy thresholds: returns recommended minimum confidence threshold
    /// per strategy based on historical ROI. Strategies with low ROI get higher thresholds
    /// (more selective), high-ROI strategies get lower thresholds (more exploratory).
    pub fn adaptive_thresholds(&self) -> Result<HashMap<String, f64>> {
        let roi = self.strategy_roi()?;
        let confirmation_rates = self.strategy_confirmation_rates(30).unwrap_or_default();
        let momentum_map = self.strategy_momentum(7, 5).unwrap_or_default();
        let mut thresholds = HashMap::new();
        for (strategy, total, _confirmed, _rejected, roi_val, _) in &roi {
            // Use confirmation rate to further penalize unreliable strategies
            let conf_rate = confirmation_rates
                .get(strategy)
                .map(|(_, _, r)| *r)
                .unwrap_or(0.5);

            let base_threshold = if *total < 10 {
                0.3 // Default: moderate threshold for new strategies
            } else if *roi_val >= 0.3 {
                0.2 // Low threshold: explore more for high-ROI strategies
            } else if *roi_val >= 0.1 {
                0.35 // Standard threshold
            } else if *roi_val >= 0.02 {
                0.5 // Higher threshold: be more selective
            } else {
                0.65 // Very selective: only keep high-confidence hypotheses
            };

            // Confirmation-rate adjustment: strategies with <30% confirmation
            // get an additional threshold increase (requiring higher confidence
            // to pass). This is a softer alternative to full suspension.
            let conf_penalty = if conf_rate < 0.15 {
                0.20 // Very unreliable: strong penalty
            } else if conf_rate < 0.30 {
                0.10 // Unreliable: moderate penalty
            } else {
                0.0
            };

            // Momentum adjustment: strategies improving recently get a threshold
            // reduction (more exploratory); degrading strategies get penalized.
            let momentum_adj = momentum_map
                .get(strategy)
                .map(|&(_, _, m)| {
                    if m > 0.10 {
                        -0.05 // Improving: lower threshold
                    } else if m < -0.15 {
                        0.05 // Degrading: raise threshold
                    } else {
                        0.0
                    }
                })
                .unwrap_or(0.0);

            let combined: f64 = base_threshold + conf_penalty + momentum_adj;
            thresholds.insert(strategy.clone(), combined.min(0.85));
        }
        Ok(thresholds)
    }

    /// Exploration/exploitation budget allocator for hypothesis generation strategies.
    /// Uses Upper Confidence Bound (UCB1) to balance generating hypotheses from
    /// proven strategies (exploitation) vs trying underexplored strategies (exploration).
    /// Returns a map of strategy → budget_fraction (sums to 1.0) for allocating
    /// hypothesis slots across strategies.
    pub fn strategy_budget_allocation(
        &self,
        total_budget: usize,
    ) -> Result<HashMap<String, usize>> {
        let roi_data = self.strategy_roi()?;
        if roi_data.is_empty() {
            return Ok(HashMap::new());
        }

        let total_trials: f64 = roi_data.iter().map(|(_, t, _, _, _, _)| *t as f64).sum();
        let ln_total = if total_trials > 0.0 {
            total_trials.ln()
        } else {
            1.0
        };

        // UCB1 score for each strategy: roi + sqrt(2 * ln(N) / n_i)
        let mut ucb_scores: Vec<(String, f64)> = roi_data
            .iter()
            .map(|(strategy, total, _, _, roi_val, _)| {
                let n = (*total).max(1) as f64;
                let exploitation = *roi_val;
                let exploration = (2.0 * ln_total / n).sqrt();
                (strategy.clone(), exploitation + exploration)
            })
            .collect();

        // Normalize to sum to 1.0
        let total_score: f64 = ucb_scores.iter().map(|(_, s)| s).sum();
        if total_score <= 0.0 {
            // Equal allocation
            let per = total_budget / ucb_scores.len().max(1);
            return Ok(ucb_scores
                .into_iter()
                .map(|(s, _)| (s, per.max(1)))
                .collect());
        }

        let mut allocation: HashMap<String, usize> = HashMap::new();
        let mut remaining = total_budget;
        ucb_scores.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

        for (i, (strategy, score)) in ucb_scores.iter().enumerate() {
            let fraction = score / total_score;
            let slots = if i == ucb_scores.len() - 1 {
                remaining // Last strategy gets remainder
            } else {
                let s = (fraction * total_budget as f64).round() as usize;
                s.max(1).min(remaining) // At least 1 slot per strategy
            };
            allocation.insert(strategy.clone(), slots);
            remaining = remaining.saturating_sub(slots);
        }

        Ok(allocation)
    }

    /// Strategy diversity score: Shannon entropy of hypothesis distribution across strategies.
    /// Higher entropy = more diverse discovery. Low entropy = over-reliance on one strategy.
    /// Returns (entropy_bits, dominant_strategy, dominant_fraction, recommendation).
    pub fn strategy_diversity(&self) -> Result<(f64, String, f64, String)> {
        let counts: HashMap<String, i64> = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT pattern_source, COUNT(*) FROM hypotheses GROUP BY pattern_source",
            )?;
            let rows = stmt.query_map([], |row| {
                Ok((row.get::<_, String>(0)?, row.get::<_, i64>(1)?))
            })?;
            let mut map = HashMap::new();
            for row in rows {
                let (src, count) = row?;
                map.insert(src, count);
            }
            Ok(map)
        })?;

        let total: f64 = counts.values().sum::<i64>() as f64;
        if total == 0.0 {
            return Ok((0.0, String::new(), 0.0, "No hypotheses yet".into()));
        }

        let mut entropy = 0.0_f64;
        let mut dominant = String::new();
        let mut dominant_count = 0i64;
        for (strategy, &count) in &counts {
            if count > dominant_count {
                dominant_count = count;
                dominant = strategy.clone();
            }
            let p = count as f64 / total;
            if p > 0.0 {
                entropy -= p * p.log2();
            }
        }
        let dominant_frac = dominant_count as f64 / total;
        let max_entropy = (counts.len() as f64).log2();

        let rec = if counts.len() < 3 {
            "Too few strategies active — enable more hypothesis generators".into()
        } else if entropy < max_entropy * 0.4 {
            format!(
                "Low diversity: '{}' dominates at {:.0}% — rebalance strategy weights",
                dominant,
                dominant_frac * 100.0
            )
        } else if entropy > max_entropy * 0.8 {
            "Excellent diversity across strategies".into()
        } else {
            "Good diversity — consider boosting underrepresented strategies".into()
        };

        Ok((entropy, dominant, dominant_frac, rec))
    }

    /// Compute per-strategy confirmation rates from the hypotheses table.
    /// Returns strategy → (total, confirmed, confirmation_rate).
    /// Strategies with >= min_samples and confirmation_rate < suspend_threshold
    /// should be auto-suspended to avoid wasting cycles on low-quality hypotheses.
    pub fn strategy_confirmation_rates(
        &self,
        min_samples: i64,
    ) -> Result<HashMap<String, (i64, i64, f64)>> {
        self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT pattern_source, COUNT(*) as total,
                        SUM(CASE WHEN status='confirmed' THEN 1 ELSE 0 END) as confirmed
                 FROM hypotheses
                 GROUP BY pattern_source
                 HAVING total >= ?1",
            )?;
            let rows = stmt.query_map(params![min_samples], |row| {
                let source: String = row.get(0)?;
                let total: i64 = row.get(1)?;
                let confirmed: i64 = row.get(2)?;
                Ok((source, total, confirmed))
            })?;
            let mut map = HashMap::new();
            for row in rows {
                let (source, total, confirmed) = row?;
                let rate = if total > 0 {
                    confirmed as f64 / total as f64
                } else {
                    0.0
                };
                map.insert(source, (total, confirmed, rate));
            }
            Ok(map)
        })
    }

    /// Compute strategy momentum: compare recent (last N days) confirmation rate
    /// against all-time rate. Returns map of strategy → (recent_rate, alltime_rate, momentum).
    /// Positive momentum means the strategy is improving; negative means degrading.
    pub fn strategy_momentum(
        &self,
        recent_days: i64,
        min_recent_samples: i64,
    ) -> Result<HashMap<String, (f64, f64, f64)>> {
        let cutoff = (Utc::now() - chrono::Duration::days(recent_days))
            .format("%Y-%m-%d %H:%M:%S")
            .to_string();
        self.brain.with_conn(|conn| {
            // Recent rates
            let mut stmt = conn.prepare(
                "SELECT pattern_source,
                        COUNT(*) as total,
                        SUM(CASE WHEN status='confirmed' THEN 1 ELSE 0 END) as confirmed
                 FROM hypotheses
                 WHERE discovered_at >= ?1
                 GROUP BY pattern_source
                 HAVING total >= ?2",
            )?;
            let recent: HashMap<String, (i64, i64)> = stmt
                .query_map(params![cutoff, min_recent_samples], |row| {
                    Ok((
                        row.get::<_, String>(0)?,
                        row.get::<_, i64>(1)?,
                        row.get::<_, i64>(2)?,
                    ))
                })?
                .filter_map(|r| r.ok())
                .map(|(s, t, c)| (s, (t, c)))
                .collect();

            // All-time rates
            let mut stmt2 = conn.prepare(
                "SELECT pattern_source,
                        COUNT(*) as total,
                        SUM(CASE WHEN status='confirmed' THEN 1 ELSE 0 END) as confirmed
                 FROM hypotheses
                 GROUP BY pattern_source
                 HAVING total >= 10",
            )?;
            let alltime: HashMap<String, (i64, i64)> = stmt2
                .query_map([], |row| {
                    Ok((
                        row.get::<_, String>(0)?,
                        row.get::<_, i64>(1)?,
                        row.get::<_, i64>(2)?,
                    ))
                })?
                .filter_map(|r| r.ok())
                .map(|(s, t, c)| (s, (t, c)))
                .collect();

            let mut result = HashMap::new();
            for (strategy, (rt, rc)) in &recent {
                let recent_rate = if *rt > 0 {
                    *rc as f64 / *rt as f64
                } else {
                    0.0
                };
                let (at, ac) = alltime.get(strategy).copied().unwrap_or((0, 0));
                let alltime_rate = if at > 0 { ac as f64 / at as f64 } else { 0.0 };
                let momentum = recent_rate - alltime_rate;
                result.insert(strategy.clone(), (recent_rate, alltime_rate, momentum));
            }
            Ok(result)
        })
    }

    /// Returns set of strategy names that should be suspended (confirmation rate
    /// below threshold with sufficient samples). These strategies waste compute.
    pub fn suspended_strategies(&self) -> Result<HashSet<String>> {
        let rates = self.strategy_confirmation_rates(50)?;
        let momentum_map = self.strategy_momentum(7, 5).unwrap_or_default();
        let mut suspended = HashSet::new();
        for (strategy, (_total, _confirmed, rate)) in &rates {
            if *rate < 0.20 {
                // Check momentum: if the strategy is recently improving significantly,
                // give it a second chance instead of suspending
                if let Some(&(recent_rate, _, momentum)) = momentum_map.get(strategy) {
                    if momentum > 0.15 && recent_rate >= 0.30 {
                        eprintln!(
                            "[PROMETHEUS] Strategy '{}' below threshold ({:.1}%) but showing positive momentum ({:.1}% recent) — keeping active",
                            strategy, rate * 100.0, recent_rate * 100.0
                        );
                        continue;
                    }
                }
                suspended.insert(strategy.clone());
                eprintln!(
                    "[PROMETHEUS] Auto-suspending strategy '{}' (confirmation rate {:.1}% < 20%)",
                    strategy,
                    rate * 100.0
                );
            }
        }
        Ok(suspended)
    }

    /// Get total discovery count (confirmed hypotheses that were recorded).
    pub fn discovery_count(&self) -> Result<i64> {
        self.brain.with_conn(|conn| {
            let exists: bool = conn.query_row(
                "SELECT COUNT(*) > 0 FROM sqlite_master WHERE type='table' AND name='discoveries'",
                [],
                |row| row.get(0),
            )?;
            if !exists {
                return Ok(0);
            }
            conn.query_row("SELECT COUNT(*) FROM discoveries", [], |row| row.get(0))
        })
    }

    /// Discovery score: how many confirmed hypotheses involve each entity.
    pub fn discovery_scores(&self) -> Result<HashMap<String, i64>> {
        let confirmed = self.list_hypotheses(Some(HypothesisStatus::Confirmed))?;
        let mut scores: HashMap<String, i64> = HashMap::new();
        for h in &confirmed {
            *scores.entry(h.subject.clone()).or_insert(0) += 1;
            if h.object != "?" {
                *scores.entry(h.object.clone()).or_insert(0) += 1;
            }
        }
        Ok(scores)
    }

    // -----------------------------------------------------------------------
    // Full Discovery Pipeline
    // -----------------------------------------------------------------------

    /// Run the full discovery pipeline: patterns → gaps → hypotheses → validation.
    pub fn discover(&self) -> Result<DiscoveryReport> {
        let mut all_patterns = Vec::new();
        let mut all_hypotheses = Vec::new();

        // 1. Pattern discovery (adaptive thresholds based on graph density)
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;
        let meaningful_rels: usize = relations
            .iter()
            .filter(|r| meaningful.contains(&r.subject_id) || meaningful.contains(&r.object_id))
            .count();
        // Sparse graphs need lower thresholds
        let co_threshold = if meaningful_rels < 50 { 1 } else { 2 };
        let subgraph_threshold = if meaningful_rels < 100 { 1 } else { 2 };

        eprintln!(
            "[PROMETHEUS] Starting pattern discovery ({} meaningful rels)",
            meaningful_rels
        );
        let t0 = std::time::Instant::now();

        let co_occ = self.find_co_occurrences(co_threshold)?;
        eprintln!(
            "[PROMETHEUS] co_occurrences: {} in {:?}",
            co_occ.len(),
            t0.elapsed()
        );
        all_patterns.extend(co_occ);

        let t1 = std::time::Instant::now();
        let source_co = self.find_source_co_occurrences()?;
        eprintln!(
            "[PROMETHEUS] source_co: {} in {:?}",
            source_co.len(),
            t1.elapsed()
        );
        all_patterns.extend(source_co);

        let t1 = std::time::Instant::now();
        let clusters = self.find_entity_clusters()?;
        eprintln!(
            "[PROMETHEUS] clusters: {} in {:?}",
            clusters.len(),
            t1.elapsed()
        );
        all_patterns.extend(clusters);

        let t1 = std::time::Instant::now();
        let subgraphs = self.find_frequent_subgraphs(subgraph_threshold)?;
        eprintln!(
            "[PROMETHEUS] subgraphs: {} in {:?}",
            subgraphs.len(),
            t1.elapsed()
        );
        all_patterns.extend(subgraphs);

        let t1 = std::time::Instant::now();
        let temporal = self.find_temporal_patterns(2)?;
        eprintln!(
            "[PROMETHEUS] temporal: {} in {:?}",
            temporal.len(),
            t1.elapsed()
        );
        all_patterns.extend(temporal);

        let t1 = std::time::Instant::now();
        let anomalies = self.find_anomalies()?;
        eprintln!(
            "[PROMETHEUS] anomalies: {} in {:?}",
            anomalies.len(),
            t1.elapsed()
        );
        all_patterns.extend(anomalies);

        let t1 = std::time::Instant::now();
        let pmi_patterns = self.find_pmi_co_occurrences(1.0)?;
        eprintln!(
            "[PROMETHEUS] pmi: {} in {:?}",
            pmi_patterns.len(),
            t1.elapsed()
        );
        all_patterns.extend(pmi_patterns);

        let t1 = std::time::Instant::now();
        let chains = self.find_predicate_chains(2)?;
        eprintln!(
            "[PROMETHEUS] chains: {} in {:?}",
            chains.len(),
            t1.elapsed()
        );
        all_patterns.extend(chains);

        // Deduplicate and save patterns
        self.dedup_patterns(&mut all_patterns);
        for p in &all_patterns {
            let _ = self.save_pattern(p);
        }

        // 2. Gap detection & hypothesis generation (adaptive: skip low-weight strategies)
        // Auto-suspend strategies with < 20% confirmation rate over 50+ samples
        let suspended = self.suspended_strategies().unwrap_or_default();
        if !suspended.is_empty() {
            eprintln!(
                "[PROMETHEUS] Suspended {} low-ROI strategies: {:?}",
                suspended.len(),
                suspended
            );
        }
        eprintln!("[PROMETHEUS] Starting hypothesis generation...");
        let t1 = std::time::Instant::now();
        let hole_weight = self.get_pattern_weight("structural_hole")?;
        if hole_weight >= 0.05 {
            let hole_hyps = self.generate_hypotheses_from_holes()?;
            eprintln!(
                "[PROMETHEUS] holes: {} in {:?}",
                hole_hyps.len(),
                t1.elapsed()
            );
            all_hypotheses.extend(hole_hyps);
        }

        let t1 = std::time::Instant::now();
        let gap_hyps = self.generate_hypotheses_from_type_gaps()?;
        eprintln!(
            "[PROMETHEUS] type_gaps: {} in {:?}",
            gap_hyps.len(),
            t1.elapsed()
        );
        all_hypotheses.extend(gap_hyps);

        let t1 = std::time::Instant::now();
        let shared_weight = self.get_pattern_weight("shared_object")?;
        if shared_weight >= 0.10 {
            let shared_hyps = self.generate_hypotheses_from_shared_objects()?;
            eprintln!(
                "[PROMETHEUS] shared_objects: {} in {:?}",
                shared_hyps.len(),
                t1.elapsed()
            );
            all_hypotheses.extend(shared_hyps);
        }

        let t1 = std::time::Instant::now();
        let source_weight = self.get_pattern_weight("source_co_occurrence")?;
        if source_weight >= 0.05 {
            let source_hyps = self.generate_hypotheses_from_source_co_occurrence()?;
            eprintln!(
                "[PROMETHEUS] source_co_occ: {} in {:?}",
                source_hyps.len(),
                t1.elapsed()
            );
            all_hypotheses.extend(source_hyps);
        } else {
            all_patterns.push(Pattern {
                id: 0,
                pattern_type: PatternType::CoOccurrence,
                entities_involved: vec![],
                frequency: 0,
                last_seen: now_str(),
                description: format!(
                    "source_co_occurrence strategy skipped (weight {:.3} < 0.05)",
                    source_weight
                ),
            });
        }

        // 2b. Analogy-based hypotheses
        let t1 = std::time::Instant::now();
        let analogy_hyps = self.generate_hypotheses_from_analogies()?;
        eprintln!(
            "[PROMETHEUS] analogies: {} in {:?}",
            analogy_hyps.len(),
            t1.elapsed()
        );
        all_hypotheses.extend(analogy_hyps);

        // 2c. Hub-spoke hypotheses
        let t1 = std::time::Instant::now();
        let hub_weight = self.get_pattern_weight("hub_spoke")?;
        if hub_weight >= 0.10 {
            let hub_hyps = self.generate_hypotheses_from_hubs()?;
            eprintln!(
                "[PROMETHEUS] hubs: {} in {:?}",
                hub_hyps.len(),
                t1.elapsed()
            );
            all_hypotheses.extend(hub_hyps);
        }

        // 2d. Community bridge hypotheses
        let t1 = std::time::Instant::now();
        let bridge_hyps = self.generate_hypotheses_from_community_bridges()?;
        eprintln!(
            "[PROMETHEUS] community_bridges: {} in {:?}",
            bridge_hyps.len(),
            t1.elapsed()
        );
        all_hypotheses.extend(bridge_hyps);

        // 2d2. Community homophily (same type + same community + shared neighbors)
        let t1 = std::time::Instant::now();
        let ch_weight = self.get_pattern_weight("community_homophily")?;
        if ch_weight >= 0.05 {
            let ch_hyps = self.generate_hypotheses_from_community_homophily()?;
            eprintln!(
                "[PROMETHEUS] community_homophily: {} in {:?}",
                ch_hyps.len(),
                t1.elapsed()
            );
            all_hypotheses.extend(ch_hyps);
        }

        // 2e. Predicate chain hypotheses
        let t1 = std::time::Instant::now();
        let chain_weight = self.get_pattern_weight("predicate_chain")?;
        if chain_weight >= 0.05 {
            let chain_hyps = self.generate_hypotheses_from_chains()?;
            eprintln!(
                "[PROMETHEUS] pred_chains: {} in {:?}",
                chain_hyps.len(),
                t1.elapsed()
            );
            all_hypotheses.extend(chain_hyps);
        }

        // 2f. Near-miss connection hypotheses
        let t1 = std::time::Instant::now();
        let near_miss_weight = self.get_pattern_weight("near_miss")?;
        if near_miss_weight >= 0.05 {
            let near_miss_hyps = self.generate_hypotheses_from_near_misses()?;
            eprintln!(
                "[PROMETHEUS] near_miss: {} in {:?}",
                near_miss_hyps.len(),
                t1.elapsed()
            );
            all_hypotheses.extend(near_miss_hyps);
        }

        // 2g. Adamic-Adar link prediction
        let t1 = std::time::Instant::now();
        let aa_weight = self.get_pattern_weight("adamic_adar")?;
        if aa_weight >= 0.05 {
            let aa_hyps = self.generate_hypotheses_from_adamic_adar()?;
            eprintln!(
                "[PROMETHEUS] adamic_adar: {} in {:?}",
                aa_hyps.len(),
                t1.elapsed()
            );
            all_hypotheses.extend(aa_hyps);
        }

        // 2h. Resource Allocation link prediction
        let t1 = std::time::Instant::now();
        let ra_weight = self.get_pattern_weight("resource_allocation")?;
        if ra_weight >= 0.05 {
            let ra_hyps = self.generate_hypotheses_from_resource_allocation()?;
            eprintln!(
                "[PROMETHEUS] resource_alloc: {} in {:?}",
                ra_hyps.len(),
                t1.elapsed()
            );
            all_hypotheses.extend(ra_hyps);
        }

        // 2i. Type-aware link prediction
        let t1 = std::time::Instant::now();
        let ta_weight = self.get_pattern_weight("type_affinity")?;
        if ta_weight >= 0.05 {
            let ta_hyps = self.generate_hypotheses_from_type_affinity()?;
            eprintln!(
                "[PROMETHEUS] type_affinity: {} in {:?}",
                ta_hyps.len(),
                t1.elapsed()
            );
            all_hypotheses.extend(ta_hyps);
        }

        // 2j. Neighborhood overlap link prediction (robust to degree imbalance)
        let no_weight = self.get_pattern_weight("neighborhood_overlap")?;
        if no_weight >= 0.05 {
            let no_hyps = self.generate_hypotheses_from_neighborhood_overlap()?;
            all_hypotheses.extend(no_hyps);
        }

        // 2k. Triadic closure (open triads with ≥2 mutual neighbors)
        let tc_weight = self.get_pattern_weight("triadic_closure")?;
        if tc_weight >= 0.05 {
            let tc_hyps = self.generate_hypotheses_from_triadic_closure()?;
            all_hypotheses.extend(tc_hyps);
        }

        // 2l. Semantic fingerprint similarity (entities sharing same predicate-object patterns)
        let sf_weight = self.get_pattern_weight("semantic_fingerprint")?;
        if sf_weight >= 0.05 {
            let sf_hyps = self.generate_hypotheses_from_semantic_similarity()?;
            all_hypotheses.extend(sf_hyps);
        }

        // 2m. Jaccard coefficient link prediction (degree-normalized neighbor overlap)
        let jc_weight = self.get_pattern_weight("jaccard")?;
        if jc_weight >= 0.05 {
            let jc_hyps = self.generate_hypotheses_from_jaccard()?;
            all_hypotheses.extend(jc_hyps);
        }

        // 2n. Rising star hypotheses (high momentum, low degree entities)
        let rs_weight = self.get_pattern_weight("rising_star")?;
        if rs_weight >= 0.05 {
            let rs_hyps = self.generate_hypotheses_from_rising_stars()?;
            eprintln!("[PROMETHEUS] rising_stars: {} hypotheses", rs_hyps.len());
            all_hypotheses.extend(rs_hyps);
        }

        // 2o. Predicate pattern transfer (entities with similar predicate profiles missing a predicate)
        // Auto-suspended if confirmation rate < 20% over 50+ samples
        let ppt_weight = self.get_pattern_weight("predicate_transfer")?;
        if ppt_weight >= 0.05 && !suspended.contains("predicate_transfer") {
            let ppt_hyps = self.generate_hypotheses_from_predicate_transfer()?;
            eprintln!(
                "[PROMETHEUS] predicate_transfer: {} hypotheses",
                ppt_hyps.len()
            );
            all_hypotheses.extend(ppt_hyps);
        }

        // 2p. Preferential attachment (high-degree hubs not yet connected)
        let pa_weight = self.get_pattern_weight("preferential_attachment")?;
        if pa_weight >= 0.05 {
            let pa_hyps = self.generate_hypotheses_from_preferential_attachment()?;
            eprintln!(
                "[PROMETHEUS] preferential_attachment: {} hypotheses",
                pa_hyps.len()
            );
            all_hypotheses.extend(pa_hyps);
        }

        // 2r. Isolated entity connector (reduce fragmentation)
        let ic_weight = self.get_pattern_weight("isolated_connector")?;
        if ic_weight >= 0.05 {
            let ic_hyps = self.generate_hypotheses_from_isolated_connector()?;
            eprintln!(
                "[PROMETHEUS] isolated_connector: {} hypotheses",
                ic_hyps.len()
            );
            all_hypotheses.extend(ic_hyps);
        }

        // 2q. Katz similarity link prediction (multi-hop path-based)
        let katz_weight = self.get_pattern_weight("katz_similarity")?;
        if katz_weight >= 0.05 {
            let katz_hyps = self.generate_hypotheses_from_katz_similarity()?;
            eprintln!(
                "[PROMETHEUS] katz_similarity: {} hypotheses",
                katz_hyps.len()
            );
            all_hypotheses.extend(katz_hyps);
        }

        // 2s2. Concept enrichment (sparse concepts → related entities)
        let ce_weight = self.get_pattern_weight("concept_enrichment")?;
        if ce_weight >= 0.05 {
            let ce_hyps = self.generate_hypotheses_from_concept_enrichment()?;
            eprintln!(
                "[PROMETHEUS] concept_enrichment: {} hypotheses",
                ce_hyps.len()
            );
            all_hypotheses.extend(ce_hyps);
        }

        // 2s. Temporal co-discovery: isolated entities first-seen in the same
        //     time window likely originate from the same topic/article
        let tcd_weight = self.get_pattern_weight("temporal_co_discovery")?;
        if tcd_weight >= 0.05 && !suspended.contains("temporal_co_discovery") {
            let tcd_hyps = self.generate_hypotheses_from_temporal_co_discovery()?;
            eprintln!(
                "[PROMETHEUS] temporal_co_discovery: {} hypotheses",
                tcd_hyps.len()
            );
            all_hypotheses.extend(tcd_hyps);
        }

        // 2u. Predicate-profile similarity (structurally analogous entities)
        let pp_weight = self.get_pattern_weight("predicate_profile")?;
        if pp_weight >= 0.05 {
            let pp_hyps = self.generate_hypotheses_from_predicate_profile()?;
            eprintln!(
                "[PROMETHEUS] predicate_profile: {} hypotheses",
                pp_hyps.len()
            );
            all_hypotheses.extend(pp_hyps);
        }

        // 2v. Predicate covariance gaps (entities missing expected correlated predicates)
        let pcov_weight = self.get_pattern_weight("predicate_covariance")?;
        if pcov_weight >= 0.05 {
            let pcov_hyps = self.generate_hypotheses_from_predicate_covariance()?;
            eprintln!(
                "[PROMETHEUS] predicate_covariance: {} hypotheses",
                pcov_hyps.len()
            );
            all_hypotheses.extend(pcov_hyps);
        }

        // 2t. Shared predicate-object inverse inference
        let spo_weight = self.get_pattern_weight("shared_predicate_object")?;
        if spo_weight >= 0.05 {
            let spo_hyps = self.generate_hypotheses_from_shared_predicate_object()?;
            eprintln!(
                "[PROMETHEUS] shared_predicate_object: {} hypotheses",
                spo_hyps.len()
            );
            all_hypotheses.extend(spo_hyps);
        }

        // 2w. Overlap coefficient link prediction (asymmetric similarity)
        let oc_weight = self.get_pattern_weight("overlap_coefficient")?;
        if oc_weight >= 0.05 {
            let oc_hyps = self.generate_hypotheses_from_overlap_coefficient()?;
            eprintln!(
                "[PROMETHEUS] overlap_coefficient: {} hypotheses",
                oc_hyps.len()
            );
            all_hypotheses.extend(oc_hyps);
        }

        // 2x. Inverse relation inference (bidirectional knowledge)
        let ir_weight = self.get_pattern_weight("inverse_relation")?;
        if ir_weight >= 0.05 {
            let ir_hyps = self.generate_hypotheses_from_inverse_relations()?;
            eprintln!(
                "[PROMETHEUS] inverse_relation: {} hypotheses",
                ir_hyps.len()
            );
            all_hypotheses.extend(ir_hyps);
        }

        // 2z. Multi-hop transitive inference (A→B→C ⇒ A~C)
        let mh_weight = self.get_pattern_weight("multi_hop")?;
        if mh_weight >= 0.05 {
            let mh_hyps = self.generate_hypotheses_from_multi_hop()?;
            eprintln!("[PROMETHEUS] multi_hop: {} hypotheses", mh_hyps.len());
            all_hypotheses.extend(mh_hyps);
        }

        // 2y. Co-fact hypothesis (entities sharing same fact key-value pairs)
        let cf_weight = self.get_pattern_weight("co_fact")?;
        if cf_weight >= 0.05 {
            let cf_hyps = self.generate_hypotheses_from_co_facts()?;
            eprintln!("[PROMETHEUS] co_fact: {} hypotheses", cf_hyps.len());
            all_hypotheses.extend(cf_hyps);
        }

        // 2ee. Type-level semantic fingerprint (softer similarity via (pred, obj_type) patterns)
        let tlf_weight = self.get_pattern_weight("type_level_fingerprint")?;
        if tlf_weight >= 0.05 {
            let tlf_hyps = self.generate_hypotheses_from_type_level_fingerprint()?;
            eprintln!(
                "[PROMETHEUS] type_level_fingerprint: {} hypotheses",
                tlf_hyps.len()
            );
            all_hypotheses.extend(tlf_hyps);
        }

        // 2aa. Structural similarity (typed neighborhood overlap — not directly connected)
        let ss_weight = self.get_pattern_weight("structural_similarity")?;
        if ss_weight >= 0.05 {
            let ss_hyps = self.generate_hypotheses_from_structural_similarity()?;
            eprintln!(
                "[PROMETHEUS] structural_similarity: {} hypotheses",
                ss_hyps.len()
            );
            all_hypotheses.extend(ss_hyps);
        }

        // 2bb. Fact-gap strategy: entities rich in facts but relation-poor
        let fg_weight = self.get_pattern_weight("fact_gap")?;
        if fg_weight >= 0.05 {
            let fg_hyps = self.generate_hypotheses_from_fact_gaps()?;
            eprintln!("[PROMETHEUS] fact_gap: {} hypotheses", fg_hyps.len());
            all_hypotheses.extend(fg_hyps);
        }

        // 2cc. Open triangle completion (strong triadic closure principle)
        let ot_weight = self.get_pattern_weight("open_triangle")?;
        if ot_weight >= 0.05 {
            let ot_hyps = self.generate_hypotheses_from_open_triangles()?;
            eprintln!("[PROMETHEUS] open_triangle: {} hypotheses", ot_hyps.len());
            all_hypotheses.extend(ot_hyps);
        }

        // 2dd. Heat-kernel diffusion link prediction (multi-hop reachability)
        let hk_weight = self.get_pattern_weight("heat_kernel")?;
        if hk_weight >= 0.05 {
            let hk_hyps = self.generate_hypotheses_from_heat_kernel()?;
            eprintln!("[PROMETHEUS] heat_kernel: {} hypotheses", hk_hyps.len());
            all_hypotheses.extend(hk_hyps);
        }

        // 3. Island entities as gaps
        let islands = self.find_island_entities()?;

        let gaps_detected = all_hypotheses.len() + islands.len();

        // 4. Deduplicate hypotheses against existing DB, filter noise, then score and validate
        all_hypotheses.retain(|h| {
            // Skip hypotheses involving noise entities
            if is_noise_name(&h.subject) || (h.object != "?" && is_noise_name(&h.object)) {
                return false;
            }
            // Skip hypotheses with very long entity names (likely sentence fragments)
            if h.subject.len() > 60 || h.object.len() > 60 {
                return false;
            }
            // Skip entities that are mostly digits (coordinates, IDs, dates)
            let digit_ratio = |s: &str| -> f64 {
                if s.is_empty() {
                    return 0.0;
                }
                s.chars().filter(|c| c.is_ascii_digit()).count() as f64 / s.len() as f64
            };
            if digit_ratio(&h.subject) > 0.5 || (h.object != "?" && digit_ratio(&h.object) > 0.5) {
                return false;
            }
            // Skip substring hypotheses — one name contains the other (merge candidates, not discoveries)
            if h.object != "?" {
                let sl = h.subject.to_lowercase();
                let ol = h.object.to_lowercase();
                if sl.contains(&ol) || ol.contains(&sl) {
                    return false;
                }
            }
            // Skip single-word generic entities as hypothesis subjects
            if !h.subject.contains(' ') && h.subject.len() < 10 {
                let lower = h.subject.to_lowercase();
                let generics = [
                    "church",
                    "city",
                    "court",
                    "sea",
                    "bay",
                    "lake",
                    "river",
                    "bridge",
                    "federal",
                    "state",
                    "power",
                    "steam",
                    "monster",
                    "alice",
                    "grace",
                    "forest",
                    "desert",
                    "county",
                    "district",
                    "island",
                    "port",
                    "cape",
                    "berkeley",
                    "paris",
                    "london",
                    "berlin",
                    "vienna",
                    "zurich",
                    "monthly",
                    "reform",
                    "precision",
                    "regime",
                    "ancient",
                ];
                if generics.contains(&lower.as_str()) {
                    return false;
                }
            }
            // Skip hypotheses where both entities share the same first word (likely noise variants)
            // e.g., "French Army" ↔ "French Revolution" — these share a prefix, not a discovery
            if h.object != "?" {
                let s_first = h.subject.split_whitespace().next().unwrap_or("");
                let o_first = h.object.split_whitespace().next().unwrap_or("");
                if !s_first.is_empty() && s_first == o_first {
                    return false;
                }
            }
            // Skip hypotheses where both entities share the same last word (categorical, not insightful)
            // e.g., "Caroline Islands" ↔ "Faroe Islands" — same-suffix entities cluster trivially
            if h.object != "?" {
                let s_last = h.subject.split_whitespace().last().unwrap_or("");
                let o_last = h.object.split_whitespace().last().unwrap_or("");
                if !s_last.is_empty() && s_last.len() > 3 && s_last == o_last {
                    return false;
                }
            }
            !self
                .hypothesis_exists(&h.subject, &h.predicate, &h.object)
                .unwrap_or(true)
        });

        // Strategy diversity balancer: cap per-strategy contributions to ensure
        // underrepresented strategies get room. Max 40% from any single strategy.
        let total_before_diversity = all_hypotheses.len();
        if total_before_diversity > 50 {
            let max_per_strategy = (total_before_diversity * 2 / 5).max(20); // 40% cap, min 20
            let mut strategy_counts: HashMap<String, usize> = HashMap::new();
            let mut overflow: Vec<Hypothesis> = Vec::new();
            let mut balanced: Vec<Hypothesis> = Vec::new();
            for h in all_hypotheses.drain(..) {
                let count = strategy_counts.entry(h.pattern_source.clone()).or_insert(0);
                if *count < max_per_strategy {
                    *count += 1;
                    balanced.push(h);
                } else {
                    overflow.push(h);
                }
            }
            // Backfill from overflow if we have room (prioritize rare strategies)
            overflow.sort_by(|a, b| {
                let a_count = strategy_counts.get(&a.pattern_source).copied().unwrap_or(0);
                let b_count = strategy_counts.get(&b.pattern_source).copied().unwrap_or(0);
                a_count.cmp(&b_count).then(
                    b.confidence
                        .partial_cmp(&a.confidence)
                        .unwrap_or(std::cmp::Ordering::Equal),
                )
            });
            balanced.extend(overflow.into_iter().take(50)); // allow some overflow
            let diversity_trimmed = total_before_diversity.saturating_sub(balanced.len());
            if diversity_trimmed > 0 {
                eprintln!(
                    "[PROMETHEUS] Strategy diversity balancer: trimmed {} hypotheses (cap {})",
                    diversity_trimmed, max_per_strategy
                );
            }
            all_hypotheses = balanced;
        }

        for h in all_hypotheses.iter_mut().take(200) {
            self.score_hypothesis(h)?;
            self.validate_hypothesis(h)?;
            let _ = self.apply_type_pair_affinity(h);
        }
        // Batch-boost using graph structure (communities + k-cores) — computed once
        let boost_slice = if all_hypotheses.len() > 200 {
            &mut all_hypotheses[..200]
        } else {
            &mut all_hypotheses[..]
        };
        let _ = self.boost_hypotheses_with_graph_structure(boost_slice);
        let lp_slice = if all_hypotheses.len() > 200 {
            &mut all_hypotheses[..200]
        } else {
            &mut all_hypotheses[..]
        };
        let _ = self.boost_with_label_propagation(lp_slice);
        // Boost/penalize based on predicate diversity impact
        let diversity_slice = if all_hypotheses.len() > 200 {
            &mut all_hypotheses[..200]
        } else {
            &mut all_hypotheses[..]
        };
        let diversity_boosted = self
            .boost_diversity_increasing_hypotheses(diversity_slice)
            .unwrap_or(0);
        if diversity_boosted > 0 {
            eprintln!(
                "[PROMETHEUS] Diversity-boosted {} hypotheses",
                diversity_boosted
            );
        }
        for h in all_hypotheses.iter().take(200) {
            let _ = self.save_hypothesis(h);
        }

        let confirmed = all_hypotheses
            .iter()
            .filter(|h| h.status == HypothesisStatus::Confirmed)
            .count();
        let rejected = all_hypotheses
            .iter()
            .filter(|h| h.status == HypothesisStatus::Rejected)
            .count();

        // Cross-domain gap analysis
        let cross_gaps = self.find_cross_domain_gaps().unwrap_or_default();
        let frontiers = self.find_knowledge_frontiers().unwrap_or_default();
        let bridges = self.find_bridge_entities().unwrap_or_default();
        let island_clusters = self.cluster_islands_for_crawl().unwrap_or_default();

        // Apply calibrated confidence to all hypotheses before scoring
        for h in all_hypotheses.iter_mut() {
            if let Ok(cal) = self.calibrated_confidence(&h.pattern_source, h.confidence) {
                h.confidence = cal;
            }
        }

        // Apply adaptive thresholds: filter out hypotheses below per-strategy threshold
        let adaptive_thresholds = self.adaptive_thresholds().unwrap_or_default();
        let pre_adaptive = all_hypotheses.len();
        all_hypotheses.retain(|h| {
            let threshold = adaptive_thresholds
                .get(&h.pattern_source)
                .copied()
                .unwrap_or(0.3);
            h.confidence >= threshold
        });
        let adaptive_filtered = pre_adaptive - all_hypotheses.len();
        if adaptive_filtered > 0 {
            eprintln!(
                "[PROMETHEUS] Adaptive threshold filtered {} low-ROI hypotheses",
                adaptive_filtered
            );
        }

        // Decay old hypotheses (meta-learning)
        let decayed = self.decay_old_hypotheses(7).unwrap_or(0);

        // Prune stale low-confidence hypotheses (>14 days old)
        let pruned = self.prune_stale_hypotheses(14).unwrap_or(0);

        // Recycle previously rejected hypotheses from high-ROI strategies
        let (recycled, recycle_promoted) = self.recycle_rejected_hypotheses().unwrap_or((0, 0));
        if recycled > 0 {
            eprintln!(
                "[PROMETHEUS] Hypothesis recycling: {} re-evaluated, {} promoted",
                recycled, recycle_promoted
            );
        }

        // Auto-resolve existing testing hypotheses
        let (auto_confirmed, auto_rejected) = self.auto_resolve_hypotheses().unwrap_or((0, 0));

        // Resolve mutual exclusions (contradictory hypotheses for same entity pair)
        let (excl_promoted, excl_rejected) = self.resolve_mutual_exclusions().unwrap_or((0, 0));
        if excl_promoted + excl_rejected > 0 {
            eprintln!(
                "[PROMETHEUS] Mutual exclusion: {} promoted, {} rejected",
                excl_promoted, excl_rejected
            );
        }

        // Find connectable islands
        let connectable = self.find_connectable_islands().unwrap_or_default();

        // Prioritize gaps
        let gap_priorities = self.prioritize_gaps().unwrap_or_default();

        // Predicate diversity analysis
        let (total_rels, _generic_rels, diverse_rels, div_ratio) =
            self.predicate_diversity().unwrap_or((0, 0, 0, 0.0));

        // Exact-name deduplication (catches case-only duplicates)
        let exact_deduped = self.dedup_exact_name_matches().unwrap_or(0);

        // Reverse containment island merge (island "Euler" → "Leonhard Euler")
        let reverse_merged = self.reverse_containment_island_merge().unwrap_or(0);

        // Bulk reject stale testing hypotheses (>5 days, confidence < 0.5)
        let bulk_rejected = self.bulk_reject_stale_testing(5, 0.5).unwrap_or(0);

        // Clean up fragment hypotheses (single-word subject+object = NLP noise, not discoveries)
        let fragment_cleaned = self.cleanup_fragment_hypotheses().unwrap_or(0);

        // Reject hypotheses involving concatenated entity names
        let concat_hyp_rejected = self.cleanup_concatenated_hypotheses().unwrap_or(0);
        if concat_hyp_rejected > 0 {
            eprintln!(
                "[PROMETHEUS] Rejected {} hypotheses with concatenated entity names",
                concat_hyp_rejected
            );
        }

        // Fuzzy duplicate detection + auto-merge high-confidence dupes
        let fuzzy_dupes = self.find_fuzzy_duplicates().unwrap_or_default();
        let merge_candidates = fuzzy_dupes.iter().filter(|d| d.3 == "merge").count();
        let auto_merged = self.auto_merge_duplicates(&fuzzy_dupes).unwrap_or(0);

        // Topic coverage
        let topic_coverage = self.topic_coverage_analysis().unwrap_or_default();
        let sparse_topics = topic_coverage.iter().filter(|t| t.3 < 0.01).count();

        // Name subsumption detection (abbreviated entity forms)
        let subsumptions = self.find_name_subsumptions().unwrap_or_default();

        // Fix misclassified entity types
        let types_fixed = self.fix_entity_types().unwrap_or(0);

        // Infer entity types from neighborhood context
        let types_inferred = self.infer_types_from_neighborhood().unwrap_or(0);

        // Purge noise entities (cleanup)
        let purged = self.purge_noise_entities().unwrap_or(0);

        // Bulk quality cleanup (aggressive isolated noise removal)
        let bulk_cleaned = self.bulk_quality_cleanup().unwrap_or(0);

        // Deep island cleanup (remove low-confidence isolated entities without facts)
        let deep_cleaned = self.deep_island_cleanup(0.8).unwrap_or(0);

        // Prune self-referential relations (object is substring of subject — NLP noise)
        let self_ref_pruned = self.prune_self_referential_relations().unwrap_or(0);

        // Purge orphaned surname concepts left behind by self-referential cleanup
        let surname_concepts_purged = self.purge_orphaned_surname_concepts().unwrap_or(0);

        // Prune unsupported contemporary_of relations (no other evidence)
        let contemporary_pruned = self.prune_unsupported_contemporary().unwrap_or(0);

        // Downgrade isolated contemporary_of relations (weak evidence)
        let contemporary_downgraded = self.downgrade_isolated_contemporary().unwrap_or(0);

        // Prune stranger contemporary_of (no shared non-contemporary neighbors)
        let stranger_contemporary_pruned = self.prune_stranger_contemporary().unwrap_or(0);

        // Prune low-degree contemporary_of (both endpoints have ≤2 non-contemporary rels)
        let low_degree_contemporary_pruned = self.prune_low_degree_contemporary(2).unwrap_or(0);

        // Predicate normalization (reduce "is" overuse)
        let normalized = self.normalize_predicates().unwrap_or(0);

        // Demote uniform-predicate hubs (entities where >90% of relations use one predicate)
        let uniform_demoted = self.demote_uniform_hubs(5, 0.90).unwrap_or(0);

        // Island reconnection
        let reconnected = self.reconnect_islands().unwrap_or(0);

        // Fact-based relation inference
        let fact_inferred = self.infer_relations_from_facts().unwrap_or(0);

        // High-confidence prefix variant merge (cross-type, 3x degree threshold)
        // Run BEFORE compound decomposition to avoid degree inflation
        let hc_prefix_merged = self.merge_high_confidence_prefix_variants().unwrap_or(0);

        // Entity name cross-referencing
        let name_crossrefs = self.crossref_entity_names().unwrap_or(0);

        // Compound entity decomposition (break "Ada Lovelace Building" → named_after → "Ada Lovelace")
        let (compound_rels, compound_merged) = self.decompose_compound_entities().unwrap_or((0, 0));

        // Split concatenated entity names ("Caucasus Crimea Balkans" → components)
        let (concat_rels, concat_cleaned) = self.split_concatenated_entities().unwrap_or((0, 0));

        // Prefix island consolidation (merge "Euler" island → "Leonhard Euler" connected)
        let prefix_merged = self.consolidate_prefix_islands().unwrap_or(0);

        // Suffix-strip island merge ("Ada Lovelace WIRED" → "Ada Lovelace")
        let suffix_merged = self.suffix_strip_island_merge().unwrap_or(0);

        // Word-overlap island merging (aggressive: merge "Marie Curie Avenue" → "Marie Curie")
        let word_merged = self.word_overlap_island_merge(0.6).unwrap_or(0);

        // Fragment island purging (remove single-word fragments like "Lovelace", "Hopper")
        let fragment_purged = self.purge_fragment_islands().unwrap_or(0);

        // Prefix-strip island merge ("Christy Grace Hopper" → "Grace Hopper" by stripping leading words)
        let prefix_strip_merged = self.prefix_strip_island_merge().unwrap_or(0);

        // Name variant merge: merge titled/suffixed variants into canonical forms
        // ("Professor Claude Shannon" → "Claude Shannon", "Claude Shannon Time" → "Claude Shannon")
        let name_variants_merged = self.merge_name_variants().unwrap_or(0);

        // Auto-consolidation: merge high-scoring entity pairs from consolidation analysis
        let auto_consolidated = self.auto_consolidate_entities(0.65).unwrap_or(0);

        // Dissolve single-word name fragment hubs ("Charles" → "Charles Babbage", etc.)
        let fragments_dissolved = self.dissolve_name_fragment_hubs().unwrap_or(0);

        // Multi-pass convergence: repeat merge strategies until no more progress
        let mut convergence_merges = 0usize;
        for _pass in 0..3 {
            let mut pass_merges = 0usize;
            pass_merges += self.dedup_exact_name_matches().unwrap_or(0);
            pass_merges += self.suffix_strip_island_merge().unwrap_or(0);
            pass_merges += self.consolidate_prefix_islands().unwrap_or(0);
            pass_merges += self.word_overlap_island_merge(0.6).unwrap_or(0);
            pass_merges += self.reverse_containment_island_merge().unwrap_or(0);
            pass_merges += self.purge_fragment_islands().unwrap_or(0);
            pass_merges += self.prefix_strip_island_merge().unwrap_or(0);
            pass_merges += self.merge_name_variants().unwrap_or(0);
            pass_merges += self.merge_abbreviation_variants().unwrap_or(0);
            pass_merges += self.aggressive_prefix_dedup().unwrap_or(0);
            pass_merges += self.merge_high_confidence_prefix_variants().unwrap_or(0);
            pass_merges += self.dissolve_name_fragment_hubs().unwrap_or(0);
            pass_merges += self.strip_leading_adjectives().unwrap_or(0);
            pass_merges += self.merge_reversed_names().unwrap_or(0);
            pass_merges += self.merge_nickname_variants().unwrap_or(0);
            pass_merges += self.merge_transliteration_variants().unwrap_or(0);
            if pass_merges == 0 {
                break;
            }
            convergence_merges += pass_merges;
        }

        // Merge abbreviation variants (St→Saint, Mt→Mount, etc.)
        let abbreviation_merged = self.merge_abbreviation_variants().unwrap_or(0);

        // Merge connected entities where one name contains the other
        let containment_merged = self.merge_connected_containment().unwrap_or(0);

        // Aggressive same-type prefix deduplication
        let aggressive_deduped = self.aggressive_prefix_dedup().unwrap_or(0);

        // Purge generic single-word islands (adverbs, adjectives, citation surnames)
        let generic_purged = self.purge_generic_single_word_islands().unwrap_or(0);

        // Purge multi-word island noise (fragments, misclassifications)
        let multiword_purged = self.purge_multiword_island_noise().unwrap_or(0);

        // Purge fragment island entities (NLP extraction errors)
        let fragment_islands_purged = self.purge_fragment_island_entities().unwrap_or(0);

        // Purge single-word concept islands (adjectives, common nouns, non-English stubs)
        let concept_islands_purged = self.purge_single_word_concept_islands().unwrap_or(0);

        // Purge mistyped person islands (multi-word "person" entities that aren't people)
        let mistyped_person_purged = self.purge_mistyped_person_islands().unwrap_or(0);

        // Purge low-confidence concept islands (NLP noise with confidence ≤ 0.55 and no facts)
        let low_conf_purged = self.purge_low_confidence_concept_islands(0.60).unwrap_or(0);

        // Reclassify concept islands that match known place names
        let concepts_to_places = self.reclassify_concept_islands_as_places().unwrap_or(0);

        // Fix country-concatenation entities (e.g., "Netherlands Oskar Klein" → merge into "Oskar Klein")
        let country_concat_fixed = self.fix_country_concatenation_entities().unwrap_or(0);

        // Purge topic-prefix concatenation entities (e.g., "Punic Carthage" → "Carthage")
        let topic_prefix_purged = self.purge_topic_prefix_entities().unwrap_or(0);

        // Merge prefix-noise entities (e.g., "Devastated Tim Berners-Lee" → "Tim Berners-Lee")
        let prefix_noise_merged = self.merge_prefix_noise_entities().unwrap_or(0);

        // Merge reversed names ("Neumann John" → "John von Neumann")
        let reversed_names_merged = self.merge_reversed_names().unwrap_or(0);

        // Nickname/diminutive expansion merge (Alex→Alexander, etc.)
        let nickname_merged = self.merge_nickname_variants().unwrap_or(0);

        // Transliteration variant merge (edit distance on same-surname entities)
        let translit_merged = self.merge_transliteration_variants().unwrap_or(0);

        // Extended concept reclassification (well-known places/tech misclassified as concept)
        let concepts_reclassified = self.reclassify_isolated_concepts_extended().unwrap_or(0);

        // Graph-informed concept reclassification (match island names to connected entity surnames)
        let graph_surname_reclassified = self
            .reclassify_concept_islands_by_graph_surnames()
            .unwrap_or(0);

        // Purge common English word concept islands
        let common_word_purged = self.purge_common_word_concept_islands().unwrap_or(0);

        // Reclassify connected single-word concepts that match known person surnames
        let surname_concepts_reclassified =
            self.reclassify_connected_surname_concepts().unwrap_or(0);

        // Purge isolated person entities with all-common-English-word names
        let common_english_purged = self.purge_common_english_person_islands().unwrap_or(0);

        // Reclassify person islands matching geographic/event/concept patterns
        let geo_reclassified = self.reclassify_geographic_person_islands().unwrap_or(0);

        // Reclassify "CityName RegionName" person islands to place
        let place_region_reclassified = self.reclassify_place_region_person_islands().unwrap_or(0);

        // Reclassify two-word compound concept persons (e.g., "Aneutronic Fusion")
        let compound_reclassified = self.reclassify_compound_concept_persons().unwrap_or(0);

        // Reclassify connected person entities with obvious geographic/institutional suffixes
        let geo_connected_reclassified =
            self.reclassify_geographic_persons_connected().unwrap_or(0);

        // Reclassify en-dash compound names (e.g., "Calabi–Yau" → concept)
        let endash_reclassified = self.reclassify_endash_compound_islands().unwrap_or(0);

        // Purge ancient low-confidence islands (>30 days old, conf ≤0.5, no facts/relations)
        let ancient_purged = self.purge_ancient_low_confidence_islands(30).unwrap_or(0);

        // Purge NLP concatenation noise ("Examples Yang", "Neumann John Examples")
        let concat_noise_purged = self.purge_concatenation_noise().unwrap_or(0);

        // Token-based island reconnection (TF-IDF shared token matching)
        let token_reconnected = self.reconnect_islands_by_tokens().unwrap_or(0);

        // Name-containment island reconnection (substring matching within same type)
        let name_containment_reconnected =
            self.reconnect_islands_by_name_containment().unwrap_or(0);

        // Single-word island reconnection (match isolated single-word entities to connected multi-word entities)
        let single_word_reconnected = self.reconnect_single_word_islands().unwrap_or(0);

        // Cross-component merger: find entities in different components with matching names
        let cross_component_merged = self.merge_cross_component_duplicates().unwrap_or(0);

        // TF-IDF weighted island reconnection (semantic token matching)
        let tfidf_reconnected = self.tfidf_island_reconnect().unwrap_or(0);

        // Source-cohort island reconnection (temporal co-extraction)
        let source_reconnected = self.reconnect_islands_by_source().unwrap_or(0);

        // Broader minute-level temporal cohort reconnection
        let minute_cohort_reconnected = self.reconnect_islands_by_minute_cohort().unwrap_or(0);

        // Large cohort type-partitioned reconnection (handles cohorts >50 that minute-cohort skips)
        let large_cohort_reconnected = self.reconnect_large_cohorts_by_type().unwrap_or(0);

        // Domain-keyword island reconnection (high-value entities with shared name tokens)
        let domain_reconnected = self.reconnect_islands_by_domain().unwrap_or(0);

        // Fact-value bridge: connect isolated entities to entities mentioned in their fact values
        let fact_value_bridged = self.reconnect_via_fact_values().unwrap_or(0);

        // Predicate-object token island reconnection (match island names to relation objects)
        let pred_obj_reconnected = self
            .reconnect_islands_by_predicate_object_tokens()
            .unwrap_or(0);

        // Reclassify island concepts by token-type voting from connected graph
        let token_type_reclassified = self.reclassify_islands_by_token_type().unwrap_or(0);

        // Purge foreign-language extraction noise from islands
        let foreign_purged = self.purge_foreign_language_islands().unwrap_or(0);

        // Reconnect high-value isolated entities via surname/source matching
        let hv_island_reconnected = self.reconnect_high_value_islands_by_surname().unwrap_or(0);

        // Reconnect high-value isolated entities via temporal cohort (no hub required)
        let hv_cohort_reconnected = self.reconnect_high_value_islands().unwrap_or(0);

        // Surname-only island reconnection (broader reach than high-value reconnect)
        let surname_reconnected = self.reconnect_islands_by_surname_match().unwrap_or(0);

        // Source-URL name matching (connect islands whose names appear in relation source URLs)
        let source_url_reconnected = self.reconnect_islands_by_source_url_match().unwrap_or(0);

        // Purge single-word participle/adjective concepts (NLP noise like "Entangled", "Interacting")
        // Runs AFTER reconnection strategies to catch noise that was re-connected
        let participle_purged = self.purge_participle_concept_entities().unwrap_or(0);

        // Purge person islands with trailing noise words (NLP fragments)
        let trailing_fragment_purged = self.purge_trailing_fragment_person_islands().unwrap_or(0);

        // Purge Latin phrase person islands
        let latin_phrase_purged = self.purge_latin_phrase_person_islands().unwrap_or(0);

        // Reconnect isolated places via geographic context tokens
        let geographic_reconnected = self.reconnect_places_by_geographic_context().unwrap_or(0);

        // Boost confidence of well-known entities
        let known_boosted = self.boost_known_entity_confidence().unwrap_or(0);

        // Auto-seed frontier with Wikipedia URLs for high-value isolated entities
        let frontier_seeded = self.seed_frontier_for_isolated_entities(50).unwrap_or(0);

        // Seed frontier for connected but fact-poor entities (enrichment targets)
        let enrichment_seeded = self.seed_frontier_for_fact_poor_entities(20).unwrap_or(0);

        // Detect discovery plateau and log recommendation
        let (is_plateau, _plateau_metric, plateau_rec) = self
            .detect_discovery_plateau(10)
            .unwrap_or((false, 0.0, String::new()));
        if is_plateau {
            eprintln!("[PROMETHEUS] ⚠ Discovery plateau detected: {}", plateau_rec);
        }

        // Refine generic predicates using entity type pairs
        let predicates_refined = self.refine_associated_with().unwrap_or(0);
        let contextual_refined = self.refine_associated_with_contextual().unwrap_or(0);
        let contributed_refined = self.refine_contributed_to().unwrap_or(0);

        // Refine generic "related_to" predicates into type-specific ones
        let related_to_refined = self.refine_related_to().unwrap_or(0);

        // Refine contemporary_of using shared-neighbor evidence
        let contemporary_refined = self.refine_contemporary_of().unwrap_or(0);

        // Refine overused "pioneered" into type-specific predicates
        let pioneered_refined = self.refine_pioneered().unwrap_or(0);

        // Refine mismatched "active_in" relations by entity type pairs
        let active_in_refined = self.refine_active_in().unwrap_or(0);

        // Demote weak contemporary_of relations lacking shared-neighbor evidence
        let weak_contemporary_demoted = self.demote_weak_contemporary().unwrap_or(0);

        // Prune redundant contemporary_of (already-connected pairs + non-person + domain-refined)
        let redundant_contemporary_pruned = self.prune_redundant_contemporary().unwrap_or(0);
        if redundant_contemporary_pruned > 0 {
            eprintln!(
                "[PROMETHEUS] Pruned/refined {} redundant contemporary_of relations",
                redundant_contemporary_pruned
            );
        }

        // Promote mature testing hypotheses (>7 days, confidence >= 0.75)
        let promoted = self.promote_mature_hypotheses(7, 0.75).unwrap_or(0);

        // Strategy-aware accelerated promotion: for strategies with >85% historical
        // confirmation rate, promote testing hypotheses at a lower threshold (0.60)
        // after just 3 days. High-ROI strategies have earned this trust.
        let accelerated_promoted = self
            .promote_high_roi_mature_hypotheses(3, 0.60, 0.85)
            .unwrap_or(0);
        if accelerated_promoted > 0 {
            eprintln!(
                "[PROMETHEUS] Accelerated promotion: {} hypotheses from high-ROI strategies",
                accelerated_promoted
            );
        }

        // Decay stale hypotheses (>14 days testing, 5% decay per cycle, reject below 0.15)
        let (h_decayed, h_auto_rej) = self
            .decay_stale_hypotheses(14, 0.95, 0.15)
            .unwrap_or((0, 0));
        if h_decayed > 0 || h_auto_rej > 0 {
            eprintln!(
                "[PROMETHEUS] Hypothesis decay: {} decayed, {} auto-rejected",
                h_decayed, h_auto_rej
            );
        }

        // Cross-strategy reinforcement: boost hypotheses supported by multiple strategies
        let cross_reinforced = self.cross_strategy_reinforcement().unwrap_or(0);

        // Hypothesis pair deduplication (prevent bloat from multiple runs)
        let pair_deduped = self.dedup_hypotheses_by_pair().unwrap_or(0);

        // Remove symmetric duplicates (A contemporary_of B == B contemporary_of A)
        let symmetric_deduped = self.dedup_symmetric_hypotheses().unwrap_or(0);
        if symmetric_deduped > 0 {
            eprintln!(
                "[PROMETHEUS] Removed {} symmetric duplicate hypotheses",
                symmetric_deduped
            );
        }

        // Cap total hypothesis count to prevent unbounded DB growth
        let hyp_capped = self.cap_hypothesis_count(2000).unwrap_or(0);

        // Network motif census (structural patterns)
        let motifs = crate::graph::motif_census(self.brain).unwrap_or_default();

        // K-core analysis: find the dense backbone
        let (max_k, core_members) =
            crate::graph::densest_core(self.brain, 3).unwrap_or((0, vec![]));

        // Save graph snapshot for trend tracking
        let _snapshot_id = crate::graph::save_graph_snapshot(self.brain).unwrap_or(0);

        // Graph quality score
        let (quality_score, quality_components) =
            crate::graph::graph_quality_score(self.brain).unwrap_or((0.0, HashMap::new()));

        // Track discovery velocity (include auto-resolved counts for accurate metrics)
        let total_confirmed = confirmed + auto_confirmed + excl_promoted;
        let total_rejected = rejected + auto_rejected + excl_rejected + bulk_rejected;
        let _ = self.track_discovery_velocity(
            all_patterns.len(),
            all_hypotheses.len(),
            total_confirmed,
            total_rejected,
        );

        // Discovery health check (trend analysis + hypothesis clustering)
        let health_line = self
            .discovery_health_check()
            .map(|h| format!(", health: {}", h))
            .unwrap_or_default();

        // Get trend comparison if previous snapshots exist
        let trend_line = {
            let snapshots = crate::graph::get_graph_snapshots(self.brain, 2).unwrap_or_default();
            if snapshots.len() >= 2 {
                format!(
                    ", trend: {}",
                    crate::graph::format_trend(&snapshots[0], &snapshots[1])
                )
            } else {
                String::new()
            }
        };

        let summary = format!(
            "Discovered {} patterns, generated {} new hypotheses ({} confirmed, {} rejected), \
             auto-resolved {} existing ({}✓ {}✗), {} island entities, {} meaningful relations, \
             {} cross-domain gaps, {} knowledge frontiers, {} bridge entities, {} connectable islands, \
             {} prioritized gaps, {} decayed, {} pruned, {} island clusters, \
             predicate diversity: {}/{} ({:.0}% diverse), \
             {} exact-name deduped, {} reverse-containment merged, {} bulk-rejected stale, \
             {} fuzzy duplicates ({} auto-merge, {} auto-merged), {} sparse topic domains, \
             {} name subsumptions found, {} types fixed, {} types inferred from neighborhood, {} noise entities purged, \
             {} bulk cleaned, {} deep cleaned, {} self-ref pruned, {} surname-concepts purged, {} contemporary pruned, {} contemporary downgraded, {} predicates normalized, {} uniform-hub demoted, {} islands reconnected, \
             {} fact-inferred relations, {} name cross-references, \
             {} compound relations + {} compound merged, {} concat split ({}r/{}c), {} prefix-merged, {} suffix-merged, {} word-overlap merged, \
             {} fragment-purged, {} prefix-strip merged, {} name-variants merged, {} auto-consolidated, \
             {} fragment-hubs dissolved, {} hc-prefix merged, {} convergence-pass merges, \
             {} connected-containment merged, {} aggressive-prefix deduped, \
             {} generic islands purged, {} multiword noise purged, {} fragment islands purged, {} concept islands purged, {} mistyped-person purged, {} low-conf concepts purged, {} participle-concepts purged, {} concepts→places, {} country-concat fixed, {} topic-prefix purged, {} prefix-noise merged, {} reversed-names merged, {} nickname-merged, {} transliteration-merged, {} concepts-reclassified-ext, {} graph-surname-reclassified, {} common-word-purged, {} surname-concepts-reclassified, {} common-english-person-purged, {} geo-person-reclassified, {} place-region-reclassified, {} compound-concept-reclassified, {} geo-connected-reclassified, {} endash-reclassified, {} ancient-islands-purged, {} abbreviation-merged, {} concat-noise purged, {} token-reconnected, {} name-containment reconnected, {} single-word reconnected, {} cross-component merged, {} tfidf-reconnected, {} source-cohort reconnected, {} minute-cohort reconnected, {} large-cohort-type reconnected, {} domain-keyword reconnected, {} fact-value bridged, {} pred-obj-token reconnected, {} token-type reclassified, {} foreign-purged, {} hv-island-reconnected, {} hv-cohort-reconnected, {} surname-reconnected, {} source-url-reconnected, {} known-boosted, {} predicates refined, {} contextual-refined, {} contributed_to refined, {} related_to refined, {} contemporary_of refined, {} pioneered refined, {} active_in refined, {} weak contemporary demoted, {} redundant contemporary pruned, {} hypotheses promoted ({} accelerated), \
             {} fragment hypotheses cleaned, {} concat-entity hypotheses rejected, \
             {} mutual-exclusion ({}✓ {}✗), \
             {} cross-strategy reinforced, {} hypothesis pairs deduped, {} hypotheses capped, k-core: k={} with {} entities in dense backbone, \
             motifs: {}△ {}ff {}ch {}↔{}",
            all_patterns.len(),
            all_hypotheses.len(),
            confirmed,
            rejected,
            auto_confirmed + auto_rejected,
            auto_confirmed,
            auto_rejected,
            islands.len(),
            meaningful_rels,
            cross_gaps.len(),
            frontiers.len(),
            bridges.len(),
            connectable.len(),
            gap_priorities.len(),
            decayed,
            pruned,
            island_clusters.len(),
            diverse_rels,
            total_rels,
            div_ratio * 100.0,
            exact_deduped,
            reverse_merged,
            bulk_rejected,
            fuzzy_dupes.len(),
            merge_candidates,
            auto_merged,
            sparse_topics,
            subsumptions.len(),
            types_fixed,
            types_inferred,
            purged,
            bulk_cleaned,
            deep_cleaned,
            self_ref_pruned,
            surname_concepts_purged,
            contemporary_pruned,
            contemporary_downgraded,
            normalized,
            uniform_demoted,
            reconnected,
            fact_inferred,
            name_crossrefs,
            compound_rels,
            compound_merged,
            concat_rels + concat_cleaned,
            concat_rels,
            concat_cleaned,
            prefix_merged,
            suffix_merged,
            word_merged,
            fragment_purged,
            prefix_strip_merged,
            name_variants_merged,
            auto_consolidated,
            fragments_dissolved,
            hc_prefix_merged,
            convergence_merges,
            containment_merged,
            aggressive_deduped,
            generic_purged,
            multiword_purged,
            fragment_islands_purged,
            concept_islands_purged,
            mistyped_person_purged,
            low_conf_purged,
            participle_purged,
            concepts_to_places,
            country_concat_fixed,
            topic_prefix_purged,
            prefix_noise_merged,
            reversed_names_merged,
            nickname_merged,
            translit_merged,
            concepts_reclassified,
            graph_surname_reclassified,
            common_word_purged,
            surname_concepts_reclassified,
            common_english_purged,
            geo_reclassified,
            place_region_reclassified,
            compound_reclassified,
            geo_connected_reclassified,
            endash_reclassified,
            ancient_purged,
            abbreviation_merged,
            concat_noise_purged,
            token_reconnected,
            name_containment_reconnected,
            single_word_reconnected,
            cross_component_merged,
            tfidf_reconnected,
            source_reconnected,
            minute_cohort_reconnected,
            large_cohort_reconnected,
            domain_reconnected,
            fact_value_bridged,
            pred_obj_reconnected,
            token_type_reclassified,
            foreign_purged,
            hv_island_reconnected,
            hv_cohort_reconnected,
            surname_reconnected,
            source_url_reconnected,
            known_boosted,
            predicates_refined,
            contextual_refined,
            contributed_refined,
            related_to_refined,
            contemporary_refined,
            pioneered_refined,
            active_in_refined,
            weak_contemporary_demoted,
            redundant_contemporary_pruned,
            promoted,
            accelerated_promoted,
            fragment_cleaned,
            concat_hyp_rejected,
            excl_promoted + excl_rejected,
            excl_promoted,
            excl_rejected,
            cross_reinforced,
            pair_deduped,
            hyp_capped,
            max_k,
            core_members.len(),
            trend_line,
            motifs.get("triangles").copied().unwrap_or(0),
            motifs.get("feed_forward_loops").copied().unwrap_or(0),
            motifs.get("chains").copied().unwrap_or(0),
            motifs.get("mutual_dyads").copied().unwrap_or(0),
        );
        // Append health check and strategy diversity to summary
        let diversity_line = self
            .strategy_diversity()
            .map(|(ent, dom, frac, rec)| {
                format!(
                    ", strategy diversity: {:.2} bits (dominant: {} at {:.0}%, {})",
                    ent,
                    dom,
                    frac * 100.0,
                    rec
                )
            })
            .unwrap_or_default();
        let quality_line = format!(
            ", quality: {:.1}% (conn:{:.0}% dens:{:.0}% coh:{:.0}% pred:{:.0}% type:{:.0}%)",
            quality_score * 100.0,
            quality_components.get("connectivity").unwrap_or(&0.0) * 100.0,
            quality_components.get("density").unwrap_or(&0.0) * 100.0,
            quality_components.get("cohesion").unwrap_or(&0.0) * 100.0,
            quality_components
                .get("predicate_diversity")
                .unwrap_or(&0.0)
                * 100.0,
            quality_components.get("type_diversity").unwrap_or(&0.0) * 100.0,
        );
        let plateau_line = if is_plateau {
            format!(", ⚠ PLATEAU: {}", plateau_rec)
        } else {
            String::new()
        };
        let frontier_line = if frontier_seeded > 0 || enrichment_seeded > 0 {
            format!(
                ", {} frontier URLs seeded ({} enrichment)",
                frontier_seeded + enrichment_seeded,
                enrichment_seeded
            )
        } else {
            String::new()
        };
        let stranger_line =
            if stranger_contemporary_pruned > 0 || low_degree_contemporary_pruned > 0 {
                format!(
                    ", {} stranger contemporary_of pruned, {} low-degree contemporary_of pruned",
                    stranger_contemporary_pruned, low_degree_contemporary_pruned
                )
            } else {
                String::new()
            };
        let new_cleanup_line =
            if trailing_fragment_purged + latin_phrase_purged + geographic_reconnected > 0 {
                format!(
                ", {} trailing-fragment-purged, {} latin-phrase-purged, {} geographic-reconnected",
                trailing_fragment_purged, latin_phrase_purged, geographic_reconnected
            )
            } else {
                String::new()
            };
        let summary = format!(
            "{}{}{}{}{}{}{}{}",
            summary,
            health_line,
            diversity_line,
            quality_line,
            plateau_line,
            frontier_line,
            stranger_line,
            new_cleanup_line
        );

        Ok(DiscoveryReport {
            patterns_found: all_patterns,
            hypotheses_generated: all_hypotheses,
            gaps_detected,
            summary,
        })
    }

    // -----------------------------------------------------------------------
    // Cross-domain gap detection
    // -----------------------------------------------------------------------

    /// Find disconnected clusters that likely should be connected based on entity types.
    pub fn find_cross_domain_gaps(&self) -> Result<Vec<(Vec<String>, Vec<String>, String)>> {
        let components = crate::graph::connected_components(self.brain)?;
        if components.len() < 2 {
            return Ok(vec![]);
        }
        let entities = self.brain.all_entities()?;
        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        // For each component, collect entity types (excluding noise types)
        let noise_types: HashSet<&str> = ["phrase", "source", "url"].iter().copied().collect();
        let mut component_types: Vec<(Vec<String>, HashMap<String, usize>)> = Vec::new();
        for comp in &components {
            // Skip tiny components (likely noise fragments)
            if comp.len() < 3 {
                continue;
            }
            let mut types: HashMap<String, usize> = HashMap::new();
            let mut names: Vec<String> = Vec::new();
            for &id in comp {
                if let Some(e) = id_to_entity.get(&id) {
                    if !noise_types.contains(e.entity_type.as_str()) && !is_noise_name(&e.name) {
                        *types.entry(e.entity_type.clone()).or_insert(0) += 1;
                        if names.len() < 5 && !is_noise_name(&e.name) {
                            names.push(e.name.clone());
                        }
                    }
                }
            }
            if !types.is_empty() && names.len() >= 2 {
                component_types.push((names, types));
            }
        }

        // Find pairs of components sharing high-value entity types
        let skip_types: HashSet<&str> = ["unknown", "phrase", "source", "url"]
            .iter()
            .copied()
            .collect();
        let mut gaps = Vec::new();
        for i in 0..component_types.len().min(20) {
            for j in (i + 1)..component_types.len().min(20) {
                let (names_i, types_i) = &component_types[i];
                let (names_j, types_j) = &component_types[j];
                let shared: Vec<String> = types_i
                    .keys()
                    .filter(|t| types_j.contains_key(*t) && !skip_types.contains(t.as_str()))
                    .cloned()
                    .collect();
                if !shared.is_empty() {
                    gaps.push((
                        names_i.clone(),
                        names_j.clone(),
                        format!(
                            "Clusters share types [{}] but are disconnected",
                            shared.join(", ")
                        ),
                    ));
                }
            }
        }
        Ok(gaps)
    }

    /// Find entities that bridge multiple communities — potential cross-domain connectors.
    pub fn find_bridge_entities(&self) -> Result<Vec<(String, String, usize)>> {
        let components = crate::graph::connected_components(self.brain)?;
        if components.len() < 2 {
            return Ok(vec![]);
        }
        // Map entity → component index
        let mut entity_comp: HashMap<i64, usize> = HashMap::new();
        for (idx, comp) in components.iter().enumerate() {
            for &id in comp {
                entity_comp.insert(id, idx);
            }
        }
        // Find entities whose neighbours span multiple components (via relations)
        let relations = self.brain.all_relations()?;
        let mut entity_comps: HashMap<i64, HashSet<usize>> = HashMap::new();
        for r in &relations {
            if let Some(&comp) = entity_comp.get(&r.object_id) {
                entity_comps.entry(r.subject_id).or_default().insert(comp);
            }
            if let Some(&comp) = entity_comp.get(&r.subject_id) {
                entity_comps.entry(r.object_id).or_default().insert(comp);
            }
        }
        let mut bridges: Vec<(String, String, usize)> = Vec::new();
        for (eid, comps) in &entity_comps {
            if comps.len() >= 2 {
                let name = self.entity_name(*eid)?;
                let etype = self
                    .brain
                    .get_entity_by_id(*eid)?
                    .map(|e| e.entity_type)
                    .unwrap_or_default();
                if !is_noise_name(&name) && !is_noise_type(&etype) {
                    bridges.push((name, etype, comps.len()));
                }
            }
        }
        bridges.sort_by(|a, b| b.2.cmp(&a.2));
        bridges.truncate(20);
        Ok(bridges)
    }

    /// Prune self-referential relations where the object name is a substring
    /// of the subject name (NLP surname/fragment extraction errors).
    /// e.g. "Jeffrey Goldstone" pioneered "Goldstone" — the "concept" is just the surname.
    pub fn prune_self_referential_relations(&self) -> Result<usize> {
        let pruned = self.brain.with_conn(|conn| {
            // Delete relations where LOWER(object.name) is a substring of LOWER(subject.name)
            // and the object name is short (≤ 2 words) — longer names are more likely real concepts.
            let count = conn.execute(
                "DELETE FROM relations WHERE id IN (
                    SELECT r.id FROM relations r
                    JOIN entities s ON r.subject_id = s.id
                    JOIN entities o ON r.object_id = o.id
                    WHERE LOWER(s.name) LIKE '%' || LOWER(o.name) || '%'
                    AND LENGTH(o.name) - LENGTH(REPLACE(o.name, ' ', '')) < 2
                    AND o.name != s.name
                    AND LENGTH(o.name) >= 2
                )",
                [],
            )?;
            Ok(count)
        })?;
        if pruned > 0 {
            eprintln!(
                "[PROMETHEUS] Pruned {} self-referential relations (object is substring of subject)",
                pruned
            );
        }
        Ok(pruned)
    }

    /// Prune orphaned concept entities that were only targets of self-referential
    /// relations (surname fragments like "Goldstone", "Rossby", etc.)
    pub fn purge_orphaned_surname_concepts(&self) -> Result<usize> {
        let purged = self.brain.with_conn(|conn| {
            // Delete concept entities that:
            // 1. Are single-word
            // 2. Have no remaining relations (subject or object)
            // 3. Have no facts
            // 4. Are typed as "concept"
            let count = conn.execute(
                "DELETE FROM entities WHERE id IN (
                    SELECT e.id FROM entities e
                    WHERE e.entity_type = 'concept'
                    AND e.name NOT LIKE '% %'
                    AND LENGTH(e.name) BETWEEN 3 AND 20
                    AND e.id NOT IN (SELECT subject_id FROM relations)
                    AND e.id NOT IN (SELECT object_id FROM relations)
                    AND e.id NOT IN (SELECT entity_id FROM facts)
                    AND e.confidence < 0.8
                )",
                [],
            )?;
            Ok(count)
        })?;
        if purged > 0 {
            eprintln!(
                "[PROMETHEUS] Purged {} orphaned single-word concept entities (post-relation cleanup)",
                purged
            );
        }
        Ok(purged)
    }

    /// Prune `contemporary_of` relations where neither entity has any other
    /// predicate — these are pure type-default noise with no supporting evidence.
    pub fn prune_unsupported_contemporary(&self) -> Result<usize> {
        let pruned = self.brain.with_conn(|conn| {
            let count = conn.execute(
                "DELETE FROM relations WHERE predicate = 'contemporary_of'
                 AND NOT EXISTS (
                     SELECT 1 FROM relations r2
                     WHERE (r2.subject_id = relations.subject_id OR r2.object_id = relations.subject_id)
                     AND r2.predicate != 'contemporary_of' AND r2.id != relations.id
                 )
                 AND NOT EXISTS (
                     SELECT 1 FROM relations r3
                     WHERE (r3.subject_id = relations.object_id OR r3.object_id = relations.object_id)
                     AND r3.predicate != 'contemporary_of' AND r3.id != relations.id
                 )",
                [],
            )?;
            Ok(count)
        })?;
        Ok(pruned)
    }

    /// Downgrade low-confidence `contemporary_of` where entities share no
    /// neighbors at all (not even via contemporary_of chains). These are
    /// weakest-link relations that inflate the graph without adding signal.
    pub fn downgrade_isolated_contemporary(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        // Build adjacency excluding contemporary_of
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            if r.predicate == "contemporary_of" {
                continue;
            }
            adj.entry(r.subject_id).or_default().insert(r.object_id);
            adj.entry(r.object_id).or_default().insert(r.subject_id);
        }

        let mut downgraded = 0usize;
        for r in &relations {
            if r.predicate != "contemporary_of" || r.confidence <= 0.3 {
                continue;
            }
            let s_neighbors = adj.get(&r.subject_id).map(|s| s.len()).unwrap_or(0);
            let o_neighbors = adj.get(&r.object_id).map(|s| s.len()).unwrap_or(0);
            // Both entities have some non-contemporary connections → keep
            if s_neighbors >= 2 && o_neighbors >= 2 {
                continue;
            }
            // At least one entity is poorly connected outside contemporary_of → downgrade
            let ok = self.brain.with_conn(|conn| {
                conn.execute(
                    "UPDATE relations SET confidence = 0.3 WHERE id = ?1 AND confidence > 0.3",
                    params![r.id],
                )?;
                Ok(true)
            })?;
            if ok {
                downgraded += 1;
            }
        }
        Ok(downgraded)
    }

    /// Prune `contemporary_of` relations between entities that share zero
    /// mutual neighbors via non-contemporary predicates and have confidence ≤ 0.3.
    /// These are the weakest temporal co-existence signals — entities that merely
    /// existed in the same era but have no topical overlap. Removing them improves
    /// predicate entropy and reduces noise in community detection.
    pub fn prune_stranger_contemporary(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;

        // Build adjacency from non-contemporary predicates only
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            if r.predicate == "contemporary_of" {
                continue;
            }
            adj.entry(r.subject_id).or_default().insert(r.object_id);
            adj.entry(r.object_id).or_default().insert(r.subject_id);
        }

        let contemporary_rels: Vec<_> = relations
            .iter()
            .filter(|r| r.predicate == "contemporary_of" && r.confidence <= 0.50)
            .collect();

        let mut pruned = 0usize;
        for r in &contemporary_rels {
            let s_neighbors = adj.get(&r.subject_id).cloned().unwrap_or_default();
            let o_neighbors = adj.get(&r.object_id).cloned().unwrap_or_default();
            let mutual = s_neighbors.intersection(&o_neighbors).count();
            if mutual == 0 {
                // No shared neighbors via meaningful predicates — pure temporal noise
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                    Ok(())
                })?;
                pruned += 1;
            }
        }
        Ok(pruned)
    }

    /// Prune contemporary_of relations where both endpoints are low-degree
    /// (≤ max_other_degree non-contemporary relations each). These are weak
    /// temporal inferences that inflate edge count without real knowledge.
    pub fn prune_low_degree_contemporary(&self, max_other_degree: usize) -> Result<usize> {
        let relations = self.brain.all_relations()?;

        // Count non-contemporary degree for each entity
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            if r.predicate != "contemporary_of" {
                *degree.entry(r.subject_id).or_insert(0) += 1;
                *degree.entry(r.object_id).or_insert(0) += 1;
            }
        }

        let contemporary_rels: Vec<_> = relations
            .iter()
            .filter(|r| r.predicate == "contemporary_of")
            .collect();

        let mut pruned = 0usize;
        for r in &contemporary_rels {
            let s_deg = degree.get(&r.subject_id).copied().unwrap_or(0);
            let o_deg = degree.get(&r.object_id).copied().unwrap_or(0);
            if s_deg <= max_other_degree && o_deg <= max_other_degree {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                    Ok(())
                })?;
                pruned += 1;
            }
        }
        Ok(pruned)
    }

    /// Decay confidence of old unconfirmed hypotheses (meta-learning cleanup).
    pub fn decay_old_hypotheses(&self, max_age_days: i64) -> Result<usize> {
        let cutoff = (Utc::now() - chrono::Duration::days(max_age_days))
            .naive_utc()
            .format("%Y-%m-%d %H:%M:%S")
            .to_string();
        let count = self.brain.with_conn(|conn| {
            let updated = conn.execute(
                "UPDATE hypotheses SET confidence = confidence * 0.8 WHERE status IN ('proposed', 'testing') AND discovered_at < ?1",
                params![cutoff],
            )?;
            Ok(updated)
        })?;
        Ok(count)
    }

    /// Suggest topics to crawl based on knowledge gaps.
    pub fn suggest_crawl_topics(&self) -> Result<Vec<(String, String)>> {
        let mut suggestions = Vec::new();

        // 1. Entity types with low knowledge density (skip noise types)
        let density = crate::graph::knowledge_density(self.brain)?;
        for (etype, (count, avg)) in &density {
            if *count >= 3 && *avg < 0.5 && !is_noise_type(etype) && *etype != "unknown" {
                suggestions.push((
                    etype.clone(),
                    format!(
                        "Type '{}' has {} entities but only {:.1} avg relations — needs enrichment",
                        etype, count, avg
                    ),
                ));
            }
        }

        // 2. Entities involved in hypotheses with object "?"
        let hyps = self.list_hypotheses(Some(HypothesisStatus::Proposed))?;
        for h in hyps.iter().take(10) {
            if h.object == "?" {
                suggestions.push((
                    h.subject.clone(),
                    format!(
                        "Hypothesis: '{}' likely has '{}' but value unknown",
                        h.subject, h.predicate
                    ),
                ));
            }
        }

        // 3. High-centrality entities with few facts
        let pr = crate::graph::pagerank(self.brain, 0.85, 20)?;
        let mut ranked: Vec<(i64, f64)> = pr.into_iter().collect();
        ranked.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
        for (id, score) in ranked.iter().take(20) {
            if let Some(e) = self.brain.get_entity_by_id(*id)? {
                if e.entity_type == "phrase" || e.entity_type == "source" {
                    continue;
                }
                let facts = self.brain.get_facts_for(*id)?;
                let rels = self.brain.get_relations_for(*id)?;
                if facts.len() + rels.len() < 3 {
                    suggestions.push((
                        e.name.clone(),
                        format!(
                            "High-rank entity ({:.4}) '{}' has only {} facts+relations",
                            score,
                            e.name,
                            facts.len() + rels.len()
                        ),
                    ));
                }
            }
        }

        Ok(suggestions)
    }

    /// Rank connected entities by enrichment priority: which entities would benefit
    /// most from deeper crawling? Combines PageRank importance, low fact/relation
    /// count, and high-value entity type into a single score.
    /// Returns (entity_name, entity_type, score, reason) sorted by priority.
    pub fn rank_enrichment_targets(
        &self,
        limit: usize,
    ) -> Result<Vec<(String, String, f64, String)>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        // Only consider connected entities
        let mut connected: HashSet<i64> = HashSet::new();
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // PageRank for importance weighting
        let pr = crate::graph::pagerank(self.brain, 0.85, 20)?;

        // Type value weights
        let type_weight = |t: &str| -> f64 {
            match t {
                "person" => 1.5,
                "concept" => 1.3,
                "organization" => 1.2,
                "place" => 1.0,
                "technology" => 1.4,
                "event" => 1.1,
                _ => 0.5,
            }
        };

        let mut targets: Vec<(String, String, f64, String)> = Vec::new();
        for e in &entities {
            if !connected.contains(&e.id) || is_noise_type(&e.entity_type) || is_noise_name(&e.name)
            {
                continue;
            }
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            let rank = pr.get(&e.id).copied().unwrap_or(0.0);
            let facts = self.brain.get_facts_for(e.id)?.len();

            // Enrichment score: high importance + low knowledge = high priority
            // Entities with high PageRank but few connections/facts need enrichment most
            let sparsity = 1.0 / (1.0 + deg as f64 + facts as f64);
            let importance = rank * 10000.0; // Normalize PageRank
            let tw = type_weight(&e.entity_type);
            let score = importance * sparsity * tw;

            if score > 0.001 {
                let reason = format!(
                    "PageRank {:.4}, {} relations, {} facts — {} type",
                    rank, deg, facts, e.entity_type
                );
                targets.push((e.name.clone(), e.entity_type.clone(), score, reason));
            }
        }

        targets.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap_or(std::cmp::Ordering::Equal));
        targets.truncate(limit);
        Ok(targets)
    }

    /// Detect multi-word island entities that are likely NLP extraction artifacts.
    /// Patterns: "Noun Verb" fragments, reversed "Surname Firstname" citation format,
    /// entities containing common academic terms mixed with proper nouns.
    /// Returns count of entities purged.
    pub fn purge_multiword_island_noise(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Words that signal sentence fragments when appearing in entity names
        let fragment_signals: HashSet<&str> = [
            "quiz",
            "film",
            "printed",
            "feature",
            "mosaics",
            "explorations",
            "mission",
            "lexicon",
            "revue",
            "etablierte",
            "kirchen",
            "lycée",
            "recipients",
            "glance",
            "visualize",
            "shrieking",
            "servile",
            "thrilling",
            "arrives",
            "vessel",
            "resting",
            "gefolgschaft",
            "hintergrund",
            "überblick",
        ]
        .iter()
        .copied()
        .collect();

        // Non-person type indicators wrongly classified as persons
        let not_person_patterns: &[&str] =
            &["age", "bsd", "hat", "kong", "ship", "hms", "uss", "iss"];

        let mut purged = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let lower = e.name.to_lowercase();
            let words: Vec<&str> = lower.split_whitespace().collect();

            let should_purge = if words.len() >= 2 {
                // Check for fragment signal words
                words.iter().any(|w| fragment_signals.contains(w))
            } else {
                false
            };

            // Purge entities with names containing non-ASCII quote artifacts
            let has_artifact = e.name.contains("Has-") && e.name.contains("Ayş");

            if should_purge || has_artifact {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    Ok(())
                })?;
                purged += 1;
            }

            // Fix type misclassification: "Bronze Age", "Red Hat", etc. aren't persons
            if e.entity_type == "person" && words.len() == 2 {
                let last = words[1];
                if not_person_patterns.contains(&last) {
                    let new_type = if last == "age" {
                        "concept"
                    } else if last == "hat" || last == "bsd" {
                        "technology"
                    } else {
                        "concept"
                    };
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "UPDATE entities SET entity_type = ?1 WHERE id = ?2",
                            params![new_type, e.id],
                        )?;
                        Ok(())
                    })?;
                }
            }
        }
        Ok(purged)
    }

    /// Purge island entities whose names contain mixed-case fragments indicating
    /// NLP extraction errors (e.g. "Derivatives Several", "CEO Calista Redmond",
    /// "Heimgartner Susanna Zürich"). These are sentence fragments, not real entities.
    /// Also fixes "State X" person misclassifications for connected entities.
    /// Returns count of entities removed.
    pub fn purge_fragment_island_entities(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Words that signal extraction fragments when appearing in entity names
        let fragment_words: HashSet<&str> = [
            "several",
            "various",
            "multiple",
            "numerous",
            "certain",
            "particular",
            "respective",
            "notable",
            "significant",
            "prominent",
            "renowned",
            "alleged",
            "supposed",
            "purported",
            "attempted",
            "failed",
            "increasingly",
            "particularly",
            "especially",
            "approximately",
            "subsequently",
            "simultaneously",
            "predominantly",
            "primarily",
        ]
        .iter()
        .copied()
        .collect();

        // Title prefixes that when combined with names create fragments
        let title_prefixes: &[&str] = &[
            "ceo",
            "cfo",
            "cto",
            "coo",
            "vp",
            "svp",
            "evp",
            "director",
            "chairman",
            "president",
            "secretary",
            "minister",
            "ambassador",
            "governor",
            "senator",
            "professor",
            "dr",
            "dean",
            "provost",
        ];

        let mut purged = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let lower = e.name.to_lowercase();
            let words: Vec<&str> = lower.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }

            let should_purge =
                // Contains fragment signal words
                words.iter().any(|w| fragment_words.contains(w))
                // Starts with a title prefix (e.g. "CEO Calista Redmond")
                || title_prefixes.contains(&words[0])
                // "State X" patterns that are secretary-of-state fragments
                || (words[0] == "state" && words.len() >= 2 && e.entity_type == "person")
                // Names with 4+ words where first word is lowercase-looking role
                || (words.len() >= 4 && words[0].len() <= 6 && e.entity_type == "person"
                    && !words[0].chars().next().unwrap_or('a').is_uppercase()
                    && e.name.chars().next().unwrap_or('a').is_lowercase());

            if should_purge {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    Ok(())
                })?;
                purged += 1;
            }
        }
        Ok(purged)
    }

    /// Purge single-word concept/unknown islands that are clearly noise (adjectives, common nouns,
    /// non-English words without context). These are NLP extraction artifacts that add no knowledge.
    /// Returns count of entities purged.
    pub fn purge_single_word_concept_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        let mut purged = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let name = e.name.trim();
            let words: Vec<&str> = name.split_whitespace().collect();
            let lower = name.to_lowercase();

            let should_purge = match words.len() {
                1 => {
                    // Single-word concepts/unknowns that are just adjectives or common nouns
                    let is_concept_or_unknown =
                        e.entity_type == "concept" || e.entity_type == "unknown";
                    let too_generic = lower.len() <= 12
                        && (lower.ends_with("ic")
                            || lower.ends_with("al")
                            || lower.ends_with("ous")
                            || lower.ends_with("ive")
                            || lower.ends_with("ment")
                            || lower.ends_with("tion")
                            || lower.ends_with("ness")
                            || lower.ends_with("ung") // German suffix
                            || lower.ends_with("keit") // German suffix
                            || lower.ends_with("schaft") // German suffix
                            || lower.ends_with("ing")
                            || lower.ends_with("ity")
                            || lower.ends_with("ism")
                            || lower.ends_with("ure")
                            || lower.ends_with("ary")
                            || lower.ends_with("ery")
                            || lower.ends_with("ory"));
                    // Also purge plural common nouns as concepts
                    let is_plural_common = is_concept_or_unknown
                        && lower.ends_with('s')
                        && !lower.ends_with("ss")
                        && lower.len() <= 10
                        && lower.chars().next().is_some_and(|c| c.is_lowercase());
                    // Common English words that are clearly not standalone entities
                    let common_non_entities = [
                        "inside",
                        "outside",
                        "suppose",
                        "choose",
                        "twelve",
                        "tribute",
                        "spirit",
                        "prior",
                        "interest",
                        "denote",
                        "wisdom",
                        "resting",
                        "highest-ever",
                        "prevent",
                        "protect",
                        "produce",
                        "promote",
                        "propose",
                        "promise",
                        "provide",
                        "pursue",
                        "receive",
                        "reduce",
                        "remain",
                        "remove",
                        "repeat",
                        "replace",
                        "require",
                        "resolve",
                        "respond",
                        "restore",
                        "reveal",
                        "select",
                        "separate",
                        "suggest",
                        "support",
                        "survive",
                        "threat",
                        "toward",
                        "unlike",
                        "within",
                        "without",
                        "among",
                        "beneath",
                        "beside",
                        "beyond",
                        "despite",
                        "except",
                        "thought",
                        "enough",
                        "perhaps",
                        "rather",
                        "though",
                        "unless",
                        "whether",
                        "almost",
                        "simply",
                        "indeed",
                        "merely",
                        "hardly",
                        "likely",
                        "mainly",
                        "mostly",
                        "nearly",
                        "partly",
                        "quite",
                        "truly",
                        "fully",
                        "deeply",
                        "highly",
                        "widely",
                        "closely",
                        "directly",
                        "exactly",
                        "rapidly",
                        "slowly",
                        "roughly",
                        "slightly",
                        "somewhat",
                        "largely",
                        "entirely",
                        "possibly",
                        "hole",
                        "sign",
                        "myth",
                        "tell",
                        "swin",
                        "administration",
                        "allies",
                        "animal",
                        "attached",
                        "browser",
                        "bulbs",
                        "career",
                        "century",
                        "channel",
                        "chaos",
                        "choice",
                        "cities",
                        "compare",
                        "comprehensive",
                        "computational",
                        "computer",
                        "concise",
                        "connected",
                        "construction",
                        "copper",
                        "dates",
                        "discovery",
                        "disorder",
                        "doctoral",
                        "elementary",
                        "learn",
                        "please",
                        "similarly",
                        "studies",
                        "wayback",
                        "furthermore",
                        "nevertheless",
                        "moreover",
                        "otherwise",
                        "essentially",
                        "originally",
                        "unfortunately",
                        "hence",
                        "likewise",
                        "overview",
                        "references",
                        "contents",
                        "introduction",
                        "summary",
                        "volume",
                        "professor",
                        "institute",
                        "university",
                        "museum",
                        "temple",
                        "examples",
                        "images",
                        "results",
                        "resources",
                        "progress",
                        "research",
                        "review",
                        "guide",
                        "standard",
                        "structure",
                        "pattern",
                        "procedure",
                        "practice",
                        "growth",
                        "spread",
                        "controversy",
                        "contribution",
                        "cooperation",
                        "exploration",
                        "expressed",
                        "estimate",
                        "hypothesis",
                        "literature",
                        "philosophy",
                        "religion",
                        "grammar",
                        "logic",
                        "baronet",
                        "basins",
                        "beavers",
                        "chronology",
                        "combinatorial",
                        "centers",
                    ];
                    let is_common_non_entity =
                        is_concept_or_unknown && common_non_entities.contains(&lower.as_str());
                    // Only purge generic-looking single words for concepts
                    (is_concept_or_unknown && too_generic && lower.len() < 15)
                        || is_plural_common
                        || is_common_non_entity
                }
                2 | 3 => {
                    // Multi-word fragments: "Seventh Through", "Open Zihintpause Pause"
                    let has_ordinal = words[0].to_lowercase().ends_with("th")
                        && [
                            "four", "fif", "six", "seven", "eigh", "nin", "ten", "eleven", "twelf",
                        ]
                        .iter()
                        .any(|p| words[0].to_lowercase().starts_with(p));
                    let has_prep_end = ["through", "about", "during", "within", "between"]
                        .contains(&words.last().unwrap_or(&"").to_lowercase().as_str());
                    // "Scholarship Est", "Golden Peaches" for non-place types
                    let ends_with_noise = ["est", "pause", "peaches"]
                        .contains(&words.last().unwrap_or(&"").to_lowercase().as_str());
                    (has_ordinal && has_prep_end)
                        || ends_with_noise
                        || (e.entity_type == "person"
                            && words.len() == 2
                            && ["arts", "open", "scholarship"]
                                .contains(&words[0].to_lowercase().as_str()))
                }
                _ => false,
            };

            if should_purge {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    Ok(())
                })?;
                purged += 1;
            }
        }
        Ok(purged)
    }

    /// Cluster island entities by type and name similarity to suggest batch crawl targets.
    pub fn cluster_islands_for_crawl(&self) -> Result<Vec<(String, Vec<String>)>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }
        // Group islands by type
        let mut type_islands: HashMap<String, Vec<String>> = HashMap::new();
        for e in &entities {
            if !connected.contains(&e.id)
                && !is_noise_type(&e.entity_type)
                && !is_noise_name(&e.name)
            {
                type_islands
                    .entry(e.entity_type.clone())
                    .or_default()
                    .push(e.name.clone());
            }
        }
        let mut clusters: Vec<(String, Vec<String>)> = type_islands
            .into_iter()
            .filter(|(_, names)| names.len() >= 2)
            .map(|(t, mut names)| {
                names.sort();
                names.truncate(20);
                (t, names)
            })
            .collect();
        clusters.sort_by(|a, b| b.1.len().cmp(&a.1.len()));
        Ok(clusters)
    }

    /// Find islands that could plausibly connect to existing hub entities.
    /// Uses name substring matching and type affinity to suggest links.
    pub fn find_connectable_islands(&self) -> Result<Vec<(String, String, String, f64)>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        // Build connected set and degree map
        let mut connected: HashSet<i64> = HashSet::new();
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Collect hubs (connected entities with degree >= 2)
        let hubs: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| {
                connected.contains(&e.id)
                    && degree.get(&e.id).copied().unwrap_or(0) >= 2
                    && !is_noise_type(&e.entity_type)
                    && !is_noise_name(&e.name)
            })
            .collect();

        // Collect high-value islands
        let islands: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| {
                !connected.contains(&e.id)
                    && HIGH_VALUE_TYPES.contains(&e.entity_type.as_str())
                    && !is_noise_name(&e.name)
                    && e.name.len() >= 3
            })
            .collect();

        let mut suggestions = Vec::new();

        // For each island, check if any hub name contains it or vice versa
        for island in islands.iter().take(500) {
            let island_lower = island.name.to_lowercase();
            let island_words: HashSet<&str> = island_lower.split_whitespace().collect();
            if island_words.is_empty() {
                continue;
            }

            for hub in &hubs {
                let hub_lower = hub.name.to_lowercase();
                let hub_words: HashSet<&str> = hub_lower.split_whitespace().collect();

                // Check word overlap (at least one significant shared word)
                let shared: Vec<&&str> = island_words
                    .intersection(&hub_words)
                    .filter(|w| w.len() >= 4)
                    .collect();

                if !shared.is_empty() {
                    let overlap_ratio =
                        shared.len() as f64 / island_words.len().max(hub_words.len()) as f64;
                    let confidence = 0.3 + overlap_ratio * 0.4;
                    let reason = format!(
                        "Island '{}' ({}) shares words [{}] with hub '{}' ({})",
                        island.name,
                        island.entity_type,
                        shared.iter().map(|s| **s).collect::<Vec<_>>().join(", "),
                        hub.name,
                        hub.entity_type
                    );
                    suggestions.push((island.name.clone(), hub.name.clone(), reason, confidence));
                }
            }
        }

        // Sort by confidence descending
        suggestions.sort_by(|a, b| b.3.partial_cmp(&a.3).unwrap_or(std::cmp::Ordering::Equal));
        suggestions.truncate(50);
        Ok(suggestions)
    }

    /// Compute a priority score for each knowledge gap to guide crawling.
    /// Higher score = more important to fill.
    pub fn prioritize_gaps(&self) -> Result<Vec<(String, String, f64)>> {
        let mut gap_scores: Vec<(String, String, f64)> = Vec::new();

        // 1. Islands of high-value types get base priority
        let islands = self.find_island_entities()?;
        let mut type_counts: HashMap<String, usize> = HashMap::new();
        for (_, etype) in &islands {
            *type_counts.entry(etype.clone()).or_insert(0) += 1;
        }

        // Types with many islands are systematic gaps
        for (etype, count) in &type_counts {
            if *count >= 5 && HIGH_VALUE_TYPES.contains(&etype.as_str()) {
                let score = (*count as f64).ln() * 2.0;
                gap_scores.push((
                    etype.clone(),
                    format!(
                        "{} disconnected '{}' entities — systematic gap",
                        count, etype
                    ),
                    score,
                ));
            }
        }

        // 2. Hypotheses with object "?" (unknown targets)
        let hyps = self.list_hypotheses(Some(HypothesisStatus::Proposed))?;
        let unknown_count = hyps.iter().filter(|h| h.object == "?").count();
        if unknown_count > 0 {
            gap_scores.push((
                "unknown_relations".to_string(),
                format!("{} hypotheses have unknown objects", unknown_count),
                (unknown_count as f64).ln() * 1.5,
            ));
        }

        // 3. Cross-domain gaps (disconnected clusters)
        let cross_gaps = self.find_cross_domain_gaps()?;
        if !cross_gaps.is_empty() {
            gap_scores.push((
                "cross_domain".to_string(),
                format!(
                    "{} disconnected cluster pairs sharing entity types",
                    cross_gaps.len()
                ),
                (cross_gaps.len() as f64).ln() * 3.0,
            ));
        }

        gap_scores.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap_or(std::cmp::Ordering::Equal));
        Ok(gap_scores)
    }

    /// Auto-confirm or reject hypotheses based on graph evidence.
    /// More aggressive than validate_hypothesis — checks transitive paths.
    pub fn auto_resolve_hypotheses(&self) -> Result<(usize, usize)> {
        let mut hyps = self.list_hypotheses(Some(HypothesisStatus::Testing))?;
        let mut confirmed = 0usize;
        let mut rejected = 0usize;

        // Pre-load pattern weights for strategy-aware resolution
        let weights = self.get_pattern_weights().unwrap_or_default();
        let weight_map: HashMap<String, f64> = weights
            .iter()
            .map(|w| (w.pattern_type.clone(), w.weight))
            .collect();

        // Pre-compute suspended strategies (hypothesis-level confirmation rate < 20%)
        let suspended = self.suspended_strategies().unwrap_or_default();

        for h in hyps.iter_mut() {
            // Batch-reject all testing hypotheses from suspended strategies
            // (confirmation rate < 20% with 50+ samples — these are proven losers)
            if suspended.contains(&h.pattern_source) && h.confidence < 0.75 {
                self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                self.record_outcome(&h.pattern_source, false)?;
                rejected += 1;
                continue;
            }

            // Strategy-aware rejection: auto-reject hypotheses from strategies with
            // historically very low confirmation rates (weight < 0.10)
            let source_weight = weight_map.get(&h.pattern_source).copied().unwrap_or(1.0);
            if source_weight < 0.10 && h.confidence < 0.6 {
                self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                self.record_outcome(&h.pattern_source, false)?;
                rejected += 1;
                continue;
            }

            // Fast-track resolution for katz_similarity: these hypotheses are
            // path-based and can be validated immediately — if no path ≤3 hops
            // exists between subject and object, reject early.
            if h.pattern_source == "katz_similarity" && h.object != "?" {
                let path = crate::graph::shortest_path(self.brain, &h.subject, &h.object)?;
                match path {
                    Some(p) if p.len() <= 3 => {
                        // Direct or 2-hop path — confirm if confidence is decent
                        if h.confidence >= 0.55 {
                            self.update_hypothesis_status(h.id, HypothesisStatus::Confirmed)?;
                            self.record_outcome(&h.pattern_source, true)?;
                            confirmed += 1;
                            continue;
                        }
                    }
                    _ => {
                        // No short path — reject if stale (>12h)
                        if let Ok(discovered) = chrono::NaiveDateTime::parse_from_str(
                            &h.discovered_at,
                            "%Y-%m-%d %H:%M:%S",
                        ) {
                            let now = chrono::Utc::now().naive_utc();
                            if (now - discovered).num_hours() >= 12 {
                                self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                                self.record_outcome(&h.pattern_source, false)?;
                                rejected += 1;
                                continue;
                            }
                        }
                    }
                }
            }

            // Aggressive open-ended rejection: hypotheses with object="?" from
            // strategies with < 30% confirmation rate are speculative noise.
            // Reject them after 24h if confidence is below 0.70.
            if h.object == "?" && source_weight < 0.50 {
                if let Ok(discovered) =
                    chrono::NaiveDateTime::parse_from_str(&h.discovered_at, "%Y-%m-%d %H:%M:%S")
                {
                    let now = chrono::Utc::now().naive_utc();
                    let age_hours = (now - discovered).num_hours();
                    if age_hours >= 24 && h.confidence < 0.70 {
                        self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                        self.record_outcome(&h.pattern_source, false)?;
                        rejected += 1;
                        continue;
                    }
                }
            }

            // Age-based stale rejection: hypotheses stuck in testing for >48 hours
            // without accumulating evidence are likely noise. Reject low-confidence
            // stale hypotheses; promote high-confidence ones that haven't been
            // contradicted.
            if let Ok(discovered) =
                chrono::NaiveDateTime::parse_from_str(&h.discovered_at, "%Y-%m-%d %H:%M:%S")
            {
                let now = chrono::Utc::now().naive_utc();
                let age_hours = (now - discovered).num_hours();
                if age_hours >= 48 {
                    if h.confidence < 0.55 {
                        // Low-confidence and stale — reject
                        self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                        self.record_outcome(&h.pattern_source, false)?;
                        rejected += 1;
                        continue;
                    } else if age_hours >= 96 && h.confidence < 0.65 {
                        // Even moderate confidence — reject after 4 days
                        self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                        self.record_outcome(&h.pattern_source, false)?;
                        rejected += 1;
                        continue;
                    }
                }
            }

            // Check if a path exists between subject and object (evidence of relation)
            if h.object != "?" {
                // Reject single-word fragment pairs (NLP noise like "Grace" → "Hopper")
                if !h.subject.contains(' ')
                    && h.subject.len() <= 12
                    && !h.object.contains(' ')
                    && h.object.len() <= 12
                {
                    self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                    self.record_outcome(&h.pattern_source, false)?;
                    rejected += 1;
                    continue;
                }

                // Skip substring pairs — these are merge candidates, not discoveries
                let sl = h.subject.to_lowercase();
                let ol = h.object.to_lowercase();
                if sl.contains(&ol) || ol.contains(&sl) {
                    self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                    self.record_outcome(&h.pattern_source, false)?;
                    rejected += 1;
                    continue;
                }

                // Reject hypotheses where either entity looks like a topic-prefix
                // concatenation artifact (e.g., "Punic Carthage", "Yuan China")
                let topic_prefixes_check: &[&str] = &[
                    "punic",
                    "yuan",
                    "ming",
                    "tang",
                    "han",
                    "qing",
                    "song",
                    "sui",
                    "byzantine",
                    "ottoman",
                    "illyria",
                    "lancel",
                    "efficiency",
                    "devastated",
                    "celebrated",
                    "legendary",
                    "renowned",
                    "crusader",
                    "viking",
                    "celtic",
                    "gallic",
                    "frankish",
                ];
                let subj_first = h
                    .subject
                    .split_whitespace()
                    .next()
                    .unwrap_or("")
                    .to_lowercase();
                let obj_first = h
                    .object
                    .split_whitespace()
                    .next()
                    .unwrap_or("")
                    .to_lowercase();
                if topic_prefixes_check.contains(&subj_first.as_str())
                    || (h.object != "?" && topic_prefixes_check.contains(&obj_first.as_str()))
                {
                    self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                    self.record_outcome(&h.pattern_source, false)?;
                    rejected += 1;
                    continue;
                }

                // Semantic coherence: reject predicates that require matching
                // entity types when the types actually differ
                let subj_ent = self.brain.get_entity_by_name(&h.subject)?;
                let obj_ent = self.brain.get_entity_by_name(&h.object)?;
                if let (Some(ref se), Some(ref oe)) = (&subj_ent, &obj_ent) {
                    if !predicate_type_compatible(&h.predicate, &se.entity_type, &oe.entity_type) {
                        self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                        self.record_outcome(&h.pattern_source, false)?;
                        rejected += 1;
                        continue;
                    }
                }

                let path = crate::graph::shortest_path(self.brain, &h.subject, &h.object)?;
                if let Some(p) = &path {
                    // Only confirm via path if it's a direct connection (len=2) or
                    // a 2-hop path (len=3) with high base confidence
                    let path_boost = match p.len() {
                        2 => 0.25,                        // direct connection is strong evidence
                        3 if h.confidence >= 0.6 => 0.10, // 2-hop with high existing confidence
                        _ => 0.0,                         // longer paths are too weak
                    };
                    if path_boost > 0.0 {
                        let new_conf = (h.confidence + path_boost).min(1.0);
                        if new_conf >= CONFIRMATION_THRESHOLD {
                            self.update_hypothesis_status(h.id, HypothesisStatus::Confirmed)?;
                            self.record_outcome(&h.pattern_source, true)?;
                            // Record as discovery
                            let evidence = vec![
                                format!("Path of length {} found between entities", p.len()),
                                format!("Pattern source: {}", h.pattern_source),
                            ];
                            let _ = self.save_discovery(h.id, &evidence);
                            confirmed += 1;
                            continue;
                        }
                    }
                }

                // Check if entities even exist with relations
                let subj = self.brain.get_entity_by_name(&h.subject)?;
                let obj = self.brain.get_entity_by_name(&h.object)?;
                match (&subj, &obj) {
                    (None, _) | (_, None) => {
                        // Entity deleted or never existed — reject
                        self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                        self.record_outcome(&h.pattern_source, false)?;
                        rejected += 1;
                    }
                    (Some(s_ent), Some(o_ent)) => {
                        // Multi-signal evidence accumulation before final decision
                        let mut evidence_score: f64 = h.confidence;

                        // Signal 1: Shared source URLs
                        let s_sources: HashSet<String> = self
                            .brain
                            .get_source_urls_for(s_ent.id)?
                            .into_iter()
                            .collect();
                        let o_sources: HashSet<String> = self
                            .brain
                            .get_source_urls_for(o_ent.id)?
                            .into_iter()
                            .collect();
                        let shared_sources = s_sources.intersection(&o_sources).count();
                        if shared_sources >= 2 {
                            evidence_score += 0.05 * shared_sources as f64;
                        }

                        // Signal 2: Shared fact keys with matching values
                        let s_facts = self.brain.get_facts_for(s_ent.id)?;
                        let o_facts = self.brain.get_facts_for(o_ent.id)?;
                        let s_kv: HashSet<(String, String)> = s_facts
                            .iter()
                            .map(|f| (f.key.clone(), f.value.clone()))
                            .collect();
                        let o_kv: HashSet<(String, String)> = o_facts
                            .iter()
                            .map(|f| (f.key.clone(), f.value.clone()))
                            .collect();
                        let shared_facts = s_kv.intersection(&o_kv).count();
                        if shared_facts >= 1 {
                            evidence_score += 0.10 * shared_facts as f64;
                        }

                        // Signal 3: Same entity type + high degree (well-established entities)
                        let s_rels = self.brain.get_relations_for(s_ent.id)?;
                        let o_rels = self.brain.get_relations_for(o_ent.id)?;
                        if s_ent.entity_type == o_ent.entity_type {
                            if s_rels.len() >= 3 && o_rels.len() >= 3 {
                                evidence_score += 0.05;
                            }
                        }

                        // Signal 4: Mutual neighbors (shared connections)
                        let s_neighbors: HashSet<i64> = s_rels
                            .iter()
                            .flat_map(|r| {
                                let s_id = self
                                    .brain
                                    .get_entity_by_name(&r.0)
                                    .ok()
                                    .flatten()
                                    .map(|e| e.id)
                                    .unwrap_or(-1);
                                let o_id = self
                                    .brain
                                    .get_entity_by_name(&r.2)
                                    .ok()
                                    .flatten()
                                    .map(|e| e.id)
                                    .unwrap_or(-1);
                                vec![s_id, o_id]
                            })
                            .filter(|&id| id != s_ent.id && id >= 0)
                            .collect();
                        let o_neighbors: HashSet<i64> = o_rels
                            .iter()
                            .flat_map(|r| {
                                let s_id = self
                                    .brain
                                    .get_entity_by_name(&r.0)
                                    .ok()
                                    .flatten()
                                    .map(|e| e.id)
                                    .unwrap_or(-1);
                                let o_id = self
                                    .brain
                                    .get_entity_by_name(&r.2)
                                    .ok()
                                    .flatten()
                                    .map(|e| e.id)
                                    .unwrap_or(-1);
                                vec![s_id, o_id]
                            })
                            .filter(|&id| id != o_ent.id && id >= 0)
                            .collect();
                        let mutual_neighbors = s_neighbors.intersection(&o_neighbors).count();
                        if mutual_neighbors >= 2 {
                            evidence_score += 0.05 * (mutual_neighbors as f64).min(4.0);
                        }

                        // Signal 5: Temporal co-discovery special case — if both
                        // entities are still isolated (degree 0-1), resolve based on
                        // shared sources or age.  These hypotheses can't get path
                        // confirmation because both endpoints lack connections.
                        if h.pattern_source == "temporal_co_discovery"
                            || h.pattern_source == "source_co_occurrence"
                        {
                            if shared_sources >= 1 {
                                // Co-crawled entities sharing a source — strong signal
                                evidence_score += 0.15;
                            } else if s_rels.is_empty() && o_rels.is_empty() {
                                // Both still isolated with no shared source after being
                                // hypothesis'd — weak signal, but not contradicted.
                                // Confirm at reduced threshold if entity types are compatible.
                                if types_compatible(&s_ent.entity_type, &o_ent.entity_type)
                                    && h.confidence >= 0.4
                                {
                                    evidence_score += 0.10;
                                }
                            }
                        }

                        // Signal 6: Name-token overlap — entities sharing significant
                        // name tokens (beyond common words) suggest topical relatedness.
                        // Particularly useful for temporal_co_discovery and concept_enrichment.
                        {
                            let stop_words: HashSet<&str> = [
                                "the", "of", "and", "in", "for", "to", "a", "an", "is", "at", "on",
                                "by", "with", "from", "or", "as", "it",
                            ]
                            .into_iter()
                            .collect();
                            let s_tokens: HashSet<String> = h
                                .subject
                                .to_lowercase()
                                .split_whitespace()
                                .filter(|w| w.len() > 2 && !stop_words.contains(w))
                                .map(|s| s.to_string())
                                .collect();
                            let o_tokens: HashSet<String> = h
                                .object
                                .to_lowercase()
                                .split_whitespace()
                                .filter(|w| w.len() > 2 && !stop_words.contains(w))
                                .map(|s| s.to_string())
                                .collect();
                            let shared_tokens = s_tokens.intersection(&o_tokens).count();
                            if shared_tokens >= 1 && !s_tokens.is_empty() && !o_tokens.is_empty() {
                                let overlap_ratio = shared_tokens as f64
                                    / s_tokens.len().min(o_tokens.len()) as f64;
                                evidence_score += 0.08 * overlap_ratio;
                            }
                        }

                        // Signal 7: Entity type affinity — concept_enrichment and
                        // predicate_transfer hypotheses linking a concept to a
                        // person/org/technology are high-value knowledge connections.
                        if h.pattern_source == "concept_enrichment"
                            || h.pattern_source == "predicate_transfer"
                        {
                            let types = [&s_ent.entity_type, &o_ent.entity_type];
                            let has_concept = types.iter().any(|t| t.as_str() == "concept");
                            let has_specific = types.iter().any(|t| {
                                matches!(
                                    t.as_str(),
                                    "person"
                                        | "organization"
                                        | "technology"
                                        | "company"
                                        | "product"
                                )
                            });
                            if has_concept && has_specific {
                                evidence_score += 0.08;
                            }
                        }

                        // Signal 8: Entity importance — highly accessed entities
                        // are more likely to be well-established knowledge nodes.
                        // Boost hypotheses where both entities have been accessed
                        // multiple times (indicates user/crawl interest).
                        {
                            let combined_access =
                                s_ent.access_count as f64 + o_ent.access_count as f64;
                            if combined_access >= 10.0 {
                                evidence_score += 0.06;
                            } else if combined_access >= 4.0 {
                                evidence_score += 0.03;
                            }
                            // Fact richness: entities with more facts are better established
                            let total_facts = s_facts.len() + o_facts.len();
                            if total_facts >= 6 {
                                evidence_score += 0.06;
                            } else if total_facts >= 3 {
                                evidence_score += 0.03;
                            }
                        }

                        // Signal 9: Strategy ROI boost — hypotheses from high-ROI
                        // strategies deserve more trust when other signals are marginal.
                        {
                            let strat_weight =
                                weight_map.get(&h.pattern_source).copied().unwrap_or(0.5);
                            if strat_weight >= 0.95 && evidence_score >= 0.6 {
                                evidence_score += 0.05;
                            }
                        }

                        // Confirm if accumulated evidence is strong enough
                        if evidence_score >= CONFIRMATION_THRESHOLD {
                            self.update_hypothesis_status(h.id, HypothesisStatus::Confirmed)?;
                            self.record_outcome(&h.pattern_source, true)?;
                            let evidence = vec![
                                format!(
                                    "Multi-signal confirmation: score {:.2} (sources={}, facts={})",
                                    evidence_score, shared_sources, shared_facts
                                ),
                                format!("Pattern source: {}", h.pattern_source),
                            ];
                            let _ = self.save_discovery(h.id, &evidence);
                            confirmed += 1;
                        } else if self.check_contradiction(h)? {
                            self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                            self.record_outcome(&h.pattern_source, false)?;
                            rejected += 1;
                        }
                    }
                }
            } else {
                // object == "?" — predicate-existence hypotheses (e.g. predicate_transfer)
                // Confirm if the entity has since gained the predicted predicate;
                // reject if the entity no longer exists or has been idle too long.
                let subj = self.brain.get_entity_by_name(&h.subject)?;
                match subj {
                    None => {
                        // Entity deleted — reject
                        self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                        self.record_outcome(&h.pattern_source, false)?;
                        rejected += 1;
                    }
                    Some(s_ent) => {
                        let rels = self.brain.get_relations_for(s_ent.id)?;
                        // Tuple: (subject_name, predicate, object_name, confidence)
                        let has_exact = rels.iter().any(|r| r.0 == h.subject && r.1 == h.predicate);
                        // Also check for similar predicates (fuzzy match)
                        let has_similar = rels
                            .iter()
                            .any(|r| r.0 == h.subject && predicates_similar(&r.1, &h.predicate));
                        if has_exact {
                            // Entity acquired the predicted predicate — confirmed!
                            self.update_hypothesis_status(h.id, HypothesisStatus::Confirmed)?;
                            self.record_outcome(&h.pattern_source, true)?;
                            let evidence = vec![
                                format!("{} now has predicate '{}'", h.subject, h.predicate),
                                format!("Pattern source: {}", h.pattern_source),
                            ];
                            let _ = self.save_discovery(h.id, &evidence);
                            confirmed += 1;
                        } else if has_similar && h.confidence >= 0.5 {
                            // Entity has a semantically similar predicate — close enough
                            self.update_hypothesis_status(h.id, HypothesisStatus::Confirmed)?;
                            self.record_outcome(&h.pattern_source, true)?;
                            let matching_pred = rels
                                .iter()
                                .find(|r| {
                                    r.0 == h.subject && predicates_similar(&r.1, &h.predicate)
                                })
                                .map(|r| r.1.clone())
                                .unwrap_or_default();
                            let evidence = vec![
                                format!(
                                    "{} has similar predicate '{}' (predicted '{}')",
                                    h.subject, matching_pred, h.predicate
                                ),
                                format!("Pattern source: {}", h.pattern_source),
                            ];
                            let _ = self.save_discovery(h.id, &evidence);
                            confirmed += 1;
                        } else if rels.is_empty() {
                            // Entity is completely isolated — reject low-confidence ones
                            if h.confidence < 0.5 {
                                self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                                self.record_outcome(&h.pattern_source, false)?;
                                rejected += 1;
                            }
                        } else if rels.len() >= 3 && !has_similar {
                            // Entity is well-connected but hasn't acquired
                            // any similar predicate — reject if confidence is marginal
                            if h.confidence < 0.6 {
                                self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                                self.record_outcome(&h.pattern_source, false)?;
                                rejected += 1;
                            }
                        }
                    }
                }
            }
        }
        Ok((confirmed, rejected))
    }

    /// Batch-boost hypotheses using graph structure (communities + k-cores).
    /// Computes expensive graph metrics once, then applies to all hypotheses.
    pub fn boost_hypotheses_with_graph_structure(
        &self,
        hypotheses: &mut [Hypothesis],
    ) -> Result<()> {
        // Compute once
        let communities = crate::graph::louvain_communities(self.brain)?;
        let cores = crate::graph::k_core_decomposition(self.brain)?;

        // Build name→id cache
        let entities = self.brain.all_entities()?;
        let name_to_id: HashMap<String, i64> = entities
            .iter()
            .map(|e| (e.name.to_lowercase(), e.id))
            .collect();

        for h in hypotheses.iter_mut() {
            if h.object == "?" {
                continue;
            }
            let s_id = name_to_id.get(&h.subject.to_lowercase()).copied();
            let o_id = name_to_id.get(&h.object.to_lowercase()).copied();

            if let (Some(sid), Some(oid)) = (s_id, o_id) {
                // Community co-membership boost
                if let (Some(&cs), Some(&co)) = (communities.get(&sid), communities.get(&oid)) {
                    if cs == co {
                        h.confidence = (h.confidence + 0.15).min(1.0);
                        h.evidence_for.push(
                            "Subject and object belong to the same graph community".to_string(),
                        );
                    }
                }
                // K-core depth boost
                let s_core = cores.get(&sid).copied().unwrap_or(0);
                let o_core = cores.get(&oid).copied().unwrap_or(0);
                let min_core = s_core.min(o_core);
                if min_core >= 2 {
                    h.confidence = (h.confidence + 0.05 * min_core as f64).min(1.0);
                    h.evidence_for.push(format!(
                        "Both entities in {}-core (deeply embedded)",
                        min_core
                    ));
                }

                // Cross-community bridge boost: hypotheses connecting DIFFERENT communities
                // are more valuable for knowledge integration than same-community ones.
                if let (Some(&cs), Some(&co)) = (communities.get(&sid), communities.get(&oid)) {
                    if cs != co {
                        // Compute connectivity value — how much would this edge reduce fragmentation?
                        if let Ok(cv) = crate::graph::edge_connectivity_value(self.brain, sid, oid)
                        {
                            if cv > 0.0 {
                                let bridge_boost = (cv * 50.0).min(0.25); // scale: max +0.25
                                h.confidence = (h.confidence + bridge_boost).min(1.0);
                                h.evidence_for.push(format!(
                                    "Bridges different communities (connectivity value: {:.4})",
                                    cv
                                ));
                            }
                        }
                    }
                }

                // PageRank importance boost: hypotheses about important entities are more valuable
                // (computed lazily — only if entities have high degree, which correlates with PR)
                let s_rels = self
                    .brain
                    .get_relations_for(sid)
                    .ok()
                    .map(|r| r.len())
                    .unwrap_or(0);
                let o_rels = self
                    .brain
                    .get_relations_for(oid)
                    .ok()
                    .map(|r| r.len())
                    .unwrap_or(0);
                let min_rels = s_rels.min(o_rels);
                if min_rels >= 5 {
                    // Both entities are well-connected — hypothesis is about important entities
                    let importance_boost = (min_rels as f64 * 0.01).min(0.15);
                    h.confidence = (h.confidence + importance_boost).min(1.0);
                    h.evidence_for.push(format!(
                        "Both entities well-connected ({}/{} relations)",
                        s_rels, o_rels
                    ));
                }
            }
        }
        Ok(())
    }

    /// Resolve mutual exclusions among testing hypotheses.
    /// If two hypotheses predict contradictory predicates for the same entity pair
    /// (e.g., A "allied_with" B vs A "opposed_to" B), reject the weaker one.
    pub fn resolve_mutual_exclusions(&self) -> Result<(usize, usize)> {
        let testing = self.list_hypotheses(Some(HypothesisStatus::Testing))?;
        if testing.len() < 2 {
            return Ok((0, 0));
        }

        // Group hypotheses by normalized entity pair (alphabetically sorted)
        let mut pair_groups: HashMap<(String, String), Vec<&Hypothesis>> = HashMap::new();
        for h in &testing {
            if h.object == "?" {
                continue;
            }
            let key = if h.subject <= h.object {
                (h.subject.clone(), h.object.clone())
            } else {
                (h.object.clone(), h.subject.clone())
            };
            pair_groups.entry(key).or_default().push(h);
        }

        let mut promoted = 0usize;
        let mut rejected = 0usize;

        for (_pair, group) in &pair_groups {
            if group.len() < 2 {
                continue;
            }

            for (i, hi) in group.iter().enumerate() {
                for hj in group.iter().skip(i + 1) {
                    if is_contradicting_predicate(&hi.predicate, &hj.predicate) {
                        let (winner, loser) = if hi.confidence >= hj.confidence {
                            (hi, hj)
                        } else {
                            (hj, hi)
                        };

                        self.update_hypothesis_status(loser.id, HypothesisStatus::Rejected)?;
                        self.record_outcome(&loser.pattern_source, false)?;
                        rejected += 1;

                        if winner.confidence >= 0.65 {
                            self.update_hypothesis_status(winner.id, HypothesisStatus::Confirmed)?;
                            self.record_outcome(&winner.pattern_source, true)?;
                            let evidence = vec![format!(
                                "Confirmed by mutual exclusion: contradicts weaker hypothesis (pred='{}', conf={:.2})",
                                loser.predicate, loser.confidence
                            )];
                            let _ = self.save_discovery(winner.id, &evidence);
                            promoted += 1;
                        }
                    }
                }
            }
        }

        Ok((promoted, rejected))
    }

    /// Boost hypotheses using label propagation communities as an alternative
    /// signal to Louvain. Cross-method agreement strengthens confidence.
    pub fn boost_with_label_propagation(&self, hypotheses: &mut [Hypothesis]) -> Result<()> {
        let lp_communities = crate::graph::label_propagation(self.brain, 20)?;

        // Invert: entity_id → community label
        let mut entity_to_lp: HashMap<i64, usize> = HashMap::new();
        for (&lbl, members) in &lp_communities {
            for &id in members {
                entity_to_lp.insert(id, lbl);
            }
        }

        let entities = self.brain.all_entities()?;
        let name_to_id: HashMap<String, i64> = entities
            .iter()
            .map(|e| (e.name.to_lowercase(), e.id))
            .collect();

        for h in hypotheses.iter_mut() {
            if h.object == "?" {
                continue;
            }
            let s_id = name_to_id.get(&h.subject.to_lowercase()).copied();
            let o_id = name_to_id.get(&h.object.to_lowercase()).copied();

            if let (Some(sid), Some(oid)) = (s_id, o_id) {
                if let (Some(&lp_s), Some(&lp_o)) = (entity_to_lp.get(&sid), entity_to_lp.get(&oid))
                {
                    if lp_s == lp_o {
                        h.confidence = (h.confidence + 0.08).min(1.0);
                        h.evidence_for.push(
                            "Same label-propagation community (independent of Louvain)".to_string(),
                        );
                    }
                }
            }
        }
        Ok(())
    }

    /// Check if a hypothesis already exists (dedup across runs).
    fn hypothesis_exists(&self, subject: &str, predicate: &str, object: &str) -> Result<bool> {
        self.brain.with_conn(|conn| {
            let count: i64 = conn.query_row(
                "SELECT COUNT(*) FROM hypotheses WHERE subject = ?1 AND predicate = ?2 AND object = ?3",
                params![subject, predicate, object],
                |row| row.get(0),
            )?;
            if count > 0 {
                return Ok(true);
            }
            // Also check symmetric form for symmetric predicates
            if is_symmetric_predicate(predicate) && object != "?" {
                let rev_count: i64 = conn.query_row(
                    "SELECT COUNT(*) FROM hypotheses WHERE subject = ?1 AND predicate = ?2 AND object = ?3",
                    params![object, predicate, subject],
                    |row| row.get(0),
                )?;
                return Ok(rev_count > 0);
            }
            Ok(false)
        })
    }

    /// Deduplicate patterns: merge identical descriptions, increment frequency.
    fn dedup_patterns(&self, patterns: &mut Vec<Pattern>) {
        let mut seen: HashMap<String, usize> = HashMap::new();
        let mut deduped: Vec<Pattern> = Vec::new();
        for p in patterns.drain(..) {
            if let Some(&idx) = seen.get(&p.description) {
                deduped[idx].frequency += p.frequency;
            } else {
                seen.insert(p.description.clone(), deduped.len());
                deduped.push(p);
            }
        }
        *patterns = deduped;
    }

    // -----------------------------------------------------------------------
    // Explain
    // -----------------------------------------------------------------------

    /// Build a full explanation for a hypothesis.
    pub fn explain(&self, hypothesis_id: i64) -> Result<Option<String>> {
        let h = self.get_hypothesis(hypothesis_id)?;
        match h {
            None => Ok(None),
            Some(h) => {
                let mut lines = Vec::new();
                lines.push(format!("# Hypothesis #{}", h.id));
                lines.push(format!(
                    "**Claim:** {} {} {}",
                    h.subject, h.predicate, h.object
                ));
                lines.push(format!(
                    "**Status:** {} | **Confidence:** {:.2}",
                    h.status.as_str(),
                    h.confidence
                ));
                lines.push(format!("**Source pattern:** {}", h.pattern_source));
                lines.push(format!("**Discovered:** {}", h.discovered_at));
                lines.push(String::new());
                lines.push("## Reasoning Chain".to_string());
                for (i, step) in h.reasoning_chain.iter().enumerate() {
                    lines.push(format!("{}. {}", i + 1, step));
                }
                lines.push(String::new());
                if !h.evidence_for.is_empty() {
                    lines.push("## Evidence For".to_string());
                    for e in &h.evidence_for {
                        lines.push(format!("- ✅ {}", e));
                    }
                    lines.push(String::new());
                }
                if !h.evidence_against.is_empty() {
                    lines.push("## Evidence Against".to_string());
                    for e in &h.evidence_against {
                        lines.push(format!("- ❌ {}", e));
                    }
                }
                Ok(Some(lines.join("\n")))
            }
        }
    }

    // -----------------------------------------------------------------------
    // Hypothesis clustering
    // -----------------------------------------------------------------------

    /// Cluster hypotheses by shared entities to surface thematic insights.
    /// Groups hypotheses that mention overlapping subjects/objects into clusters.
    /// Returns Vec of (theme_label, hypotheses_in_cluster).
    pub fn cluster_hypotheses(&self) -> Result<Vec<(String, Vec<Hypothesis>)>> {
        let hyps = self.list_hypotheses(Some(HypothesisStatus::Proposed))?;
        if hyps.is_empty() {
            return Ok(vec![]);
        }

        // Build entity → hypothesis index
        let mut entity_hyps: HashMap<String, Vec<usize>> = HashMap::new();
        for (idx, h) in hyps.iter().enumerate() {
            entity_hyps.entry(h.subject.clone()).or_default().push(idx);
            if h.object != "?" {
                entity_hyps.entry(h.object.clone()).or_default().push(idx);
            }
        }

        // Union-Find to cluster hypotheses sharing entities
        let n = hyps.len();
        let mut parent: Vec<usize> = (0..n).collect();

        fn find(parent: &mut [usize], x: usize) -> usize {
            if parent[x] != x {
                parent[x] = find(parent, parent[x]);
            }
            parent[x]
        }
        fn union(parent: &mut [usize], a: usize, b: usize) {
            let ra = find(parent, a);
            let rb = find(parent, b);
            if ra != rb {
                parent[ra] = rb;
            }
        }

        for indices in entity_hyps.values() {
            if indices.len() >= 2 {
                for w in indices.windows(2) {
                    union(&mut parent, w[0], w[1]);
                }
            }
        }

        // Group by root
        let mut clusters: HashMap<usize, Vec<usize>> = HashMap::new();
        for i in 0..n {
            let root = find(&mut parent, i);
            clusters.entry(root).or_default().push(i);
        }

        // Build result with theme labels (most frequent entity in cluster)
        let mut result: Vec<(String, Vec<Hypothesis>)> = Vec::new();
        for indices in clusters.values() {
            if indices.len() < 2 {
                continue; // skip singletons
            }
            // Find the most mentioned entity as theme label
            let mut entity_freq: HashMap<&str, usize> = HashMap::new();
            for &idx in indices {
                *entity_freq.entry(&hyps[idx].subject).or_insert(0) += 1;
                if hyps[idx].object != "?" {
                    *entity_freq.entry(&hyps[idx].object).or_insert(0) += 1;
                }
            }
            let theme = entity_freq
                .into_iter()
                .max_by_key(|(_, c)| *c)
                .map(|(name, _)| name.to_string())
                .unwrap_or_default();

            let cluster_hyps: Vec<Hypothesis> =
                indices.iter().map(|&idx| hyps[idx].clone()).collect();
            result.push((theme, cluster_hyps));
        }
        result.sort_by(|a, b| b.1.len().cmp(&a.1.len()));
        result.truncate(20);
        Ok(result)
    }

    /// Discovery health check: analyze trend and recommend next actions.
    pub fn discovery_health_check(&self) -> Result<String> {
        let (trend, avg_delta, _, recommendation) =
            crate::graph::analyze_discovery_trend(self.brain, 6)?;

        let velocity = self.get_velocity_trend(5)?;
        let velocity_note = if velocity.len() >= 2 {
            let recent_hyps = velocity[0].2;
            let prev_hyps = velocity[1].2;
            if recent_hyps < prev_hyps / 2 && recent_hyps < 20 {
                " Hypothesis generation dropping — knowledge frontiers may be exhausted."
            } else {
                ""
            }
        } else {
            ""
        };

        let clusters = self.cluster_hypotheses()?;
        let cluster_note = if !clusters.is_empty() {
            format!(
                " {} hypothesis clusters found; largest theme: '{}' ({} hypotheses).",
                clusters.len(),
                clusters[0].0,
                clusters[0].1.len()
            )
        } else {
            String::new()
        };

        // Graph connectivity evolution (meta-learning signal)
        let evolution_note = match crate::graph::connectivity_evolution(self.brain, 10) {
            Ok(evo) => {
                let mut notes = Vec::new();
                if let Some(&(slope, _)) = evo.get("fragmentation") {
                    if slope > 0.001 {
                        notes.push("fragmentation↑ (more island merging needed)");
                    } else if slope < -0.005 {
                        notes.push("fragmentation↓ (consolidation working)");
                    }
                }
                if let Some(&(slope, _)) = evo.get("density") {
                    if slope < -1e-6 {
                        notes.push("density↓ (new entities outpacing connections)");
                    } else if slope > 1e-6 {
                        notes.push("density↑ (knowledge deepening)");
                    }
                }
                if notes.is_empty() {
                    String::new()
                } else {
                    format!(" Evolution: {}.", notes.join(", "))
                }
            }
            Err(_) => String::new(),
        };

        Ok(format!(
            "Discovery trend: {} (avg Δrels: {:.1}). {}{}{}{}",
            trend, avg_delta, recommendation, velocity_note, cluster_note, evolution_note
        ))
    }

    // -----------------------------------------------------------------------
    // Frontier auto-seeding
    // -----------------------------------------------------------------------

    /// Seed the crawl frontier with Wikipedia URLs for high-value isolated entities.
    /// These are well-known entities (high confidence) that have zero relations and
    /// zero facts — crawling their Wikipedia pages would integrate them into the graph.
    /// Returns the number of URLs added to the frontier.
    pub fn seed_frontier_for_isolated_entities(&self, max_seeds: usize) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        let mut connected_ids: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected_ids.insert(r.subject_id);
            connected_ids.insert(r.object_id);
        }

        // High-value isolated entities: meaningful, high confidence, multi-word names
        let mut candidates: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| {
                !connected_ids.contains(&e.id)
                    && meaningful.contains(&e.id)
                    && e.confidence >= 0.8
                    && !is_noise_name(&e.name)
                    && HIGH_VALUE_TYPES.contains(&e.entity_type.as_str())
                    && e.name.len() >= 3
            })
            .collect();

        // Prioritize: multi-word names first (more likely to have Wikipedia pages),
        // then by confidence, then by entity type (person > place > concept > org)
        candidates.sort_by(|a, b| {
            let a_words = a.name.split_whitespace().count();
            let b_words = b.name.split_whitespace().count();
            let type_priority = |t: &str| -> u8 {
                match t {
                    "person" => 0,
                    "place" => 1,
                    "organization" => 2,
                    "concept" => 3,
                    "technology" => 4,
                    _ => 5,
                }
            };
            b_words
                .cmp(&a_words)
                .then(
                    b.confidence
                        .partial_cmp(&a.confidence)
                        .unwrap_or(std::cmp::Ordering::Equal),
                )
                .then(type_priority(&a.entity_type).cmp(&type_priority(&b.entity_type)))
        });

        let mut seeded = 0usize;
        for ent in candidates.iter().take(max_seeds) {
            // Generate Wikipedia URL from entity name
            let wiki_title = ent.name.replace(' ', "_");
            let url = format!("https://en.wikipedia.org/wiki/{}", wiki_title);
            // Priority: higher for persons and places (more likely to exist)
            let priority = match ent.entity_type.as_str() {
                "person" => 8,
                "place" => 7,
                "organization" => 6,
                _ => 5,
            };
            if self.brain.add_to_frontier(&url, priority).is_ok() {
                seeded += 1;
            }
        }

        if seeded > 0 {
            eprintln!(
                "[PROMETHEUS] Seeded frontier with {} Wikipedia URLs for isolated entities",
                seeded
            );
        }
        Ok(seeded)
    }

    /// Seed frontier URLs for connected but fact-poor entities.
    /// These are high-PageRank entities with many relations but zero facts —
    /// crawling their Wikipedia pages would add structured knowledge (birth/death
    /// dates, descriptions, key facts).
    pub fn seed_frontier_for_fact_poor_entities(&self, max_seeds: usize) -> Result<usize> {
        // Find connected entities with ≥3 relations but 0 facts, ordered by relation count
        let candidates: Vec<(String, String, i64)> = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT e.name, e.entity_type,
                        (SELECT COUNT(*) FROM relations r WHERE r.subject_id = e.id OR r.object_id = e.id) as rel_count
                 FROM entities e
                 WHERE e.entity_type IN ('person', 'place', 'organization', 'technology', 'concept')
                   AND (SELECT COUNT(*) FROM facts f WHERE f.entity_id = e.id) = 0
                   AND rel_count >= 3
                 ORDER BY rel_count DESC
                 LIMIT ?1",
            )?;
            let rows = stmt
                .query_map(params![max_seeds * 2], |row| {
                    Ok((row.get(0)?, row.get(1)?, row.get(2)?))
                })?
                .filter_map(|r| r.ok())
                .collect();
            Ok(rows)
        })?;

        let mut seeded = 0usize;
        for (name, entity_type, _rel_count) in &candidates {
            if seeded >= max_seeds {
                break;
            }
            if is_noise_name(name) || name.len() < 3 {
                continue;
            }
            let wiki_title = name.replace(' ', "_");
            let url = format!("https://en.wikipedia.org/wiki/{}", wiki_title);
            let priority = match entity_type.as_str() {
                "person" => 9, // Higher than isolated entity seeds
                "place" => 8,
                "organization" => 7,
                _ => 6,
            };
            if self.brain.add_to_frontier(&url, priority).is_ok() {
                seeded += 1;
            }
        }

        if seeded > 0 {
            eprintln!(
                "[PROMETHEUS] Seeded frontier with {} Wikipedia URLs for fact-poor connected entities",
                seeded
            );
        }
        Ok(seeded)
    }

    // -----------------------------------------------------------------------
    // Confidence band accuracy (meta-learning)
    // -----------------------------------------------------------------------

    /// Analyze historical accuracy of hypotheses grouped by confidence band.
    /// Returns Vec of (band_label, total, confirmed, rejected, accuracy).
    /// Used to calibrate future hypothesis confidence — if 0.6-0.7 band has
    /// only 50% accuracy, we know to be more conservative in that range.
    pub fn confidence_band_accuracy(&self) -> Result<Vec<(String, i64, i64, i64, f64)>> {
        let bands = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT
                    CASE
                        WHEN confidence < 0.3 THEN '0.0-0.3'
                        WHEN confidence < 0.5 THEN '0.3-0.5'
                        WHEN confidence < 0.6 THEN '0.5-0.6'
                        WHEN confidence < 0.7 THEN '0.6-0.7'
                        WHEN confidence < 0.8 THEN '0.7-0.8'
                        WHEN confidence < 0.9 THEN '0.8-0.9'
                        ELSE '0.9-1.0'
                    END as band,
                    COUNT(*) as total,
                    SUM(CASE WHEN status='confirmed' THEN 1 ELSE 0 END) as confirmed,
                    SUM(CASE WHEN status='rejected' THEN 1 ELSE 0 END) as rejected
                 FROM hypotheses
                 WHERE status IN ('confirmed', 'rejected')
                 GROUP BY band
                 ORDER BY band",
            )?;
            let rows = stmt.query_map([], |row| {
                let band: String = row.get(0)?;
                let total: i64 = row.get(1)?;
                let confirmed: i64 = row.get(2)?;
                let rejected: i64 = row.get(3)?;
                let accuracy = if total > 0 {
                    confirmed as f64 / total as f64
                } else {
                    0.0
                };
                Ok((band, total, confirmed, rejected, accuracy))
            })?;
            rows.collect::<Result<Vec<_>, _>>()
        })?;
        Ok(bands)
    }

    // -----------------------------------------------------------------------
    // Discovery plateau detection
    // -----------------------------------------------------------------------

    /// Detect when discovery is plateauing by analyzing recent graph snapshots.
    /// Returns (is_plateauing, plateau_metric, recommendation).
    /// Plateau = fragmentation hasn't improved by >0.5% over last N snapshots.
    pub fn detect_discovery_plateau(&self, lookback: usize) -> Result<(bool, f64, String)> {
        let snapshots = crate::graph::get_graph_snapshots(self.brain, lookback.max(3))?;
        if snapshots.len() < 3 {
            return Ok((
                false,
                0.0,
                "Insufficient data for plateau detection".to_string(),
            ));
        }

        // Tuple: (taken_at, entities, relations, components, largest_pct, avg_degree, isolated, density, fragmentation, modularity)
        // Check fragmentation trend
        let recent_frag = snapshots[0].8;
        let oldest_frag = snapshots[snapshots.len() - 1].8;
        let frag_improvement = oldest_frag - recent_frag; // positive = improving

        // Check density trend
        let recent_density = snapshots[0].7;
        let oldest_density = snapshots[snapshots.len() - 1].7;
        let density_improvement = recent_density - oldest_density; // positive = improving

        // Check if isolated count is stable
        let recent_isolated = snapshots[0].6 as f64;
        let oldest_isolated = snapshots[snapshots.len() - 1].6 as f64;
        let isolation_reduction = oldest_isolated - recent_isolated; // positive = improving

        let is_plateauing = frag_improvement.abs() < 0.005
            && density_improvement.abs() < 1e-6
            && isolation_reduction.abs() < 50.0;

        let recommendation = if is_plateauing {
            let mut rec = Vec::new();
            if recent_frag > 0.8 {
                rec.push("High fragmentation persists — seed frontier with isolated entity Wikipedia pages");
            }
            if recent_density < 0.001 {
                rec.push("Very low density — focus on enriching existing entities over discovering new ones");
            }
            if isolation_reduction < 10.0 {
                rec.push("Island reduction stalled — try aggressive name-token matching or targeted crawls");
            }
            if rec.is_empty() {
                "Discovery plateau detected but metrics are healthy".to_string()
            } else {
                rec.join("; ")
            }
        } else if frag_improvement > 0.01 {
            format!(
                "Healthy convergence: fragmentation improved by {:.1}%",
                frag_improvement * 100.0
            )
        } else if frag_improvement < -0.005 {
            "Fragmentation worsening — new entities being added faster than they're being connected"
                .to_string()
        } else {
            "Stable — minimal changes in graph structure".to_string()
        };

        Ok((is_plateauing, frag_improvement, recommendation))
    }

    /// Generate targeted crawl suggestions for the most impactful entities to enrich.
    /// Combines isolated entity importance, name recognizability, and type diversity.
    /// Returns Vec of (entity_name, entity_type, priority_score, reason).
    pub fn suggest_targeted_crawls(
        &self,
        limit: usize,
    ) -> Result<Vec<(String, String, f64, String)>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut connected_ids: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected_ids.insert(r.subject_id);
            connected_ids.insert(r.object_id);
        }

        let meaningful = meaningful_ids(self.brain)?;

        let mut suggestions: Vec<(String, String, f64, String)> = Vec::new();

        for e in &entities {
            if connected_ids.contains(&e.id) || !meaningful.contains(&e.id) {
                continue;
            }
            if is_noise_name(&e.name) || is_noise_type(&e.entity_type) {
                continue;
            }
            if e.confidence < 0.7 {
                continue;
            }

            let mut score: f64 = e.confidence;
            let mut reasons: Vec<String> = Vec::new();

            // Multi-word names are more likely to be unique/crawlable
            let word_count = e.name.split_whitespace().count();
            if word_count >= 2 {
                score += 0.15;
                reasons.push(format!("{}-word name", word_count));
            }

            // Entity type priority
            match e.entity_type.as_str() {
                "person" => {
                    score += 0.1;
                    reasons.push("person (high crawl value)".to_string());
                }
                "organization" => {
                    score += 0.08;
                    reasons.push("organization".to_string());
                }
                "place" => {
                    score += 0.06;
                    reasons.push("place".to_string());
                }
                "technology" | "concept" => {
                    score += 0.04;
                    reasons.push(e.entity_type.clone());
                }
                _ => {}
            }

            // High confidence = more likely real entity
            if e.confidence >= 0.95 {
                score += 0.05;
                reasons.push("very high confidence".to_string());
            }

            suggestions.push((
                e.name.clone(),
                e.entity_type.clone(),
                score,
                reasons.join(", "),
            ));
        }

        suggestions.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap_or(std::cmp::Ordering::Equal));
        suggestions.truncate(limit);
        Ok(suggestions)
    }

    // -----------------------------------------------------------------------
    // Output formats
    // -----------------------------------------------------------------------

    pub fn report_json(&self, report: &DiscoveryReport) -> String {
        serde_json::to_string_pretty(report).unwrap_or_else(|_| "{}".to_string())
    }

    /// Find likely duplicate entities using word-overlap similarity.
    /// Returns (entity_a, entity_b, similarity, suggested_action).
    /// Much better than Levenshtein for multi-word entity names like
    /// "Swiss Federal Institute" vs "Federal Institute of Switzerland".
    pub fn find_fuzzy_duplicates(&self) -> Result<Vec<(String, String, f64, String)>> {
        let entities = self.brain.all_entities()?;
        let meaningful: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| {
                !is_noise_type(&e.entity_type) && !is_noise_name(&e.name) && e.name.len() >= 3
            })
            .collect();

        // Build word sets for each entity (lowercase, skip tiny words)
        // Normalize abbreviations first so "St Gotthard" matches "Saint Gotthard"
        let word_sets: Vec<(i64, &str, &str, HashSet<String>)> = meaningful
            .iter()
            .map(|e| {
                let normalized = normalize_abbreviations(&e.name);
                let words: HashSet<String> = normalized
                    .to_lowercase()
                    .split_whitespace()
                    .filter(|w| w.len() >= 3)
                    .map(|w| w.trim_matches(|c: char| !c.is_alphanumeric()).to_string())
                    .filter(|w| !w.is_empty())
                    .collect();
                (e.id, e.name.as_str(), e.entity_type.as_str(), words)
            })
            .collect();

        let mut duplicates = Vec::new();

        // Only compare entities of the same type, limit comparisons
        let mut by_type: HashMap<&str, Vec<usize>> = HashMap::new();
        for (idx, (_, _, etype, _)) in word_sets.iter().enumerate() {
            by_type.entry(etype).or_default().push(idx);
        }

        for indices in by_type.values() {
            // Skip huge groups to avoid O(n²) explosion
            if indices.len() > 2000 {
                continue;
            }
            for (pos_i, &idx_a) in indices.iter().enumerate() {
                let (_, name_a, _, words_a) = &word_sets[idx_a];
                if words_a.is_empty() {
                    continue;
                }
                for &idx_b in &indices[(pos_i + 1)..] {
                    let (_, name_b, _, words_b) = &word_sets[idx_b];
                    if words_b.is_empty() {
                        continue;
                    }

                    let intersection = words_a.intersection(words_b).count();
                    if intersection == 0 {
                        continue;
                    }
                    let union = words_a.union(words_b).count();
                    let jaccard = intersection as f64 / union as f64;

                    // High overlap = likely duplicate
                    if jaccard >= 0.5 && intersection >= 2 {
                        // Auto-merge if very high Jaccard OR if Jaccard ≥ 0.7 and
                        // names have similar length (avoids merging "X" into "X Y Z")
                        let len_ratio = name_a.len().min(name_b.len()) as f64
                            / name_a.len().max(name_b.len()) as f64;
                        let action = if jaccard >= 0.8 || (jaccard >= 0.7 && len_ratio >= 0.6) {
                            "merge".to_string()
                        } else {
                            "review".to_string()
                        };
                        duplicates.push((name_a.to_string(), name_b.to_string(), jaccard, action));
                    }

                    // Also check containment (one name is subset of another)
                    let contained = intersection as f64 / words_a.len().min(words_b.len()) as f64;
                    if contained >= 0.9 && jaccard < 0.5 && words_a.len().min(words_b.len()) >= 2 {
                        duplicates.push((
                            name_a.to_string(),
                            name_b.to_string(),
                            contained * 0.7, // lower confidence for containment
                            "review-containment".to_string(),
                        ));
                    }
                }
            }
        }

        duplicates.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap_or(std::cmp::Ordering::Equal));
        duplicates.truncate(100);
        Ok(duplicates)
    }

    /// Auto-merge fuzzy duplicates with high confidence.
    /// Keeps the shorter/cleaner name, merges the longer/noisier one into it.
    pub fn auto_merge_duplicates(&self, dupes: &[(String, String, f64, String)]) -> Result<usize> {
        let mut merged = 0usize;
        for (name_a, name_b, _sim, action) in dupes {
            if action != "merge" {
                continue;
            }
            let entity_a = self.brain.get_entity_by_name(name_a)?;
            let entity_b = self.brain.get_entity_by_name(name_b)?;
            if let (Some(a), Some(b)) = (entity_a, entity_b) {
                // Keep the shorter name (usually cleaner)
                let (keep, remove) = if a.name.len() <= b.name.len() {
                    (a, b)
                } else {
                    (b, a)
                };
                self.brain.merge_entities(remove.id, keep.id)?;
                merged += 1;
            }
        }
        Ok(merged)
    }

    /// Analyze topic coverage: group entities by source URL domain and measure
    /// how well each topic area is connected internally.
    pub fn topic_coverage_analysis(&self) -> Result<Vec<(String, usize, usize, f64, String)>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        // Build entity→source mapping from relations
        let mut entity_sources: HashMap<i64, HashSet<String>> = HashMap::new();
        for r in &relations {
            if !r.source_url.is_empty() {
                // Extract domain from URL
                let domain = extract_domain(&r.source_url);
                entity_sources
                    .entry(r.subject_id)
                    .or_default()
                    .insert(domain.clone());
                entity_sources
                    .entry(r.object_id)
                    .or_default()
                    .insert(domain);
            }
        }

        // Also check source_url on entities themselves (from facts)
        let facts_sources: HashMap<i64, HashSet<String>> = {
            let mut m: HashMap<i64, HashSet<String>> = HashMap::new();
            for e in &entities {
                let facts = self.brain.get_facts_for(e.id)?;
                for f in &facts {
                    if !f.source_url.is_empty() {
                        let domain = extract_domain(&f.source_url);
                        m.entry(e.id).or_default().insert(domain);
                    }
                }
            }
            m
        };

        // Merge
        for (eid, sources) in facts_sources {
            entity_sources.entry(eid).or_default().extend(sources);
        }

        // Group entities by domain
        let mut domain_entities: HashMap<String, HashSet<i64>> = HashMap::new();
        for (eid, domains) in &entity_sources {
            for d in domains {
                domain_entities.entry(d.clone()).or_default().insert(*eid);
            }
        }

        // For each domain: count entities, count internal relations, compute density
        let meaningful = meaningful_ids(self.brain)?;
        let mut coverage: Vec<(String, usize, usize, f64, String)> = Vec::new();

        for (domain, eids) in &domain_entities {
            let meaningful_count = eids.iter().filter(|id| meaningful.contains(id)).count();
            if meaningful_count < 2 {
                continue;
            }

            // Count relations between entities in this domain
            let mut internal_rels = 0usize;
            for r in &relations {
                if eids.contains(&r.subject_id) && eids.contains(&r.object_id) {
                    internal_rels += 1;
                }
            }

            let max_possible = meaningful_count * (meaningful_count - 1) / 2;
            let density = if max_possible > 0 {
                internal_rels as f64 / max_possible as f64
            } else {
                0.0
            };

            let assessment = if density < 0.01 {
                "sparse — needs more crawling".to_string()
            } else if density < 0.1 {
                "moderate — some connections exist".to_string()
            } else {
                "well-connected".to_string()
            };

            coverage.push((
                domain.clone(),
                meaningful_count,
                internal_rels,
                density,
                assessment,
            ));
        }

        coverage.sort_by(|a, b| b.1.cmp(&a.1)); // Sort by entity count
        Ok(coverage)
    }

    /// Graph evolution metrics: compare current state against prior discovery runs.
    /// Tracks growth rate, connectivity improvement, and hypothesis hit rate.
    pub fn evolution_metrics(&self) -> Result<HashMap<String, f64>> {
        let mut metrics: HashMap<String, f64> = HashMap::new();

        // Hypothesis success rate by pattern type
        let weights = self.get_pattern_weights()?;
        for w in &weights {
            metrics.insert(format!("hit_rate_{}", w.pattern_type), w.weight);
        }

        // Overall hypothesis stats
        let all_hyps = self.list_hypotheses(None)?;
        let total = all_hyps.len() as f64;
        let confirmed = all_hyps
            .iter()
            .filter(|h| h.status == HypothesisStatus::Confirmed)
            .count() as f64;
        let rejected = all_hyps
            .iter()
            .filter(|h| h.status == HypothesisStatus::Rejected)
            .count() as f64;

        if total > 0.0 {
            metrics.insert("hypothesis_confirmation_rate".into(), confirmed / total);
            metrics.insert("hypothesis_rejection_rate".into(), rejected / total);
        }

        // Island ratio (lower is better)
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }
        let meaningful = meaningful_ids(self.brain)?;
        let meaningful_islands = meaningful
            .iter()
            .filter(|id| !connected.contains(id))
            .count();
        if !meaningful.is_empty() {
            metrics.insert(
                "island_ratio".into(),
                meaningful_islands as f64 / meaningful.len() as f64,
            );
        }

        // Predicate diversity
        let (_, _, _, div_ratio) = self.predicate_diversity()?;
        metrics.insert("predicate_diversity".into(), div_ratio);

        // Entity count and relation count
        metrics.insert("total_entities".into(), entities.len() as f64);
        metrics.insert("total_relations".into(), relations.len() as f64);
        metrics.insert("meaningful_entities".into(), meaningful.len() as f64);

        Ok(metrics)
    }

    /// Find entities that are likely abbreviated forms of other entities.
    /// E.g., "Ada" ↔ "Ada Lovelace", "RISC-V" ↔ "RISC-V Foundation".
    /// Returns (short_name, full_name, entity_type, confidence).
    pub fn find_name_subsumptions(&self) -> Result<Vec<(String, String, String, f64)>> {
        let entities = self.brain.all_entities()?;
        let meaningful: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| {
                !is_noise_type(&e.entity_type) && !is_noise_name(&e.name) && e.name.len() >= 2
            })
            .collect();

        // Group by type for efficient comparison
        let mut by_type: HashMap<&str, Vec<&crate::db::Entity>> = HashMap::new();
        for e in &meaningful {
            by_type.entry(&e.entity_type).or_default().push(e);
        }

        let mut subsumptions = Vec::new();
        for group in by_type.values() {
            if group.len() > 3000 {
                continue; // skip huge groups
            }
            for i in 0..group.len() {
                let short = group[i];
                let short_lower = short.name.to_lowercase();
                let short_words: Vec<&str> = short_lower.split_whitespace().collect();
                if short_words.len() > 3 {
                    continue; // only look at short names as "abbreviations"
                }
                for j in 0..group.len() {
                    if i == j {
                        continue;
                    }
                    let full = group[j];
                    let full_lower = full.name.to_lowercase();
                    let full_words: Vec<&str> = full_lower.split_whitespace().collect();
                    // Short name must be shorter
                    if short_words.len() >= full_words.len() {
                        continue;
                    }
                    // Check if short name starts full name (e.g. "Ada" starts "Ada Lovelace")
                    if full_lower.starts_with(&short_lower)
                        && full_lower.len() > short_lower.len() + 1
                    {
                        let conf = 0.6 + (short_words.len() as f64 * 0.1).min(0.3);
                        subsumptions.push((
                            short.name.clone(),
                            full.name.clone(),
                            short.entity_type.clone(),
                            conf,
                        ));
                    }
                    // Also check suffix match: "Elwood Shannon" ends "Claude Elwood Shannon"
                    else if full_lower.ends_with(&short_lower)
                        && full_lower.len() > short_lower.len() + 1
                        && short_words.len() >= 2
                    {
                        let conf = 0.55 + (short_words.len() as f64 * 0.1).min(0.3);
                        subsumptions.push((
                            short.name.clone(),
                            full.name.clone(),
                            short.entity_type.clone(),
                            conf,
                        ));
                    }
                }
            }
        }
        subsumptions.sort_by(|a, b| b.3.partial_cmp(&a.3).unwrap_or(std::cmp::Ordering::Equal));
        subsumptions.truncate(100);
        Ok(subsumptions)
    }

    /// Purge noise entities from the brain — entities matching noise filters
    /// that have zero or very few relations. Returns count of purged entities.
    pub fn purge_noise_entities(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        let mut purged = 0usize;
        for e in &entities {
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            // Purge isolated or near-isolated noise; also purge zero-degree entities
            // with very low confidence
            let is_noise = is_noise_name(&e.name) || is_noise_type(&e.entity_type);
            let is_low_value = deg == 0 && e.confidence < 0.3 && e.name.len() < 4;
            // Aggressive: purge isolated entities that look like citation fragments
            let is_citation_fragment = deg == 0 && looks_like_citation(&e.name);
            // For entities matching noise name patterns, purge up to degree 3
            // (many noise entities accumulate relations through automated discovery)
            let noise_threshold = if is_noise_name(&e.name) { 3 } else { 1 };
            if e.name.contains("Podolsky") || e.name.contains("Bohr Bush") {
                eprintln!(
                    "[PURGE-DEBUG] '{}': deg={}, is_noise={}, is_noise_name={}, noise_threshold={}, is_low_value={}, is_citation={}",
                    e.name, deg, is_noise, is_noise_name(&e.name), noise_threshold, is_low_value, is_citation_fragment
                );
            }
            if (deg <= noise_threshold && is_noise) || is_low_value || is_citation_fragment {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    conn.execute(
                        "DELETE FROM relations WHERE subject_id = ?1 OR object_id = ?1",
                        params![e.id],
                    )?;
                    Ok(())
                })?;
                purged += 1;
            }
        }
        Ok(purged)
    }

    /// Bulk quality cleanup: remove isolated entities that are clearly extraction noise.
    /// More aggressive than purge_noise_entities — targets patterns common in Wikipedia extraction.
    /// Returns count of entities removed.
    pub fn bulk_quality_cleanup(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        let mut removed = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue; // keep all connected entities
            }
            let should_remove = is_extraction_noise(&e.name, &e.entity_type);
            if should_remove {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    Ok(())
                })?;
                removed += 1;
            }
        }
        Ok(removed)
    }

    /// Normalize generic predicates to more specific ones using context.
    /// E.g., "Person X is Concept Y" → "Person X instance_of Concept Y".
    /// Returns count of relations updated.
    pub fn normalize_predicates(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, String> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.clone()))
            .collect();

        let mut updated = 0usize;
        for r in &relations {
            if !is_generic_predicate(&r.predicate) {
                continue;
            }
            let subj_type = id_to_type
                .get(&r.subject_id)
                .map(|s| s.as_str())
                .unwrap_or("unknown");
            let obj_type = id_to_type
                .get(&r.object_id)
                .map(|s| s.as_str())
                .unwrap_or("unknown");

            let new_pred = match (r.predicate.as_str(), subj_type, obj_type) {
                ("is", "person", "concept") => Some("instance_of"),
                ("is", "person", "organization") => Some("member_of"),
                ("is", "organization", "concept") => Some("classified_as"),
                ("is", "place", "concept") => Some("classified_as"),
                ("is", "concept", "concept") => Some("subclass_of"),
                ("is", _, "place") => Some("located_in"),
                ("is", "technology", "concept") => Some("classified_as"),
                ("is", "product", "concept") => Some("classified_as"),
                ("has", "person", "concept") => Some("possesses"),
                ("has", "organization", "concept") => Some("features"),
                ("has", "technology", "concept") => Some("features"),
                ("has", "place", "concept") => Some("features"),
                ("was", "person", "concept") => Some("formerly"),
                ("was", "person", "organization") => Some("formerly_at"),
                ("was", "organization", "concept") => Some("formerly"),
                ("are", "concept", "concept") => Some("subclass_of"),
                ("are", _, "concept") => Some("classified_as"),
                ("were", "person", "organization") => Some("formerly_at"),
                ("were", _, "concept") => Some("formerly"),
                ("had", "person", "concept") => Some("formerly_possessed"),
                ("had", "organization", "concept") => Some("formerly_had"),
                ("do", _, _) | ("does", _, _) | ("did", _, _) => Some("performs"),
                _ => None,
            };

            if let Some(np) = new_pred {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE relations SET predicate = ?1 WHERE subject_id = ?2 AND object_id = ?3 AND predicate = ?4",
                        params![np, r.subject_id, r.object_id, r.predicate],
                    )?;
                    Ok(())
                })?;
                updated += 1;
            }
        }
        // Phase 2: consolidate low-frequency verb synonyms into canonical predicates
        let verb_synonyms: &[(&[&str], &str)] = &[
            (
                &["killed", "murdered", "assassinated", "poisoned"],
                "killed",
            ),
            (&["defeated", "conquered"], "defeated"),
            (&["captured", "seized", "taken"], "captured"),
            (&["founded", "established", "formed"], "founded"),
            (&["built", "constructed", "created"], "built"),
            (&["invented", "developed", "designed"], "invented"),
            (&["wrote", "published", "authored"], "wrote"),
            (&["hired", "recruited", "appointed", "elected"], "appointed"),
            (&["joined", "enrolled", "readmitted"], "joined"),
            (&["inspired", "influenced", "informed"], "influenced"),
            (&["supported", "approved", "endorsed"], "supported"),
            (&["introduced", "launched", "released"], "launched"),
            (&["replaced", "succeeded", "relieved"], "replaced"),
            (&["portrayed", "described", "depicted"], "portrayed"),
            (&["colonized", "invaded", "raided", "attacked"], "invaded"),
            (&["signed", "agreed", "ratified"], "signed"),
            (&["named", "named_after", "called"], "named_after"),
            (&["based_in", "co_located_in", "located_in"], "located_in"),
        ];
        for (synonyms, canonical) in verb_synonyms {
            for syn in *synonyms {
                if *syn == *canonical {
                    continue;
                }
                let count: i64 = self.brain.with_conn(|conn| {
                    conn.query_row(
                        "SELECT COUNT(*) FROM relations WHERE predicate = ?1",
                        params![syn],
                        |row| row.get(0),
                    )
                })?;
                if count > 0 {
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "UPDATE relations SET predicate = ?1 WHERE predicate = ?2",
                            params![canonical, syn],
                        )?;
                        Ok(())
                    })?;
                    updated += count as usize;
                }
            }
        }

        // Phase 3: refine `active_in` to more specific predicates based on entity types.
        // `active_in` is overly generic (second most common predicate) and loses information.
        let id_to_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        for r in &relations {
            if r.predicate != "active_in" {
                continue;
            }
            let subj_type = id_to_type
                .get(&r.subject_id)
                .map(|s| s.as_str())
                .unwrap_or("unknown");
            let obj_type = id_to_type
                .get(&r.object_id)
                .map(|s| s.as_str())
                .unwrap_or("unknown");
            let obj_name = id_to_name
                .get(&r.object_id)
                .copied()
                .unwrap_or("")
                .to_lowercase();

            let new_pred = match (subj_type, obj_type) {
                ("person", "organization") => Some("member_of"),
                ("person", "place") => Some("based_in"),
                ("person", "concept") => {
                    // Distinguish: if concept looks like a field/discipline → "works_in"
                    // If it looks like a movement/era → "participated_in"
                    if obj_name.ends_with("ism")
                        || obj_name.ends_with("ics")
                        || obj_name.ends_with("ology")
                        || obj_name.ends_with("istry")
                        || obj_name.ends_with("nomy")
                        || obj_name.contains("science")
                        || obj_name.contains("theory")
                        || obj_name.contains("engineering")
                    {
                        Some("works_in")
                    } else if obj_name.contains("war")
                        || obj_name.contains("revolution")
                        || obj_name.contains("movement")
                        || obj_name.contains("campaign")
                    {
                        Some("participated_in")
                    } else {
                        None // keep active_in for ambiguous concepts
                    }
                }
                ("organization", "place") => Some("based_in"),
                ("organization", "concept") => Some("operates_in"),
                ("technology", "concept") => Some("applied_in"),
                _ => None,
            };

            if let Some(np) = new_pred {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                        params![np, r.id],
                    )?;
                    Ok(())
                })?;
                updated += 1;
            }
        }

        // Phase 4: prune contemporary_of with confidence < 0.4 where both entities
        // have ≤ 3 non-contemporary relations (weak evidence, inflates graph noise).
        {
            let mut degree: HashMap<i64, usize> = HashMap::new();
            for r in &relations {
                if r.predicate != "contemporary_of" {
                    *degree.entry(r.subject_id).or_insert(0) += 1;
                    *degree.entry(r.object_id).or_insert(0) += 1;
                }
            }
            let pruned = self.brain.with_conn(|conn| {
                let mut stmt = conn.prepare(
                    "SELECT id, subject_id, object_id, confidence FROM relations WHERE predicate = 'contemporary_of' AND confidence < 0.4"
                )?;
                let rows: Vec<(i64, i64, i64, f64)> = stmt
                    .query_map([], |row| {
                        Ok((row.get(0)?, row.get(1)?, row.get(2)?, row.get(3)?))
                    })?
                    .filter_map(|r| r.ok())
                    .collect();
                let mut count = 0usize;
                eprintln!("[PROMETHEUS] Phase 4: found {} contemporary_of candidates (conf < 0.4)", rows.len());
                for (id, sid, oid, _conf) in &rows {
                    let s_deg = degree.get(sid).copied().unwrap_or(0);
                    let o_deg = degree.get(oid).copied().unwrap_or(0);
                    if s_deg <= 3 && o_deg <= 3 {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![id])?;
                        count += 1;
                    }
                }
                Ok(count)
            })?;
            if pruned > 0 {
                eprintln!(
                    "[PROMETHEUS] Phase 4: pruned {} weak contemporary_of relations (conf < 0.4, both entities ≤3 other rels)",
                    pruned
                );
            }
            updated += pruned;
        }

        Ok(updated)
    }

    /// Reconnect island entities to the main graph using exact name matching.
    /// If an island entity's name appears as a substring of a connected entity
    /// (or vice versa), create a "same_as" or "related_to" relation.
    /// Returns count of new connections made.
    pub fn reconnect_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build lookup: lowercase name → connected entity id
        let mut name_to_connected: HashMap<String, i64> = HashMap::new();
        for e in &entities {
            if connected.contains(&e.id)
                && !is_noise_type(&e.entity_type)
                && !is_noise_name(&e.name)
            {
                name_to_connected.insert(e.name.to_lowercase(), e.id);
            }
        }

        // Find islands with exact name matches to connected entities
        let mut reconnected = 0usize;
        for e in &entities {
            if connected.contains(&e.id) || is_noise_type(&e.entity_type) || is_noise_name(&e.name)
            {
                continue;
            }
            let lower = e.name.to_lowercase();
            // Exact match with a connected entity (different ID, same name)
            if let Some(&hub_id) = name_to_connected.get(&lower) {
                if hub_id != e.id {
                    // Merge: island → hub
                    self.brain.merge_entities(e.id, hub_id)?;
                    reconnected += 1;
                    continue;
                }
            }
            // Containment match: if island name is a prefix OR suffix of a connected
            // entity's name and they share the same type, merge.
            // e.g. "Ada" → "Ada Lovelace" (prefix), "Leibniz" → "Gottfried Wilhelm Leibniz" (suffix)
            if e.name.len() >= 4 && !e.entity_type.is_empty() && e.entity_type != "unknown" {
                let mut best_match: Option<(i64, usize)> = None;
                for (cname, &cid) in &name_to_connected {
                    let is_prefix = cname.starts_with(&lower) && cname.len() > lower.len() + 1;
                    let is_suffix = cname.ends_with(&lower) && cname.len() > lower.len() + 1
                        // ensure the suffix is a whole word (preceded by space)
                        && cname.as_bytes().get(cname.len() - lower.len() - 1) == Some(&b' ');
                    if is_prefix || is_suffix {
                        // Check same type
                        if let Some(ce) = self.brain.get_entity_by_id(cid)? {
                            if ce.entity_type == e.entity_type {
                                let score = lower.len();
                                if best_match.is_none() || score > best_match.unwrap().1 {
                                    best_match = Some((cid, score));
                                }
                            }
                        }
                    }
                }
                if let Some((hub_id, _)) = best_match {
                    self.brain.merge_entities(e.id, hub_id)?;
                    reconnected += 1;
                }
            }
        }
        Ok(reconnected)
    }

    /// Compute entity importance scores combining PageRank, degree, and type value.
    /// Returns top entities sorted by composite score.
    pub fn entity_importance(
        &self,
        limit: usize,
    ) -> Result<Vec<(String, String, f64, usize, f64)>> {
        let pr = crate::graph::pagerank(self.brain, 0.85, 20)?;
        let relations = self.brain.all_relations()?;
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        let entities = self.brain.all_entities()?;
        let mut scores: Vec<(String, String, f64, usize, f64)> = Vec::new();

        for e in &entities {
            if is_noise_type(&e.entity_type) || is_noise_name(&e.name) {
                continue;
            }
            let rank = pr.get(&e.id).copied().unwrap_or(0.0);
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            let type_boost = if HIGH_VALUE_TYPES.contains(&e.entity_type.as_str()) {
                1.5
            } else {
                1.0
            };
            let composite = (rank * 10000.0 + deg as f64) * type_boost;
            if composite > 0.0 {
                scores.push((e.name.clone(), e.entity_type.clone(), rank, deg, composite));
            }
        }

        scores.sort_by(|a, b| b.4.partial_cmp(&a.4).unwrap_or(std::cmp::Ordering::Equal));
        scores.truncate(limit);
        Ok(scores)
    }

    /// Find entity type distribution anomalies — types that are over/under-represented
    /// relative to their connectivity, suggesting systematic extraction bias.
    pub fn type_distribution_analysis(&self) -> Result<Vec<(String, usize, f64, String)>> {
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;
        let relations = self.brain.all_relations()?;

        let mut type_counts: HashMap<String, usize> = HashMap::new();
        let mut type_connected: HashMap<String, usize> = HashMap::new();
        let mut connected_ids: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected_ids.insert(r.subject_id);
            connected_ids.insert(r.object_id);
        }

        for e in &entities {
            if meaningful.contains(&e.id) {
                *type_counts.entry(e.entity_type.clone()).or_insert(0) += 1;
                if connected_ids.contains(&e.id) {
                    *type_connected.entry(e.entity_type.clone()).or_insert(0) += 1;
                }
            }
        }

        let mut analysis = Vec::new();
        for (etype, count) in &type_counts {
            if *count < 5 {
                continue;
            }
            let connected = type_connected.get(etype).copied().unwrap_or(0);
            let connectivity = connected as f64 / *count as f64;
            let assessment = if connectivity < 0.05 {
                format!(
                    "EXTRACTION NOISE: {}/{} connected ({:.0}%) — likely over-extracted",
                    connected,
                    count,
                    connectivity * 100.0
                )
            } else if connectivity < 0.2 {
                format!(
                    "SPARSE: {}/{} connected ({:.0}%) — needs enrichment",
                    connected,
                    count,
                    connectivity * 100.0
                )
            } else if connectivity > 0.8 {
                format!(
                    "HEALTHY: {}/{} connected ({:.0}%)",
                    connected,
                    count,
                    connectivity * 100.0
                )
            } else {
                format!(
                    "MODERATE: {}/{} connected ({:.0}%)",
                    connected,
                    count,
                    connectivity * 100.0
                )
            };
            analysis.push((etype.clone(), *count, connectivity, assessment));
        }
        analysis.sort_by(|a, b| a.2.partial_cmp(&b.2).unwrap_or(std::cmp::Ordering::Equal));
        Ok(analysis)
    }

    /// Fix misclassified entity types based on name heuristics.
    /// E.g., "Middle East" as person → place, "Deep Neural Networks" as organization → concept.
    /// Returns count of entities re-typed.
    pub fn fix_entity_types(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let mut fixed = 0usize;

        for e in &entities {
            let lower = e.name.to_lowercase();
            let words: Vec<&str> = lower.split_whitespace().collect();
            let new_type = detect_correct_type(&lower, &words, &e.entity_type);
            if let Some(nt) = new_type {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE entities SET entity_type = ?1 WHERE id = ?2",
                        params![nt, e.id],
                    )?;
                    Ok(())
                })?;
                fixed += 1;
            }
        }
        Ok(fixed)
    }

    /// Infer entity types from neighborhood: if an "unknown" entity is connected
    /// primarily to entities of a known type via specific predicates, we can infer its type.
    /// E.g., if X →born_in→ Y and Y is "unknown" but connected to persons, Y is likely a place.
    /// Returns count of types inferred.
    pub fn infer_types_from_neighborhood(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let id_to_type: HashMap<i64, String> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.clone()))
            .collect();

        // Build per-entity: {neighbor_type → count} and {predicate_role → count}
        // predicate_role = "object_of:<pred>" or "subject_of:<pred>"
        let mut neighbor_types: HashMap<i64, HashMap<String, usize>> = HashMap::new();
        let mut pred_roles: HashMap<i64, HashMap<String, usize>> = HashMap::new();
        for r in &relations {
            if let Some(obj_type) = id_to_type.get(&r.object_id) {
                if obj_type != "unknown" {
                    *neighbor_types
                        .entry(r.subject_id)
                        .or_default()
                        .entry(obj_type.clone())
                        .or_insert(0) += 1;
                }
            }
            if let Some(subj_type) = id_to_type.get(&r.subject_id) {
                if subj_type != "unknown" {
                    *neighbor_types
                        .entry(r.object_id)
                        .or_default()
                        .entry(subj_type.clone())
                        .or_insert(0) += 1;
                }
            }
            *pred_roles
                .entry(r.subject_id)
                .or_default()
                .entry(format!("subj:{}", r.predicate))
                .or_insert(0) += 1;
            *pred_roles
                .entry(r.object_id)
                .or_default()
                .entry(format!("obj:{}", r.predicate))
                .or_insert(0) += 1;
        }

        // Predicate-based type inference rules
        let pred_type_rules: &[(&str, &str)] = &[
            ("obj:born_in", "place"),
            ("obj:located_in", "place"),
            ("obj:headquartered_in", "place"),
            ("obj:died_in", "place"),
            ("obj:capital_of", "place"),
            ("subj:born_in", "person"),
            ("subj:died_in", "person"),
            ("subj:authored", "person"),
            ("subj:invented", "person"),
            ("subj:discovered", "person"),
            ("subj:founded", "person"),
            ("obj:founded_by", "person"),
            ("obj:invented_by", "person"),
            ("obj:discovered_by", "person"),
            ("obj:developed_by", "person"),
            ("subj:subsidiary_of", "organization"),
            ("subj:member_of", "person"),
            ("obj:member_of", "organization"),
            ("subj:employs", "organization"),
            ("obj:instance_of", "concept"),
        ];

        let mut inferred = 0usize;
        for e in &entities {
            if e.entity_type != "unknown" {
                continue;
            }
            // Try predicate-based rules first
            let mut inferred_type: Option<&str> = None;
            if let Some(roles) = pred_roles.get(&e.id) {
                for &(role_pattern, target_type) in pred_type_rules {
                    if roles.get(role_pattern).copied().unwrap_or(0) >= 1 {
                        inferred_type = Some(target_type);
                        break;
                    }
                }
            }

            // Fall back to majority neighbor type if ≥3 neighbors of same type
            if inferred_type.is_none() {
                if let Some(ntypes) = neighbor_types.get(&e.id) {
                    if let Some((best_type, &count)) = ntypes.iter().max_by_key(|(_, &c)| c) {
                        if count >= 3 && !is_noise_type(best_type) {
                            // If mostly connected to persons, this might be a place/org/concept
                            // (persons cluster around shared contexts)
                            if best_type == "person" {
                                inferred_type = Some("concept"); // conservative
                            } else {
                                // Adopt the dominant neighbor type
                                inferred_type = Some(Box::leak(best_type.clone().into_boxed_str()));
                            }
                        }
                    }
                }
            }

            if let Some(new_type) = inferred_type {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE entities SET entity_type = ?1 WHERE id = ?2",
                        params![new_type, e.id],
                    )?;
                    Ok(())
                })?;
                inferred += 1;
            }
        }
        Ok(inferred)
    }

    /// Compute entity quality scores: higher score = more valuable for discovery.
    /// Score based on: degree, type quality, name quality, fact count, community membership.
    /// Returns (entity_id, score) sorted descending.
    pub fn entity_quality_scores(&self, limit: usize) -> Result<Vec<(i64, String, f64)>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        let mut scores: Vec<(i64, String, f64)> = Vec::new();
        for e in &entities {
            let mut score = 0.0_f64;
            let deg = degree.get(&e.id).copied().unwrap_or(0);

            // Degree contribution (log scale to avoid hub domination)
            score += (1.0 + deg as f64).ln() * 0.3;

            // Type quality
            if HIGH_VALUE_TYPES.contains(&e.entity_type.as_str()) {
                score += 0.3;
            } else if e.entity_type == "unknown" {
                score -= 0.2;
            } else if is_noise_type(&e.entity_type) {
                score -= 0.5;
            }

            // Name quality
            if is_noise_name(&e.name) {
                score -= 1.0;
            }
            if e.name.contains(' ') && e.name.len() >= 5 && e.name.len() <= 40 {
                score += 0.2; // multi-word, reasonable length
            }

            // Confidence
            score += e.confidence * 0.2;

            // Isolated penalty
            if deg == 0 {
                score -= 0.3;
            }

            scores.push((e.id, e.name.clone(), score));
        }
        scores.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap_or(std::cmp::Ordering::Equal));
        scores.truncate(limit);
        Ok(scores)
    }

    /// Deep island cleanup: remove isolated entities that provide no discovery value.
    /// More aggressive than bulk_quality_cleanup — removes ALL isolated entities
    /// with confidence below a threshold and no facts stored.
    /// Returns count removed.
    pub fn deep_island_cleanup(&self, min_confidence: f64) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        let mut removed = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            // Keep high-confidence entities and those with stored facts
            if e.confidence >= min_confidence {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            // Keep entities with well-known names (proper nouns, 2-3 word person names)
            if is_likely_real_entity(&e.name, &e.entity_type) {
                continue;
            }
            self.brain.with_conn(|conn| {
                conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                Ok(())
            })?;
            removed += 1;
        }
        Ok(removed)
    }

    pub fn report_markdown(&self, report: &DiscoveryReport) -> String {
        let mut md = String::new();
        md.push_str("# 🔬 PROMETHEUS Discovery Report\n\n");
        md.push_str(&format!("**Summary:** {}\n\n", report.summary));

        if !report.patterns_found.is_empty() {
            md.push_str("## Patterns Found\n\n");
            for p in &report.patterns_found {
                md.push_str(&format!(
                    "- **{}** (freq: {}): {}\n",
                    p.pattern_type.as_str(),
                    p.frequency,
                    p.description
                ));
            }
            md.push('\n');
        }

        if !report.hypotheses_generated.is_empty() {
            md.push_str("## Hypotheses Generated\n\n");
            for h in &report.hypotheses_generated {
                md.push_str(&format!(
                    "- [{:.2}] {} {} {} — *{}*\n",
                    h.confidence,
                    h.subject,
                    h.predicate,
                    h.object,
                    h.status.as_str()
                ));
            }
        }

        md
    }

    // -----------------------------------------------------------------------
    // Helpers
    // -----------------------------------------------------------------------

    /// Generate hypotheses from hub entities using type-aware Jaccard link prediction.
    /// Instead of blindly connecting hubs to all 2-hop neighbors, this strategy:
    /// 1. Only considers candidates of compatible types (same or related)
    /// 2. Uses Jaccard coefficient with a high threshold for quality
    /// 3. Infers predicates from the specific shared neighbors' edge labels
    /// 4. Requires shared neighbors to be meaningful (not noise)
    pub fn generate_hypotheses_from_hubs(&self) -> Result<Vec<Hypothesis>> {
        let hubs = crate::graph::find_hubs(self.brain, 20)?;
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Build neighbor sets
        let mut neighbors: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            neighbors
                .entry(r.subject_id)
                .or_default()
                .insert(r.object_id);
            neighbors
                .entry(r.object_id)
                .or_default()
                .insert(r.subject_id);
        }

        // Build predicate index: (subject_id, object_id) → predicate
        let mut edge_pred: HashMap<(i64, i64), String> = HashMap::new();
        for r in &relations {
            if !is_generic_predicate(&r.predicate) {
                edge_pred.insert((r.subject_id, r.object_id), r.predicate.clone());
                edge_pred.insert((r.object_id, r.subject_id), r.predicate.clone());
            }
        }

        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        // Type compatibility: which types can meaningfully link?
        let compatible_types: HashMap<&str, &[&str]> = [
            (
                "person",
                &["person", "organization", "concept", "place"][..],
            ),
            (
                "organization",
                &["person", "organization", "place", "concept"][..],
            ),
            ("place", &["place", "organization", "person"][..]),
            ("concept", &["concept", "person", "technology"][..]),
            (
                "technology",
                &["technology", "concept", "person", "organization"][..],
            ),
        ]
        .into_iter()
        .collect();

        let mut hypotheses = Vec::new();
        for (hub_id, hub_degree) in &hubs {
            // Skip very high degree hubs (too generic, e.g. "Zurich", "France")
            if *hub_degree > 30 {
                continue;
            }
            let hub = match id_to_entity.get(hub_id) {
                Some(e) => e,
                None => continue,
            };
            if is_noise_type(&hub.entity_type) || is_noise_name(&hub.name) {
                continue;
            }
            let hub_nbrs = match neighbors.get(hub_id) {
                Some(n) => n,
                None => continue,
            };

            // Get compatible types for this hub
            let compat = compatible_types
                .get(hub.entity_type.as_str())
                .copied()
                .unwrap_or(&["concept"]);

            // Find 2-hop candidates with Jaccard scoring
            let mut two_hop: HashMap<i64, HashSet<i64>> = HashMap::new(); // cand → set of shared neighbors
            for &nbr in hub_nbrs {
                if !meaningful.contains(&nbr) {
                    continue;
                }
                if let Some(nbr_nbrs) = neighbors.get(&nbr) {
                    for &nn in nbr_nbrs {
                        if nn != *hub_id && !hub_nbrs.contains(&nn) && meaningful.contains(&nn) {
                            two_hop.entry(nn).or_default().insert(nbr);
                        }
                    }
                }
            }

            for (cand_id, shared_set) in &two_hop {
                let shared_count = shared_set.len();
                if shared_count < 6 {
                    continue; // Require at least 6 shared meaningful neighbors (raised from 5 to cut false positives)
                }
                let cand = match id_to_entity.get(cand_id) {
                    Some(e) => e,
                    None => continue,
                };
                if is_noise_name(&cand.name) || is_noise_type(&cand.entity_type) {
                    continue;
                }
                // Type compatibility check
                if !compat.contains(&cand.entity_type.as_str()) {
                    continue;
                }

                // Jaccard coefficient: |shared| / |union|
                let cand_nbrs = neighbors.get(cand_id).map(|n| n.len()).unwrap_or(0);
                let union_size = hub_nbrs.len() + cand_nbrs - shared_count;
                let jaccard = if union_size > 0 {
                    shared_count as f64 / union_size as f64
                } else {
                    0.0
                };
                if jaccard < 0.45 {
                    continue; // Require strong overlap ratio (raised from 0.40 to reduce 85% rejection rate)
                }

                // Infer predicate from shared neighbors' edge labels
                let mut pred_counts: HashMap<String, usize> = HashMap::new();
                for &shared_nbr in shared_set {
                    if let Some(p) = edge_pred.get(&(*hub_id, shared_nbr)) {
                        *pred_counts.entry(p.clone()).or_insert(0) += 1;
                    }
                    if let Some(p) = edge_pred.get(&(*cand_id, shared_nbr)) {
                        *pred_counts.entry(p.clone()).or_insert(0) += 1;
                    }
                }
                let best_pred = pred_counts
                    .into_iter()
                    .max_by_key(|(_, c)| *c)
                    .map(|(p, _)| p)
                    .unwrap_or_else(|| "related_to".to_string());

                // Confidence based on Jaccard + shared count
                let base_conf =
                    0.6 + (jaccard * 0.3).min(0.3) + (shared_count as f64 * 0.02).min(0.1);
                let shared_names: Vec<String> = shared_set
                    .iter()
                    .take(4)
                    .filter_map(|id| id_to_entity.get(id).map(|e| e.name.clone()))
                    .collect();

                hypotheses.push(Hypothesis {
                    id: 0,
                    subject: hub.name.clone(),
                    predicate: best_pred,
                    object: cand.name.clone(),
                    confidence: base_conf.min(0.95),
                    evidence_for: vec![format!(
                        "Hub '{}' (degree {}) shares {} neighbors with '{}' (Jaccard: {:.2}): {}",
                        hub.name,
                        hub_degree,
                        shared_count,
                        cand.name,
                        jaccard,
                        shared_names.join(", ")
                    )],
                    evidence_against: vec![],
                    reasoning_chain: vec![
                        format!(
                            "'{}' is a hub (degree {}, type '{}'); '{}' is type '{}'",
                            hub.name, hub_degree, hub.entity_type, cand.name, cand.entity_type
                        ),
                        format!(
                            "Jaccard overlap: {:.2} ({} shared / {} union)",
                            jaccard, shared_count, union_size
                        ),
                        "Type-compatible entities with high neighborhood overlap → likely related"
                            .to_string(),
                    ],
                    status: HypothesisStatus::Proposed,
                    discovered_at: now_str(),
                    pattern_source: "hub_spoke".to_string(),
                });
                if hypotheses.len() >= 50 {
                    return Ok(hypotheses);
                }
            }
        }
        Ok(hypotheses)
    }

    /// Fact-based relation inference: create relations from facts that reference other entities.
    /// E.g., entity "France" has fact "capital: Paris" and entity "Paris" exists → create relation.
    /// Returns count of new relations created.
    pub fn infer_relations_from_facts(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Build name→id lookup (case-insensitive)
        let mut name_to_id: HashMap<String, i64> = HashMap::new();
        for e in &entities {
            if meaningful.contains(&e.id) {
                name_to_id.insert(e.name.to_lowercase(), e.id);
            }
        }

        // Build existing relation set for dedup
        let relations = self.brain.all_relations()?;
        let mut existing: HashSet<(i64, String, i64)> = HashSet::new();
        for r in &relations {
            existing.insert((r.subject_id, r.predicate.clone(), r.object_id));
        }

        // Predicate mapping: fact key → relation predicate
        let key_to_pred: HashMap<&str, &str> = [
            ("capital", "has_capital"),
            ("headquarters", "headquartered_in"),
            ("founded_by", "founded_by"),
            ("creator", "created_by"),
            ("author", "authored_by"),
            ("inventor", "invented_by"),
            ("born_in", "born_in"),
            ("died_in", "died_in"),
            ("located_in", "located_in"),
            ("part_of", "part_of"),
            ("member_of", "member_of"),
            ("parent", "child_of"),
            ("subsidiary", "subsidiary_of"),
            ("language", "uses_language"),
            ("currency", "uses_currency"),
            ("president", "led_by"),
            ("ceo", "led_by"),
            ("founder", "founded_by"),
            ("nationality", "nationality"),
            ("country", "located_in"),
            ("continent", "located_in"),
            ("region", "located_in"),
            ("city", "located_in"),
        ]
        .iter()
        .cloned()
        .collect();

        let mut created = 0usize;
        for e in &entities {
            if !meaningful.contains(&e.id) {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            for f in &facts {
                let pred = key_to_pred
                    .get(f.key.to_lowercase().as_str())
                    .copied()
                    .unwrap_or("related_to");

                // Check if fact value matches an entity name
                let value_lower = f.value.to_lowercase();
                if let Some(&target_id) = name_to_id.get(&value_lower) {
                    if target_id != e.id && !existing.contains(&(e.id, pred.to_string(), target_id))
                    {
                        self.brain
                            .upsert_relation(e.id, pred, target_id, &f.source_url)?;
                        existing.insert((e.id, pred.to_string(), target_id));
                        created += 1;
                    }
                }
            }
        }
        Ok(created)
    }

    /// Entity name cross-reference: find entities whose names contain other entity names.
    /// E.g., "Swiss Federal Council" contains "Swiss" → link to Switzerland if it exists.
    /// Only matches high-value entities with names ≥ 5 chars to avoid false positives.
    /// Returns count of new relations created.
    pub fn crossref_entity_names(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Build candidate targets: meaningful entities with short-ish names (2-4 words)
        let targets: Vec<(i64, String)> = entities
            .iter()
            .filter(|e| {
                meaningful.contains(&e.id)
                    && !is_noise_name(&e.name)
                    && e.name.len() >= 5
                    && e.name.split_whitespace().count() <= 3
                    && HIGH_VALUE_TYPES.contains(&e.entity_type.as_str())
            })
            .map(|e| (e.id, e.name.clone()))
            .collect();

        // Build existing relation set
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            connected.insert(key);
        }

        let mut created = 0usize;
        for e in &entities {
            if !meaningful.contains(&e.id) || is_noise_name(&e.name) {
                continue;
            }
            let e_lower = e.name.to_lowercase();
            let e_words: Vec<&str> = e_lower.split_whitespace().collect();
            if e_words.len() < 2 {
                continue; // only multi-word entities can contain references
            }

            for (tid, tname) in &targets {
                if *tid == e.id {
                    continue;
                }
                let t_lower = tname.to_lowercase();
                // Check if entity name contains the target name as a word boundary match
                if e_lower.contains(&t_lower) && e_lower != t_lower {
                    // Verify word boundary (not just substring within a word)
                    let idx = e_lower.find(&t_lower).unwrap();
                    let before_ok = idx == 0
                        || e_lower.as_bytes()[idx - 1] == b' '
                        || e_lower.as_bytes()[idx - 1] == b'-';
                    let end = idx + t_lower.len();
                    let after_ok = end == e_lower.len()
                        || e_lower.as_bytes()[end] == b' '
                        || e_lower.as_bytes()[end] == b'-';

                    if before_ok && after_ok {
                        let key = if e.id < *tid {
                            (e.id, *tid)
                        } else {
                            (*tid, e.id)
                        };
                        if !connected.contains(&key) {
                            self.brain
                                .upsert_relation(e.id, "associated_with", *tid, "")?;
                            connected.insert(key);
                            created += 1;
                            if created >= 200 {
                                return Ok(created);
                            }
                        }
                    }
                }
            }
        }
        Ok(created)
    }

    /// Decompose compound entity names into base entity + qualifier.
    /// E.g., "Ada Lovelace Building" → relates to existing "Ada Lovelace" entity
    /// with predicate "named_after". Also handles patterns like:
    /// - "X Award" → X + Award concept
    /// - "X Institute" → X + organization  
    /// - "X Day" → X + event
    /// Returns count of new relations created and entities cleaned up.
    pub fn decompose_compound_entities(&self) -> Result<(usize, usize)> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build name→id lookup for connected entities (these are the "real" entities)
        let mut name_to_id: HashMap<String, i64> = HashMap::new();
        for e in &entities {
            if connected.contains(&e.id)
                && !is_noise_name(&e.name)
                && !is_noise_type(&e.entity_type)
            {
                name_to_id.insert(e.name.to_lowercase(), e.id);
            }
        }
        // Also include high-confidence islands
        for e in &entities {
            if !connected.contains(&e.id) && e.confidence >= 0.8 && !is_noise_name(&e.name) {
                name_to_id.entry(e.name.to_lowercase()).or_insert(e.id);
            }
        }

        // Qualifier suffixes that indicate compound entities
        let qualifier_map: &[(&str, &str)] = &[
            ("building", "named_after"),
            ("house", "named_after"),
            ("suite", "named_after"),
            ("center", "named_after"),
            ("centre", "named_after"),
            ("institute", "named_after"),
            ("foundation", "named_after"),
            ("award", "named_after"),
            ("prize", "named_after"),
            ("day", "named_after"),
            ("biography", "subject_of"),
            ("original", "variant_of"),
            ("countess", "title_of"),
            ("theorem", "attributed_to"),
            ("equation", "attributed_to"),
            ("principle", "attributed_to"),
            ("law", "attributed_to"),
            ("conjecture", "attributed_to"),
            ("paradox", "attributed_to"),
            ("constant", "attributed_to"),
            ("transform", "attributed_to"),
            ("distribution", "attributed_to"),
            ("method", "attributed_to"),
            ("algorithm", "attributed_to"),
            ("bridge", "named_after"),
            ("tower", "named_after"),
            ("park", "named_after"),
            ("university", "named_after"),
            ("college", "named_after"),
            ("school", "named_after"),
            ("museum", "named_after"),
            ("library", "named_after"),
            ("hospital", "named_after"),
            ("airport", "named_after"),
            ("station", "named_after"),
            ("square", "named_after"),
            ("street", "named_after"),
        ];

        let mut relations_created = 0usize;
        let mut entities_merged = 0usize;
        let mut existing_rels: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            existing_rels.insert((r.subject_id, r.object_id));
        }

        for e in &entities {
            if is_noise_name(&e.name) || is_noise_type(&e.entity_type) {
                continue;
            }
            let lower = e.name.to_lowercase();
            let words: Vec<&str> = lower.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }

            // Check if the last word is a qualifier
            let last = *words.last().unwrap();
            let qualifier_pred = qualifier_map
                .iter()
                .find(|(q, _)| *q == last)
                .map(|(_, p)| *p);

            if let Some(pred) = qualifier_pred {
                // Try progressively shorter prefixes to find a matching base entity
                for prefix_len in (1..words.len()).rev() {
                    let prefix: String = words[..prefix_len].join(" ");
                    if let Some(&base_id) = name_to_id.get(&prefix) {
                        if base_id != e.id && !existing_rels.contains(&(e.id, base_id)) {
                            self.brain.upsert_relation(e.id, pred, base_id, "")?;
                            existing_rels.insert((e.id, base_id));
                            relations_created += 1;
                        }
                        break;
                    }
                }
            }

            // Also handle "X Y Z" where "Y Z" is a known entity (e.g., "Babbage Ada Lovelace")
            if words.len() >= 3 {
                for split in 1..words.len() {
                    let suffix: String = words[split..].join(" ");
                    if let Some(&base_id) = name_to_id.get(&suffix) {
                        if base_id != e.id && !existing_rels.contains(&(e.id, base_id)) {
                            self.brain
                                .upsert_relation(e.id, "references", base_id, "")?;
                            existing_rels.insert((e.id, base_id));
                            relations_created += 1;
                            break;
                        }
                    }
                }
            }
        }

        // Merge truly redundant island entities: if an island "X Qualifier" has the same
        // type as base entity "X" and the qualifier is just noise, merge into X
        let noise_qualifiers: HashSet<&str> =
            ["original", "wired", "founder"].iter().copied().collect();
        let entities_refreshed = self.brain.all_entities()?;
        for e in &entities_refreshed {
            if connected.contains(&e.id) {
                continue;
            }
            let lower = e.name.to_lowercase();
            let words: Vec<&str> = lower.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }
            let last = *words.last().unwrap();
            if !noise_qualifiers.contains(last) {
                continue;
            }
            let prefix: String = words[..words.len() - 1].join(" ");
            if let Some(&base_id) = name_to_id.get(&prefix) {
                if base_id != e.id {
                    self.brain.merge_entities(e.id, base_id)?;
                    entities_merged += 1;
                }
            }
        }

        Ok((relations_created, entities_merged))
    }

    /// Aggressive island consolidation: for each island entity, check if its name
    /// is an exact prefix match of a connected entity (same type). If so, merge.
    /// Handles cases like "Euler" (island) → "Leonhard Euler" (connected).
    /// Returns count of merges.
    pub fn consolidate_prefix_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build connected entity names (all types in one flat list for cross-type matching)
        let mut all_connected_names: Vec<(i64, String, String)> = Vec::new();
        for e in &entities {
            if connected.contains(&e.id)
                && !is_noise_name(&e.name)
                && !is_noise_type(&e.entity_type)
            {
                all_connected_names.push((e.id, e.name.to_lowercase(), e.entity_type.clone()));
            }
        }

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        // Compatible type pairs for cross-type merging
        let compatible_types = |a: &str, b: &str| -> bool {
            if a == b {
                return true;
            }
            matches!(
                (a, b),
                ("person", "concept")
                    | ("concept", "person")
                    | ("person", "organization")
                    | ("organization", "person")
                    | ("place", "concept")
                    | ("concept", "place")
                    | ("technology", "concept")
                    | ("concept", "technology")
                    | ("organization", "concept")
                    | ("concept", "organization")
            )
        };

        for e in &entities {
            if connected.contains(&e.id) || absorbed.contains(&e.id) {
                continue;
            }
            if is_noise_name(&e.name) || is_noise_type(&e.entity_type) || e.name.len() < 4 {
                continue;
            }
            let lower = e.name.to_lowercase();

            let mut best: Option<(i64, usize, bool)> = None; // (id, len, same_type)
            for (cid, cname, ctype) in &all_connected_names {
                if *cid == e.id {
                    continue;
                }
                // Island name is prefix of connected entity
                if cname.starts_with(&lower) && cname.len() > lower.len() {
                    let next_char = cname.as_bytes()[lower.len()];
                    if next_char == b' ' || next_char == b'-' {
                        let same_type = ctype == &e.entity_type;
                        if !same_type && !compatible_types(&e.entity_type, ctype) {
                            continue;
                        }
                        // Prefer same-type matches, then shorter names
                        let dominated = best.is_some_and(|(_, blen, bsame)| {
                            (same_type && !bsame) || (same_type == bsame && cname.len() < blen)
                        });
                        if best.is_none() || dominated {
                            best = Some((*cid, cname.len(), same_type));
                        }
                    }
                }
            }
            if let Some((target_id, _, _)) = best {
                self.brain.merge_entities(e.id, target_id)?;
                absorbed.insert(e.id);
                merged += 1;
            }
        }
        Ok(merged)
    }

    /// Aggressive island consolidation via word-overlap matching.
    /// For each isolated entity, find the best-matching connected entity of compatible type
    /// using Jaccard word similarity. If similarity ≥ threshold, merge.
    /// This handles cases like "Marie Curie Avenue" → merge into "Marie Curie".
    /// Returns count of merges performed.
    pub fn word_overlap_island_merge(&self, min_jaccard: f64) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build word sets for connected entities, grouped by type
        struct ConnectedInfo {
            id: i64,
            words: HashSet<String>,
            word_count: usize,
        }
        let mut connected_by_type: HashMap<String, Vec<ConnectedInfo>> = HashMap::new();
        for e in &entities {
            if !connected.contains(&e.id) || is_noise_name(&e.name) || is_noise_type(&e.entity_type)
            {
                continue;
            }
            let words: HashSet<String> = e
                .name
                .to_lowercase()
                .split_whitespace()
                .filter(|w| w.len() >= 3)
                .map(|w| w.trim_matches(|c: char| !c.is_alphanumeric()).to_string())
                .filter(|w| !w.is_empty())
                .collect();
            if words.is_empty() {
                continue;
            }
            let wc = words.len();
            connected_by_type
                .entry(e.entity_type.clone())
                .or_default()
                .push(ConnectedInfo {
                    id: e.id,
                    words,
                    word_count: wc,
                });
        }
        // Also allow cross-type matching for person→concept, person→place, etc.
        // Build a flat list for cross-type fallback
        let all_connected: Vec<&ConnectedInfo> =
            connected_by_type.values().flat_map(|v| v.iter()).collect();

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        for e in &entities {
            if connected.contains(&e.id) || absorbed.contains(&e.id) {
                continue;
            }
            if is_noise_name(&e.name) || is_noise_type(&e.entity_type) || e.name.len() < 4 {
                continue;
            }
            let island_words: HashSet<String> = e
                .name
                .to_lowercase()
                .split_whitespace()
                .filter(|w| w.len() >= 3)
                .map(|w| w.trim_matches(|c: char| !c.is_alphanumeric()).to_string())
                .filter(|w| !w.is_empty())
                .collect();
            if island_words.is_empty() {
                continue;
            }

            // First try same-type matches
            let candidates = connected_by_type.get(&e.entity_type);
            let mut best: Option<(i64, f64)> = None;

            if let Some(cands) = candidates {
                for c in cands {
                    let intersection = island_words.intersection(&c.words).count();
                    if intersection == 0 {
                        continue;
                    }
                    let union = island_words.union(&c.words).count();
                    let jaccard = intersection as f64 / union as f64;
                    // Also check containment: if island words are a superset/subset of connected
                    let containment =
                        intersection as f64 / island_words.len().min(c.word_count) as f64;
                    let score = jaccard.max(containment * 0.9); // containment slightly discounted
                    if score >= min_jaccard && (best.is_none() || score > best.unwrap().1) {
                        best = Some((c.id, score));
                    }
                }
            }

            // Cross-type fallback: only if island words fully contained in a connected entity
            if best.is_none() && island_words.len() >= 2 {
                for c in &all_connected {
                    let intersection = island_words.intersection(&c.words).count();
                    let containment = intersection as f64 / island_words.len() as f64;
                    if containment >= 0.95 && intersection >= 2 {
                        let jaccard =
                            intersection as f64 / island_words.union(&c.words).count() as f64;
                        if jaccard >= min_jaccard * 0.8 {
                            best = Some((c.id, jaccard));
                            break;
                        }
                    }
                }
            }

            if let Some((target_id, _score)) = best {
                if target_id != e.id {
                    self.brain.merge_entities(e.id, target_id)?;
                    absorbed.insert(e.id);
                    merged += 1;
                }
            }
        }
        Ok(merged)
    }

    /// Purge single-word island entities that are clearly fragments of known multi-word
    /// connected entities. E.g., "Lovelace" when "Ada Lovelace" exists connected,
    /// "Hopper" when "Grace Hopper" exists connected.
    /// Only purges when the single word appears in ≥2 connected entity names of its type.
    /// Returns count of entities removed.
    pub fn purge_fragment_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build index: for each word, which connected entities contain it?
        let mut word_to_connected: HashMap<String, Vec<i64>> = HashMap::new();
        for e in &entities {
            if connected.contains(&e.id)
                && !is_noise_type(&e.entity_type)
                && !is_noise_name(&e.name)
                && e.name.contains(' ')
            {
                for word in e.name.split_whitespace() {
                    let lower = word.to_lowercase();
                    if lower.len() >= 3 {
                        word_to_connected.entry(lower).or_default().push(e.id);
                    }
                }
            }
        }

        let mut removed = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            // Only target single-word islands
            if e.name.contains(' ') || e.name.len() < 3 {
                continue;
            }
            if is_noise_type(&e.entity_type) {
                continue;
            }
            let lower = e.name.to_lowercase();
            // Check if this word appears as a component of connected entities
            if let Some(matches) = word_to_connected.get(&lower) {
                if !matches.is_empty() {
                    // This single word is a fragment of at least one known entity
                    // Check if it has any facts worth keeping
                    let facts = self.brain.get_facts_for(e.id)?;
                    if facts.is_empty() {
                        // If there's exactly one match, merge into it
                        if matches.len() == 1 {
                            self.brain.merge_entities(e.id, matches[0])?;
                        } else {
                            // Multiple matches — just delete the fragment
                            self.brain.with_conn(|conn| {
                                conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                                Ok(())
                            })?;
                        }
                        removed += 1;
                    }
                }
            }
        }
        Ok(removed)
    }

    /// Suffix-strip island merge: for island entities like "Ada Lovelace WIRED",
    /// strip trailing words one at a time and check if a connected entity exists with
    /// the shorter name. If so, merge. This handles NLP extractors that append context
    /// words to entity names.
    /// Returns count of merges performed.
    pub fn suffix_strip_island_merge(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build name→id index for connected entities (case-insensitive)
        let mut name_to_id: HashMap<String, i64> = HashMap::new();
        for e in &entities {
            if connected.contains(&e.id) && !is_noise_name(&e.name) {
                name_to_id.insert(e.name.to_lowercase(), e.id);
            }
        }

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        for e in &entities {
            if connected.contains(&e.id) || absorbed.contains(&e.id) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }
            // Try stripping 1, 2, (up to half) trailing words
            let max_strip = (words.len() / 2).max(1).min(3);
            for strip in 1..=max_strip {
                if words.len() <= strip {
                    break;
                }
                let prefix: String = words[..words.len() - strip].join(" ");
                if prefix.len() < 3 {
                    break;
                }
                let lower_prefix = prefix.to_lowercase();
                if let Some(&target_id) = name_to_id.get(&lower_prefix) {
                    if target_id != e.id {
                        self.brain.merge_entities(e.id, target_id)?;
                        absorbed.insert(e.id);
                        merged += 1;
                        break;
                    }
                }
            }
        }
        Ok(merged)
    }

    /// Prefix-strip merge: for entities like "Christy Grace Hopper" or "Admiral Grace Hopper",
    /// strip leading words one at a time and check if a shorter-named entity exists.
    /// If so, merge the longer name into the shorter one. Works on both islands and
    /// connected entities (merges redundant variants into the canonical shorter form).
    /// Also strips trailing words as a second pass.
    /// Returns count of merges performed.
    pub fn prefix_strip_island_merge(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        // Build degree map for deciding which entity to keep
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build name→(id, degree) index (case-insensitive)
        let mut name_to_info: HashMap<String, (i64, usize)> = HashMap::new();
        for e in &entities {
            if !is_noise_name(&e.name) && !is_noise_type(&e.entity_type) {
                let deg = degree.get(&e.id).copied().unwrap_or(0);
                let lower = e.name.to_lowercase();
                // Keep the entry with the highest degree
                let existing_deg = name_to_info.get(&lower).map(|(_, d)| *d).unwrap_or(0);
                if deg > existing_deg || !name_to_info.contains_key(&lower) {
                    name_to_info.insert(lower, (e.id, deg));
                }
            }
        }

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        eprintln!(
            "PREFIX_STRIP_DEBUG: entities count={}, name_to_info count={}",
            entities.len(),
            name_to_info.len()
        );
        let mut _checked = 0usize;

        for e in &entities {
            if absorbed.contains(&e.id) || is_noise_name(&e.name) || is_noise_type(&e.entity_type) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 3 {
                continue;
            }
            _checked += 1;

            let my_deg = degree.get(&e.id).copied().unwrap_or(0);

            // Try stripping 1-2 leading words
            let max_strip = (words.len() / 2).max(1).min(2);
            let mut found = false;
            for strip in 1..=max_strip {
                if words.len() <= strip + 1 {
                    break; // result must be at least 2 words
                }
                let suffix: String = words[strip..].join(" ");
                let lower_suffix = suffix.to_lowercase();
                if let Some(&(target_id, target_deg)) = name_to_info.get(&lower_suffix) {
                    if target_id != e.id && target_deg >= my_deg {
                        // Only merge if target has more/equal connections (is more canonical)
                        self.brain.merge_entities(e.id, target_id)?;
                        absorbed.insert(e.id);
                        merged += 1;
                        found = true;
                        break;
                    }
                }
            }
            if found {
                continue;
            }

            // Also try stripping 1-2 trailing words (for patterns like "Blaise Pascal Chairs")
            for strip in 1..=max_strip {
                if words.len() <= strip + 1 {
                    break;
                }
                let prefix: String = words[..words.len() - strip].join(" ");
                let lower_prefix = prefix.to_lowercase();
                if let Some(&(target_id, target_deg)) = name_to_info.get(&lower_prefix) {
                    if target_id != e.id && target_deg >= my_deg {
                        self.brain.merge_entities(e.id, target_id)?;
                        absorbed.insert(e.id);
                        merged += 1;
                        break;
                    }
                }
            }
        }
        eprintln!(
            "PREFIX_STRIP_DEBUG: checked={}, merged={}",
            _checked, merged
        );
        Ok(merged)
    }

    /// Auto-consolidation: use entity_consolidation_candidates to find and merge
    /// entity pairs with high consolidation scores. Only merges pairs above
    /// `min_score` threshold to avoid false positives.
    /// Returns count of merges performed.
    pub fn auto_consolidate_entities(&self, min_score: f64) -> Result<usize> {
        let candidates = self.entity_consolidation_candidates(100)?;
        let mut merged = 0usize;
        let mut absorbed: HashSet<String> = HashSet::new();

        for (name_a, name_b, score, _reason) in &candidates {
            if *score < min_score {
                continue;
            }
            if absorbed.contains(name_a) || absorbed.contains(name_b) {
                continue;
            }
            let entity_a = self.brain.get_entity_by_name(name_a)?;
            let entity_b = self.brain.get_entity_by_name(name_b)?;
            if let (Some(a), Some(b)) = (entity_a, entity_b) {
                // Keep the shorter name (usually the canonical form)
                let (keep, remove) = if a.name.len() <= b.name.len() {
                    (a, b)
                } else {
                    (b, a)
                };
                self.brain.merge_entities(remove.id, keep.id)?;
                absorbed.insert(remove.name.clone());
                merged += 1;
            }
        }
        Ok(merged)
    }

    /// Merge name variants: merge entities whose names are title-prefixed or
    /// noise-suffixed versions of other entities. Works on ALL entities (not just
    /// islands), targeting patterns like:
    /// - "Professor Claude Shannon" → "Claude Shannon"
    /// - "Claude Shannon Time" → "Claude Shannon"
    /// - "Grace Hopper Admiral" → "Grace Hopper"
    /// - "Caliph Selim I" → "Selim I"
    /// Always merges into the entity with higher degree (more connections).
    /// Returns count of merges performed.
    pub fn merge_name_variants(&self) -> Result<usize> {
        // eprintln!("NAME_VARIANT_START: entering merge_name_variants");
        let entities = self.brain.all_entities()?;
        let cs_count = entities
            .iter()
            .filter(|e| e.name.contains("Claude Shannon"))
            .count();
        eprintln!(
            "NAME_VARIANT_START: got {} entities ({} contain 'Claude Shannon')",
            entities.len(),
            cs_count
        );
        let relations = self.brain.all_relations()?;

        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build name→(id, degree) index
        let mut name_to_info: HashMap<String, (i64, usize)> = HashMap::new();
        for e in &entities {
            if !is_noise_type(&e.entity_type) {
                let deg = degree.get(&e.id).copied().unwrap_or(0);
                let lower = e.name.to_lowercase();
                let existing_deg = name_to_info.get(&lower).map(|(_, d)| *d).unwrap_or(0);
                if deg > existing_deg || !name_to_info.contains_key(&lower) {
                    name_to_info.insert(lower, (e.id, deg));
                }
            }
        }

        // Title prefixes that should be stripped
        let title_prefixes: &[&str] = &[
            "professor",
            "prof",
            "dr",
            "sir",
            "lord",
            "lady",
            "king",
            "queen",
            "prince",
            "princess",
            "duke",
            "count",
            "countess",
            "baron",
            "admiral",
            "general",
            "colonel",
            "captain",
            "sultan",
            "caliph",
            "emperor",
            "empress",
            "president",
            "premier",
            "chancellor",
            "minister",
            "saint",
            "pope",
            "chaires",
            "boot",
        ];

        // Trailing noise words that NLP extractors commonly append
        let trailing_noise: &[&str] = &[
            "time",
            "created",
            "admiral",
            "original",
            "wired",
            "founder",
            "bits",
            "focus",
            "dead",
            "mass",
            "chairs",
            "accompagnées",
            "pensées",
            "even",
            "post",
            "hall",
            "suite",
            "center",
            "centre",
            "house",
            "building",
            "institute",
            "biography",
            "award",
            "day",
            "website",
            "frontiers",
            "countess",
            "working",
            "linux",
            "dm",
            "online",
            "resources",
            "imdb",
            "first",
            "start",
            "women",
            "past",
            "prestige",
            "no",
            "lwn",
            "there",
            "yesugei",
            "inc",
            "apsnews",
            "boingboing",
            "ratified",
            "ancien",
            "clark",
        ];

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        let mut iter_count = 0usize;
        for e in &entities {
            iter_count += 1;
            if e.name.contains("Claude Shannon") || e.name.contains("Professor Claude") {
                eprintln!(
                    "NAME_VARIANT_ITER[{}]: '{}' id={} absorbed={} noise_type={}",
                    iter_count,
                    e.name,
                    e.id,
                    absorbed.contains(&e.id),
                    is_noise_type(&e.entity_type)
                );
            }
            if absorbed.contains(&e.id) || is_noise_type(&e.entity_type) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if e.name.contains("Claude Shannon")
                || e.name.contains("Ada Lovelace")
                || e.name.contains("Professor Claude")
            {
                let last_w = words.last().map(|w| w.to_lowercase()).unwrap_or_default();
                let is_trailing = trailing_noise.contains(&last_w.as_str());
                let target = if words.len() >= 2 {
                    name_to_info.get(&words[..words.len() - 1].join(" ").to_lowercase())
                } else {
                    None
                };
                eprintln!(
                    "NAME_VARIANT_DEBUG: '{}' type='{}' words={} last='{}' is_trailing={} target={:?}",
                    e.name, e.entity_type, words.len(), last_w, is_trailing, target
                );
            }
            if words.len() < 2 {
                continue;
            }
            let my_deg = degree.get(&e.id).copied().unwrap_or(0);

            // 1. Strip title prefix — always merge into canonical (untitled) form
            if words.len() >= 2 {
                let first_lower = words[0].to_lowercase();
                if title_prefixes.contains(&first_lower.as_str()) {
                    let suffix: String = words[1..].join(" ");
                    let lower_suffix = suffix.to_lowercase();
                    if let Some(&(target_id, _target_deg)) = name_to_info.get(&lower_suffix) {
                        if target_id != e.id {
                            self.brain.merge_entities(e.id, target_id)?;
                            absorbed.insert(e.id);
                            merged += 1;
                            continue;
                        }
                    }
                }
            }

            // 2. Strip trailing noise word — always merge into canonical (shorter) form
            if words.len() >= 2 && !absorbed.contains(&e.id) {
                let last_lower = words.last().unwrap().to_lowercase();
                if trailing_noise.contains(&last_lower.as_str()) {
                    let prefix: String = words[..words.len() - 1].join(" ");
                    let lower_prefix = prefix.to_lowercase();
                    // eprintln!(
                    //     "TRAILING_STRIP: '{}' → trailing='{}' prefix='{}' lookup={:?}",
                    //     e.name,
                    //     last_lower,
                    //     lower_prefix,
                    //     name_to_info.get(&lower_prefix)
                    // );
                    if let Some(&(target_id, _target_deg)) = name_to_info.get(&lower_prefix) {
                        if target_id != e.id {
                            eprintln!(
                                "TRAILING_MERGE: merging '{}' (id={}) into target_id={}",
                                e.name, e.id, target_id
                            );
                            self.brain.merge_entities(e.id, target_id)?;
                            absorbed.insert(e.id);
                            merged += 1;
                            continue;
                        }
                    }
                }
            }

            // 3. Strip both leading AND trailing word for 4+ word entities
            if words.len() >= 4 {
                for lead in 1..=2 {
                    for trail in 1..=2 {
                        if lead + trail >= words.len() {
                            continue;
                        }
                        let core: String = words[lead..words.len() - trail].join(" ");
                        if core.split_whitespace().count() < 2 {
                            continue;
                        }
                        let lower_core = core.to_lowercase();
                        if let Some(&(target_id, target_deg)) = name_to_info.get(&lower_core) {
                            if target_id != e.id && target_deg > my_deg {
                                self.brain.merge_entities(e.id, target_id)?;
                                absorbed.insert(e.id);
                                merged += 1;
                                break;
                            }
                        }
                    }
                    if absorbed.contains(&e.id) {
                        break;
                    }
                }
            }
        }
        Ok(merged)
    }

    /// Merge abbreviation variants: "St Gotthard Pass" → "Saint Gotthard Pass", etc.
    /// Normalizes common abbreviations and merges into the canonical (longer) form.
    pub fn merge_abbreviation_variants(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build normalized_name → (id, name, degree) index
        let mut norm_to_entities: HashMap<String, Vec<(i64, String, usize)>> = HashMap::new();
        for e in &entities {
            if is_noise_type(&e.entity_type) || is_noise_name(&e.name) {
                continue;
            }
            let normalized = normalize_abbreviations(&e.name).to_lowercase();
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            norm_to_entities
                .entry(normalized)
                .or_default()
                .push((e.id, e.name.clone(), deg));
        }

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        for group in norm_to_entities.values() {
            if group.len() < 2 {
                continue;
            }
            // Find the canonical entity: prefer longer name (expanded form), then higher degree
            let canonical = group
                .iter()
                .filter(|(id, _, _)| !absorbed.contains(id))
                .max_by(|(_, name_a, deg_a), (_, name_b, deg_b)| {
                    name_a.len().cmp(&name_b.len()).then(deg_a.cmp(deg_b))
                });
            if let Some((canon_id, canon_name, _)) = canonical {
                for (eid, ename, _) in group {
                    if eid == canon_id || absorbed.contains(eid) {
                        continue;
                    }
                    // Only merge if names actually differ (not exact duplicates handled elsewhere)
                    if ename.to_lowercase() == canon_name.to_lowercase() {
                        continue;
                    }
                    eprintln!(
                        "[PROMETHEUS] Abbreviation merge: '{}' → '{}'",
                        ename, canon_name
                    );
                    self.brain.merge_entities(*eid, *canon_id)?;
                    absorbed.insert(*eid);
                    merged += 1;
                }
            }
        }

        Ok(merged)
    }

    /// Merge reversed name entities: "Neumann John" → "John von Neumann" etc.
    /// Detects 2-word person entities where swapping word order matches a connected entity.
    /// Also catches partial reversed names (e.g., "Elwood Shannon" → "Claude Elwood Shannon").
    /// Returns count of entities merged.
    pub fn merge_reversed_names(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build name → entity lookup (prefer connected, higher degree entities)
        let mut name_to_entity: HashMap<String, (i64, usize)> = HashMap::new();
        for e in &entities {
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            let key = e.name.to_lowercase();
            let entry = name_to_entity.entry(key).or_insert((e.id, deg));
            if deg > entry.1 {
                *entry = (e.id, deg);
            }
        }

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        for e in &entities {
            if absorbed.contains(&e.id) || e.entity_type != "person" {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();

            // Case 1: Two-word reversed names ("Neumann John" → "John Neumann" or "John von Neumann")
            if words.len() == 2 {
                let reversed = format!("{} {}", words[1], words[0]);
                let rev_lower = reversed.to_lowercase();

                // Check exact reversed match
                if let Some(&(target_id, target_deg)) = name_to_entity.get(&rev_lower) {
                    if target_id != e.id && !absorbed.contains(&target_id) {
                        let my_deg = degree.get(&e.id).copied().unwrap_or(0);
                        let (keep, remove) = if target_deg >= my_deg {
                            (target_id, e.id)
                        } else {
                            (e.id, target_id)
                        };
                        self.brain.merge_entities(remove, keep)?;
                        absorbed.insert(remove);
                        merged += 1;
                        continue;
                    }
                }

                // Check if reversed form is a substring of a connected entity
                // e.g., "Neumann John" → matches "John von Neumann"
                for candidate in &entities {
                    if candidate.id == e.id
                        || absorbed.contains(&candidate.id)
                        || candidate.entity_type != "person"
                    {
                        continue;
                    }
                    let cand_lower = candidate.name.to_lowercase();
                    let cand_words: Vec<&str> = cand_lower.split_whitespace().collect();
                    if cand_words.len() >= 3 {
                        // Check if both words appear in the candidate (in reversed order)
                        let w0 = words[0].to_lowercase();
                        let w1 = words[1].to_lowercase();
                        let has_both =
                            cand_words.contains(&w0.as_str()) && cand_words.contains(&w1.as_str());
                        if has_both {
                            let cand_deg = degree.get(&candidate.id).copied().unwrap_or(0);
                            let my_deg = degree.get(&e.id).copied().unwrap_or(0);
                            // Merge into the more connected entity
                            if cand_deg > my_deg
                                || (cand_deg == my_deg && candidate.name.len() > e.name.len())
                            {
                                self.brain.merge_entities(e.id, candidate.id)?;
                                absorbed.insert(e.id);
                                merged += 1;
                                break;
                            }
                        }
                    }
                }
            }

            // Case 2: Partial name fragments ("Elwood Shannon" → "Claude Elwood Shannon")
            if words.len() == 2 && !absorbed.contains(&e.id) {
                let w0 = words[0].to_lowercase();
                let w1 = words[1].to_lowercase();
                for candidate in &entities {
                    if candidate.id == e.id
                        || absorbed.contains(&candidate.id)
                        || candidate.entity_type != "person"
                    {
                        continue;
                    }
                    let cand_lower = candidate.name.to_lowercase();
                    let cand_words: Vec<String> = cand_lower
                        .split_whitespace()
                        .map(|s| s.to_string())
                        .collect();
                    if cand_words.len() == 3 {
                        // "Elwood Shannon" matches "Claude Elwood Shannon" (middle + last)
                        if cand_words[1] == w0 && cand_words[2] == w1 {
                            let cand_deg = degree.get(&candidate.id).copied().unwrap_or(0);
                            let my_deg = degree.get(&e.id).copied().unwrap_or(0);
                            if cand_deg >= my_deg {
                                self.brain.merge_entities(e.id, candidate.id)?;
                                absorbed.insert(e.id);
                                merged += 1;
                                break;
                            }
                        }
                    }
                }
            }
        }
        Ok(merged)
    }

    /// Purge entities containing "Examples" or other NLP concatenation noise.
    /// Catches patterns like "Examples Yang", "Neumann John Examples", etc.
    /// Returns count of entities purged.
    pub fn purge_concatenation_noise(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected_degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *connected_degree.entry(r.subject_id).or_insert(0) += 1;
            *connected_degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Words that signal NLP concatenation errors
        let concat_noise_words: HashSet<&str> = [
            "examples",
            "encounters",
            "quiz",
            "featured",
            "archived",
            "template",
            "categories",
            "namespace",
            "redirect",
            "disambiguation",
            "stub",
            "infobox",
            "navbox",
            "sidebar",
            "footer",
            "reflist",
            "citation",
            "webarchive",
            "coord",
            "newspapers",
            "rabbits",
            "bits",
            "imdb",
        ]
        .iter()
        .copied()
        .collect();

        let mut purged = 0usize;
        for e in &entities {
            let lower = e.name.to_lowercase();
            let words: Vec<&str> = lower.split_whitespace().collect();

            let should_purge = {
                let deg = connected_degree.get(&e.id).copied().unwrap_or(0);
                // Single word "Examples" or similar
                if words.len() == 1 && concat_noise_words.contains(words[0]) && deg <= 5 {
                    true
                }
                // Multi-word entities containing noise words
                // Higher degree threshold for very strong noise signals
                else if words.len() >= 2
                    && words.iter().any(|w| concat_noise_words.contains(w))
                    && deg <= 20
                {
                    true
                }
                // "Surname X Y" patterns: known surname followed by unrelated words
                // e.g., "Neumann John Examples", "Neumann German", "Neumann Barbara McClintock"
                // These are NLP extraction errors from citations like "von Neumann, John"
                else if words.len() >= 2 && e.entity_type == "person" && deg <= 20 {
                    // Check if the name looks like "Surname + random words" (citation fragment)
                    // Heuristic: if the entity has very few unique neighbors and most are also noise
                    let name_has_3plus_parts = words.len() >= 3;
                    // Detect citation-style names: single surname + multiple first/middle names
                    // that are actually separate people concatenated
                    let looks_like_concat = name_has_3plus_parts
                        && words.iter().all(|w| {
                            w.chars().next().map(|c| c.is_uppercase()).unwrap_or(false)
                                && w.len() >= 2
                                && w.chars().all(|c| c.is_alphabetic())
                        })
                        && words.len() >= 4; // 4+ capitalized words = likely concatenation noise
                    looks_like_concat
                } else {
                    false
                }
            };
            if should_purge {
                let eid = e.id;
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![eid])?;
                    Ok(())
                })?;
                purged += 1;
            }
        }
        Ok(purged)
    }

    /// Prune stale hypotheses: reject testing hypotheses older than `max_days`
    /// whose confidence has decayed below the rejection threshold.
    /// Returns count of hypotheses pruned.
    pub fn prune_stale_hypotheses(&self, max_days: i64) -> Result<usize> {
        let cutoff = (Utc::now() - chrono::Duration::days(max_days))
            .naive_utc()
            .format("%Y-%m-%d %H:%M:%S")
            .to_string();
        let count = self.brain.with_conn(|conn| {
            let updated = conn.execute(
                "UPDATE hypotheses SET status = 'rejected' \
                 WHERE status IN ('proposed', 'testing') \
                 AND discovered_at < ?1 \
                 AND confidence < ?2",
                params![cutoff, REJECTION_THRESHOLD],
            )?;
            Ok(updated)
        })?;
        Ok(count)
    }

    /// Cap hypothesis DB size: delete oldest rejected hypotheses to stay under `max_total`.
    /// Keeps confirmed and testing; only prunes rejected (lowest value).
    /// Returns count of hypotheses deleted.
    pub fn cap_hypothesis_count(&self, max_total: usize) -> Result<usize> {
        let total: i64 = self.brain.with_conn(|conn| {
            conn.query_row("SELECT COUNT(*) FROM hypotheses", [], |row| row.get(0))
        })?;
        if (total as usize) <= max_total {
            return Ok(0);
        }
        let excess = total as usize - max_total;
        // Delete oldest rejected first, then oldest confirmed
        let deleted = self.brain.with_conn(|conn| {
            let d1 = conn.execute(
                &format!(
                    "DELETE FROM hypotheses WHERE id IN (SELECT id FROM hypotheses WHERE status = 'rejected' ORDER BY discovered_at ASC LIMIT {})",
                    excess
                ),
                [],
            )?;
            let remaining = excess.saturating_sub(d1);
            let d2 = if remaining > 0 {
                conn.execute(
                    &format!(
                        "DELETE FROM hypotheses WHERE id IN (SELECT id FROM hypotheses WHERE status = 'confirmed' ORDER BY discovered_at ASC LIMIT {})",
                        remaining
                    ),
                    [],
                )?
            } else {
                0
            };
            Ok(d1 + d2)
        })?;
        Ok(deleted)
    }

    /// Clean up fragment hypotheses: reject confirmed/testing hypotheses where both
    /// subject and object are single-word short strings (likely NLP extraction fragments
    /// like "Grace" → "Hopper" rather than real discoveries).
    pub fn cleanup_fragment_hypotheses(&self) -> Result<usize> {
        let count = self.brain.with_conn(|conn| {
            let updated = conn.execute(
                "UPDATE hypotheses SET status = 'rejected' \
                 WHERE status IN ('confirmed', 'testing') \
                 AND subject NOT LIKE '% %' AND length(subject) <= 12 \
                 AND object NOT LIKE '% %' AND length(object) <= 12 \
                 AND object != '?'",
                [],
            )?;
            Ok(updated)
        })?;
        Ok(count)
    }

    /// Reject hypotheses where subject or object is a concatenated entity name (NLP noise).
    /// Returns count of hypotheses rejected.
    pub fn cleanup_concatenated_hypotheses(&self) -> Result<usize> {
        let hyps = self.list_hypotheses(Some(HypothesisStatus::Testing))?;
        let confirmed = self.list_hypotheses(Some(HypothesisStatus::Confirmed))?;
        let mut rejected = 0usize;
        for h in hyps.iter().chain(confirmed.iter()) {
            if is_noise_name(&h.subject) || (h.object != "?" && is_noise_name(&h.object)) {
                self.update_hypothesis_status(h.id, HypothesisStatus::Rejected)?;
                rejected += 1;
            }
        }
        Ok(rejected)
    }

    pub fn bulk_reject_stale_testing(&self, max_days: i64, max_confidence: f64) -> Result<usize> {
        let cutoff = (Utc::now() - chrono::Duration::days(max_days))
            .naive_utc()
            .format("%Y-%m-%d %H:%M:%S")
            .to_string();
        let count = self.brain.with_conn(|conn| {
            let updated = conn.execute(
                "UPDATE hypotheses SET status = 'rejected' \
                 WHERE status = 'testing' \
                 AND discovered_at < ?1 \
                 AND confidence < ?2",
                params![cutoff, max_confidence],
            )?;
            Ok(updated)
        })?;
        // Record rejections for meta-learning
        if count > 0 {
            // Batch record — approximate by pattern source
            let _ = self.record_outcome("stale_bulk_reject", false);
        }
        Ok(count)
    }

    /// Reconnect islands using reverse containment: find islands whose name appears
    /// as a significant substring within a connected entity name.
    /// E.g., island "Euler" → connected "Euler's Method" or "Leonhard Euler".
    /// Unlike consolidate_prefix_islands (which checks if island is a prefix),
    /// this checks if the island name appears anywhere as a word boundary match.
    /// Returns count of merges performed.
    pub fn reverse_containment_island_merge(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build word index: word → list of connected entity IDs containing that word
        let mut word_to_entities: HashMap<String, Vec<(i64, String, String)>> = HashMap::new();
        for e in &entities {
            if !connected.contains(&e.id) || is_noise_name(&e.name) || is_noise_type(&e.entity_type)
            {
                continue;
            }
            for word in e.name.to_lowercase().split_whitespace() {
                if word.len() >= 4 {
                    word_to_entities.entry(word.to_string()).or_default().push((
                        e.id,
                        e.name.to_lowercase(),
                        e.entity_type.clone(),
                    ));
                }
            }
        }

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        for e in &entities {
            if connected.contains(&e.id) || absorbed.contains(&e.id) {
                continue;
            }
            if is_noise_name(&e.name) || is_noise_type(&e.entity_type) {
                continue;
            }
            let lower = e.name.to_lowercase();
            let words: Vec<&str> = lower.split_whitespace().collect();

            // Only handle single-word or two-word islands
            if words.len() > 2 || words.is_empty() {
                continue;
            }

            // For single-word islands: find connected entities containing this word
            if words.len() == 1 && lower.len() >= 5 {
                if let Some(candidates) = word_to_entities.get(&lower) {
                    // Find best match: same type, shortest name (most specific)
                    let mut best: Option<(i64, usize)> = None;
                    for (cid, cname, ctype) in candidates {
                        if *cid == e.id {
                            continue;
                        }
                        // Must be multi-word (otherwise it's the same entity)
                        if !cname.contains(' ') {
                            continue;
                        }
                        let same_type = *ctype == e.entity_type;
                        let compatible = same_type
                            || matches!(
                                (e.entity_type.as_str(), ctype.as_str()),
                                ("concept", "person")
                                    | ("person", "concept")
                                    | ("concept", "place")
                                    | ("place", "concept")
                            );
                        if !compatible {
                            continue;
                        }
                        // Prefer same-type, then shorter names
                        let score =
                            if same_type { 10000 } else { 0 } + (1000 - cname.len().min(999));
                        if best.is_none() || score > best.unwrap().1 {
                            best = Some((*cid, score));
                        }
                    }
                    if let Some((target_id, _)) = best {
                        // Check the island has no facts worth preserving
                        let facts = self.brain.get_facts_for(e.id)?;
                        if facts.is_empty() {
                            self.brain.merge_entities(e.id, target_id)?;
                            absorbed.insert(e.id);
                            merged += 1;
                        }
                    }
                }
            }

            // For two-word islands: check if both words appear in a connected entity
            if words.len() == 2 && lower.len() >= 6 {
                let w0 = words[0].to_string();
                let w1 = words[1].to_string();
                if w0.len() < 4 || w1.len() < 4 {
                    continue;
                }
                let c0 = word_to_entities.get(&w0);
                let c1 = word_to_entities.get(&w1);
                if let (Some(list0), Some(list1)) = (c0, c1) {
                    // Find entities appearing in both word lists
                    let ids0: HashSet<i64> = list0.iter().map(|(id, _, _)| *id).collect();
                    for (cid, _cname, ctype) in list1 {
                        if ids0.contains(cid) && *cid != e.id {
                            let compatible = *ctype == e.entity_type
                                || matches!(
                                    (e.entity_type.as_str(), ctype.as_str()),
                                    ("concept", "person")
                                        | ("person", "concept")
                                        | ("concept", "place")
                                        | ("place", "concept")
                                );
                            if compatible {
                                let facts = self.brain.get_facts_for(e.id)?;
                                if facts.is_empty() {
                                    self.brain.merge_entities(e.id, *cid)?;
                                    absorbed.insert(e.id);
                                    merged += 1;
                                    break;
                                }
                            }
                        }
                    }
                }
            }
        }
        Ok(merged)
    }

    /// Deduplicate entities with identical lowercase names but different IDs.
    /// This catches exact duplicates that differ only in casing or whitespace.
    /// Returns count of merges performed.
    pub fn dedup_exact_name_matches(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let mut name_groups: HashMap<String, Vec<(i64, f64, usize)>> = HashMap::new();

        // Build relation degree for tie-breaking
        let relations = self.brain.all_relations()?;
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        for e in &entities {
            if is_noise_type(&e.entity_type) {
                continue;
            }
            let key = e.name.to_lowercase().trim().to_string();
            if key.len() < 2 {
                continue;
            }
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            name_groups
                .entry(key)
                .or_default()
                .push((e.id, e.confidence, deg));
        }

        let mut merged = 0usize;
        for (_name, mut group) in name_groups {
            if group.len() < 2 {
                continue;
            }
            // Sort: highest degree first, then highest confidence
            group.sort_by(|a, b| {
                b.2.cmp(&a.2)
                    .then(b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal))
            });
            let keep_id = group[0].0;
            for &(remove_id, _, _) in &group[1..] {
                self.brain.merge_entities(remove_id, keep_id)?;
                merged += 1;
            }
        }
        Ok(merged)
    }

    /// Generate hypotheses from Louvain community bridges: entities in different
    /// communities but sharing a common neighbour likely have an indirect relationship.
    /// More precise than structural_hole because it respects community boundaries.
    pub fn generate_hypotheses_from_community_bridges(&self) -> Result<Vec<Hypothesis>> {
        let communities = crate::graph::louvain_communities(self.brain)?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Build adjacency
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            if meaningful.contains(&r.subject_id) && meaningful.contains(&r.object_id) {
                adj.entry(r.subject_id).or_default().insert(r.object_id);
                adj.entry(r.object_id).or_default().insert(r.subject_id);
            }
        }

        // Find pairs in different communities that share ≥2 common neighbours
        let mut hypotheses = Vec::new();
        let mut seen: HashSet<(i64, i64)> = HashSet::new();
        let ids: Vec<i64> = adj.keys().copied().collect();

        for &node in &ids {
            let _node_comm = communities.get(&node).copied().unwrap_or(usize::MAX);
            let neighbours = match adj.get(&node) {
                Some(n) => n,
                None => continue,
            };
            // For each pair of this node's neighbours in different communities
            let nb_list: Vec<i64> = neighbours.iter().copied().collect();
            for i in 0..nb_list.len() {
                for j in (i + 1)..nb_list.len() {
                    let a = nb_list[i];
                    let b = nb_list[j];
                    let ca = communities.get(&a).copied().unwrap_or(usize::MAX);
                    let cb = communities.get(&b).copied().unwrap_or(usize::MAX);
                    if ca == cb {
                        continue; // same community, already well-connected
                    }
                    // Check they're not directly connected
                    if adj.get(&a).is_some_and(|s| s.contains(&b)) {
                        continue;
                    }
                    let key = if a < b { (a, b) } else { (b, a) };
                    if !seen.insert(key) {
                        continue;
                    }
                    let a_name = self.entity_name(a)?;
                    let b_name = self.entity_name(b)?;
                    let bridge_name = self.entity_name(node)?;

                    // Count shared neighbours for confidence scaling
                    let a_nb = adj.get(&a).cloned().unwrap_or_default();
                    let b_nb = adj.get(&b).cloned().unwrap_or_default();
                    let shared_count = a_nb.intersection(&b_nb).count();
                    // Base 0.40, +0.05 per shared neighbour (cap at 0.60)
                    let base_conf = (0.40 + 0.05 * shared_count as f64).min(0.60);

                    hypotheses.push(Hypothesis {
                        id: 0,
                        subject: a_name.clone(),
                        predicate: "related_to".to_string(),
                        object: b_name.clone(),
                        confidence: base_conf,
                        evidence_for: vec![format!(
                            "Both connected to bridge entity '{}' but in different communities ({}≠{}), {} shared neighbours",
                            bridge_name, ca, cb, shared_count
                        )],
                        evidence_against: vec![],
                        reasoning_chain: vec![
                            format!("{} and {} share neighbour {}", a_name, b_name, bridge_name),
                            format!("They belong to different communities ({} vs {})", ca, cb),
                            "Cross-community bridge pattern suggests potential relation".to_string(),
                        ],
                        status: HypothesisStatus::Proposed,
                        discovered_at: now_str(),
                        pattern_source: "community_bridge".to_string(),
                    });
                    if hypotheses.len() >= 50 {
                        return Ok(hypotheses);
                    }
                }
            }
        }
        Ok(hypotheses)
    }

    /// Community homophily: same-type entities within the same Louvain community
    /// that are not directly connected. Entities clustered together by community
    /// detection AND sharing the same type are very likely related — this exploits
    /// the strong signal that community_bridge already demonstrates (96% weight).
    pub fn generate_hypotheses_from_community_homophily(&self) -> Result<Vec<Hypothesis>> {
        let communities = crate::graph::louvain_communities(self.brain)?;
        let meaningful = meaningful_ids(self.brain)?;
        let relations = self.brain.all_relations()?;

        // Build adjacency for meaningful entities
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            if meaningful.contains(&r.subject_id) && meaningful.contains(&r.object_id) {
                adj.entry(r.subject_id).or_default().insert(r.object_id);
                adj.entry(r.object_id).or_default().insert(r.subject_id);
            }
        }

        // Group community members by (community, entity_type)
        let mut comm_type_groups: HashMap<(usize, String), Vec<i64>> = HashMap::new();
        for (&eid, &comm) in &communities {
            if !meaningful.contains(&eid) {
                continue;
            }
            // Only consider entities with at least 1 relation (skip total isolates)
            if !adj.contains_key(&eid) {
                continue;
            }
            if let Ok(Some(ent)) = self.brain.get_entity_by_id(eid) {
                if HIGH_VALUE_TYPES.contains(&ent.entity_type.as_str()) {
                    comm_type_groups
                        .entry((comm, ent.entity_type))
                        .or_default()
                        .push(eid);
                }
            }
        }

        let mut hypotheses = Vec::new();
        let mut seen: HashSet<(i64, i64)> = HashSet::new();

        for ((_comm, etype), members) in &comm_type_groups {
            // Only groups with 2-20 members (avoid huge/trivial groups)
            if members.len() < 2 || members.len() > 20 {
                continue;
            }
            for i in 0..members.len() {
                for j in (i + 1)..members.len() {
                    let a = members[i];
                    let b = members[j];
                    // Skip if already directly connected
                    if adj.get(&a).is_some_and(|s| s.contains(&b)) {
                        continue;
                    }
                    let key = if a < b { (a, b) } else { (b, a) };
                    if !seen.insert(key) {
                        continue;
                    }
                    // Require at least 2 shared neighbors (structural evidence)
                    // Raised from 1 to reduce false positives (2.5% → target 15%+)
                    let a_nb = adj.get(&a).cloned().unwrap_or_default();
                    let b_nb = adj.get(&b).cloned().unwrap_or_default();
                    let shared_nbrs: Vec<i64> = a_nb.intersection(&b_nb).copied().collect();
                    if shared_nbrs.len() < 2 {
                        continue;
                    }

                    // Infer a specific predicate from the edges to shared neighbors
                    // instead of always using the vague "related_to"
                    let mut pred_votes: HashMap<String, usize> = HashMap::new();
                    for &nbr in &shared_nbrs {
                        for r in &relations {
                            if (r.subject_id == a && r.object_id == nbr)
                                || (r.subject_id == nbr && r.object_id == a)
                                || (r.subject_id == b && r.object_id == nbr)
                                || (r.subject_id == nbr && r.object_id == b)
                            {
                                if !GENERIC_PREDICATES.contains(&r.predicate.as_str())
                                    && r.predicate != "related_to"
                                    && r.predicate != "associated_with"
                                {
                                    *pred_votes.entry(r.predicate.clone()).or_insert(0) += 1;
                                }
                            }
                        }
                    }
                    let inferred_pred = pred_votes
                        .into_iter()
                        .max_by_key(|(_, c)| *c)
                        .map(|(p, _)| p)
                        .unwrap_or_else(|| "collaborated_with".to_string());

                    let a_name = self.entity_name(a)?;
                    let b_name = self.entity_name(b)?;
                    let shared = shared_nbrs.len();
                    let conf = (0.55 + 0.05 * shared.min(6) as f64).min(0.85);

                    hypotheses.push(Hypothesis {
                        id: 0,
                        subject: a_name.clone(),
                        predicate: inferred_pred,
                        object: b_name.clone(),
                        confidence: conf,
                        evidence_for: vec![format!(
                            "Same community, same type ({}), {} shared neighbors",
                            etype, shared
                        )],
                        evidence_against: vec![],
                        reasoning_chain: vec![
                            format!("{} and {} are both {} entities", a_name, b_name, etype),
                            format!("Louvain grouped them in the same community"),
                            format!("{} shared neighbors provide structural evidence", shared),
                        ],
                        status: HypothesisStatus::Proposed,
                        discovered_at: now_str(),
                        pattern_source: "community_homophily".to_string(),
                    });
                    if hypotheses.len() >= 50 {
                        return Ok(hypotheses);
                    }
                }
            }
        }
        Ok(hypotheses)
    }

    /// Compute hypothesis quality metrics: distribution of confidence scores,
    /// pattern source effectiveness, and staleness analysis.
    pub fn hypothesis_quality_report(&self) -> Result<HashMap<String, f64>> {
        let all = self.list_hypotheses(None)?;
        let mut metrics: HashMap<String, f64> = HashMap::new();
        let total = all.len() as f64;
        if total == 0.0 {
            return Ok(metrics);
        }

        // Status distribution
        let mut status_counts: HashMap<String, usize> = HashMap::new();
        let mut source_counts: HashMap<String, (usize, f64)> = HashMap::new();
        let mut conf_sum = 0.0_f64;

        for h in &all {
            *status_counts
                .entry(h.status.as_str().to_string())
                .or_insert(0) += 1;
            let entry = source_counts
                .entry(h.pattern_source.clone())
                .or_insert((0, 0.0));
            entry.0 += 1;
            entry.1 += h.confidence;
            conf_sum += h.confidence;
        }

        metrics.insert("total_hypotheses".into(), total);
        metrics.insert("avg_confidence".into(), conf_sum / total);

        for (status, count) in &status_counts {
            metrics.insert(format!("status_{}", status), *count as f64);
        }

        // Average confidence per source
        for (source, (count, sum)) in &source_counts {
            metrics.insert(format!("source_{}_count", source), *count as f64);
            metrics.insert(format!("source_{}_avg_conf", source), sum / *count as f64);
        }

        Ok(metrics)
    }

    fn entity_name(&self, id: i64) -> Result<String> {
        Ok(self
            .brain
            .get_entity_by_id(id)?
            .map(|e| e.name)
            .unwrap_or_else(|| format!("#{}", id)))
    }

    // -----------------------------------------------------------------------
    // Predicate Specificity Analysis
    // -----------------------------------------------------------------------

    /// Analyze predicate quality: identify over-reliance on vague predicates
    /// and suggest more specific alternatives based on entity types and context.
    /// Returns (predicate, count, specificity_score, suggestion).
    pub fn predicate_specificity_analysis(&self) -> Result<Vec<(String, usize, f64, String)>> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Count predicates and their subject/object type pairs
        let mut pred_count: HashMap<String, usize> = HashMap::new();
        let mut pred_type_pairs: HashMap<String, HashMap<(String, String), usize>> = HashMap::new();

        for r in &relations {
            *pred_count.entry(r.predicate.clone()).or_insert(0) += 1;
            let stype = id_to_type
                .get(&r.subject_id)
                .copied()
                .unwrap_or("unknown")
                .to_string();
            let otype = id_to_type
                .get(&r.object_id)
                .copied()
                .unwrap_or("unknown")
                .to_string();
            *pred_type_pairs
                .entry(r.predicate.clone())
                .or_default()
                .entry((stype, otype))
                .or_insert(0) += 1;
        }

        let total = relations.len();
        let mut results = Vec::new();

        // Vague predicates that could be more specific
        let vague_predicates: HashMap<&str, &[(&str, &str, &str)]> = HashMap::from([
            (
                "associated_with",
                &[
                    ("person", "place", "born_in / lived_in / worked_in"),
                    ("person", "organization", "member_of / founded / worked_at"),
                    (
                        "person",
                        "person",
                        "collaborated_with / influenced / mentored",
                    ),
                    ("person", "concept", "developed / studied / contributed_to"),
                    ("organization", "place", "headquartered_in / operates_in"),
                    (
                        "concept",
                        "concept",
                        "related_to / subclass_of / derived_from",
                    ),
                    ("technology", "person", "invented_by / developed_by"),
                    ("technology", "concept", "implements / based_on"),
                ][..],
            ),
            (
                "related_to",
                &[
                    ("person", "person", "collaborated_with / influenced"),
                    ("concept", "concept", "subclass_of / part_of / derived_from"),
                    ("place", "place", "borders / located_in / part_of"),
                ][..],
            ),
        ]);

        for (pred, count) in &pred_count {
            let dominance = *count as f64 / total.max(1) as f64;
            let specificity = if vague_predicates.contains_key(pred.as_str()) {
                // Vague predicate — lower specificity
                (1.0 - dominance).max(0.0)
            } else if is_generic_predicate(pred) {
                0.1
            } else {
                // Specific predicate — high specificity
                0.8 + (0.2 * (1.0 - dominance))
            };

            let suggestion = if let Some(suggestions) = vague_predicates.get(pred.as_str()) {
                // Find most common type pairs for this predicate
                if let Some(type_pairs) = pred_type_pairs.get(pred) {
                    let mut pairs: Vec<_> = type_pairs.iter().collect();
                    pairs.sort_by(|a, b| b.1.cmp(a.1));
                    let top_pair = &pairs[0].0;
                    // Find matching suggestion
                    suggestions
                        .iter()
                        .find(|(st, ot, _)| *st == top_pair.0 && *ot == top_pair.1)
                        .map(|(st, ot, sugg)| {
                            format!(
                                "For {}->{}: consider {} (affects {} rels)",
                                st, ot, sugg, pairs[0].1
                            )
                        })
                        .unwrap_or_else(|| {
                            format!(
                                "Top usage: {}->{} ({} times) — needs domain-specific predicate",
                                top_pair.0, top_pair.1, pairs[0].1
                            )
                        })
                } else {
                    "No type pair data".to_string()
                }
            } else {
                "OK — specific predicate".to_string()
            };

            results.push((pred.clone(), *count, specificity, suggestion));
        }

        results.sort_by(|a, b| a.2.partial_cmp(&b.2).unwrap_or(std::cmp::Ordering::Equal));
        Ok(results)
    }

    /// Compute a graph-wide predicate quality score (0-1).
    /// 0 = all vague predicates, 1 = all specific predicates.
    pub fn predicate_quality_score(&self) -> Result<f64> {
        let analysis = self.predicate_specificity_analysis()?;
        if analysis.is_empty() {
            return Ok(0.0);
        }
        let total_weighted: f64 = analysis.iter().map(|(_, c, s, _)| *c as f64 * s).sum();
        let total_count: f64 = analysis.iter().map(|(_, c, _, _)| *c as f64).sum();
        Ok(if total_count > 0.0 {
            total_weighted / total_count
        } else {
            0.0
        })
    }

    /// Suggest predicate refinements for "associated_with" relations based on
    /// the entity types of subject and object.
    /// Returns Vec of (relation_subject, relation_object, current_pred, suggested_pred, confidence).
    pub fn suggest_predicate_refinements(
        &self,
        limit: usize,
    ) -> Result<Vec<(String, String, String, String, f64)>> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        // Type-pair → suggested predicate mapping
        let refinement_map: HashMap<(&str, &str), (&str, f64)> = HashMap::from([
            (("person", "place"), ("born_in_or_lived_in", 0.5)),
            (("person", "organization"), ("affiliated_with", 0.6)),
            (("person", "person"), ("collaborated_with", 0.4)),
            (("person", "concept"), ("contributed_to", 0.5)),
            (("person", "technology"), ("developed", 0.5)),
            (("organization", "place"), ("headquartered_in", 0.5)),
            (("organization", "person"), ("employs_or_founded_by", 0.4)),
            (("technology", "person"), ("invented_by", 0.6)),
            (("technology", "concept"), ("implements", 0.5)),
            (("concept", "person"), ("developed_by", 0.5)),
            (("place", "place"), ("located_in", 0.6)),
            (("event", "place"), ("occurred_in", 0.7)),
            (("event", "person"), ("involved", 0.5)),
        ]);

        let mut suggestions = Vec::new();
        for r in &relations {
            if r.predicate != "associated_with" && r.predicate != "related_to" {
                continue;
            }
            let stype = id_to_entity
                .get(&r.subject_id)
                .map(|e| e.entity_type.as_str())
                .unwrap_or("unknown");
            let otype = id_to_entity
                .get(&r.object_id)
                .map(|e| e.entity_type.as_str())
                .unwrap_or("unknown");

            if let Some(&(suggested, confidence)) = refinement_map.get(&(stype, otype)) {
                let sname = id_to_entity
                    .get(&r.subject_id)
                    .map(|e| e.name.as_str())
                    .unwrap_or("?");
                let oname = id_to_entity
                    .get(&r.object_id)
                    .map(|e| e.name.as_str())
                    .unwrap_or("?");
                suggestions.push((
                    sname.to_string(),
                    oname.to_string(),
                    r.predicate.clone(),
                    suggested.to_string(),
                    confidence,
                ));
            }
            if suggestions.len() >= limit {
                break;
            }
        }
        Ok(suggestions)
    }

    // -----------------------------------------------------------------------
    // Entity Consolidation Scoring
    // -----------------------------------------------------------------------

    /// Score entity pairs for consolidation potential using multiple signals:
    /// name similarity, type match, shared neighbours, co-source frequency.
    /// Returns sorted list of (entity_a, entity_b, consolidation_score, reason).
    pub fn entity_consolidation_candidates(
        &self,
        limit: usize,
    ) -> Result<Vec<(String, String, f64, String)>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Build connected set and neighbour map
        let mut connected: HashSet<i64> = HashSet::new();
        let mut neighbours: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
            neighbours
                .entry(r.subject_id)
                .or_default()
                .insert(r.object_id);
            neighbours
                .entry(r.object_id)
                .or_default()
                .insert(r.subject_id);
        }

        // Source co-occurrence map
        let mut source_entities: HashMap<String, HashSet<i64>> = HashMap::new();
        for r in &relations {
            if !r.source_url.is_empty() {
                source_entities
                    .entry(r.source_url.clone())
                    .or_default()
                    .insert(r.subject_id);
                source_entities
                    .entry(r.source_url.clone())
                    .or_default()
                    .insert(r.object_id);
            }
        }

        // Focus on meaningful entities, preferring connected ones with island candidates
        let meaningful_entities: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| meaningful.contains(&e.id) && !is_noise_name(&e.name))
            .collect();

        // Index by first significant word for blocking (avoid O(n²))
        let mut word_index: HashMap<String, Vec<usize>> = HashMap::new();
        for (idx, e) in meaningful_entities.iter().enumerate() {
            for word in e.name.to_lowercase().split_whitespace() {
                if word.len() >= 4 {
                    word_index.entry(word.to_string()).or_default().push(idx);
                }
            }
        }

        let mut candidates: Vec<(String, String, f64, String)> = Vec::new();
        let mut seen_pairs: HashSet<(usize, usize)> = HashSet::new();

        for indices in word_index.values() {
            if indices.len() > 200 {
                continue; // Skip extremely common words
            }
            for i in 0..indices.len() {
                for j in (i + 1)..indices.len() {
                    let idx_a = indices[i];
                    let idx_b = indices[j];
                    let pair = if idx_a < idx_b {
                        (idx_a, idx_b)
                    } else {
                        (idx_b, idx_a)
                    };
                    if !seen_pairs.insert(pair) {
                        continue;
                    }

                    let ea = meaningful_entities[idx_a];
                    let eb = meaningful_entities[idx_b];

                    // Must be same type
                    if ea.entity_type != eb.entity_type {
                        continue;
                    }

                    let mut score = 0.0_f64;
                    let mut reasons = Vec::new();

                    // 1. Name word overlap (Jaccard)
                    let words_a: HashSet<String> = ea
                        .name
                        .to_lowercase()
                        .split_whitespace()
                        .filter(|w| w.len() >= 3)
                        .map(|s| s.to_string())
                        .collect();
                    let words_b: HashSet<String> = eb
                        .name
                        .to_lowercase()
                        .split_whitespace()
                        .filter(|w| w.len() >= 3)
                        .map(|s| s.to_string())
                        .collect();
                    let intersection = words_a.intersection(&words_b).count();
                    let union = words_a.union(&words_b).count();
                    let jaccard = if union > 0 {
                        intersection as f64 / union as f64
                    } else {
                        0.0
                    };

                    if jaccard < 0.3 {
                        continue; // Not similar enough
                    }
                    score += jaccard * 0.4;
                    reasons.push(format!("name overlap {:.0}%", jaccard * 100.0));

                    // 2. Containment (one name is substring of the other)
                    let a_lower = ea.name.to_lowercase();
                    let b_lower = eb.name.to_lowercase();
                    if a_lower.contains(&b_lower) || b_lower.contains(&a_lower) {
                        score += 0.2;
                        reasons.push("name containment".to_string());
                    }

                    // 3. Shared neighbours
                    let na = neighbours.get(&ea.id).cloned().unwrap_or_default();
                    let nb = neighbours.get(&eb.id).cloned().unwrap_or_default();
                    let shared_nb = na.intersection(&nb).count();
                    if shared_nb > 0 {
                        score += (shared_nb as f64 * 0.1).min(0.2);
                        reasons.push(format!("{} shared neighbours", shared_nb));
                    }

                    // 4. Island bonus (one or both disconnected)
                    if !connected.contains(&ea.id) || !connected.contains(&eb.id) {
                        score += 0.1;
                        reasons.push("island entity".to_string());
                    }

                    // 5. Co-source frequency
                    let mut co_sources = 0usize;
                    for src_entities in source_entities.values() {
                        if src_entities.contains(&ea.id) && src_entities.contains(&eb.id) {
                            co_sources += 1;
                        }
                    }
                    if co_sources > 0 {
                        score += (co_sources as f64 * 0.05).min(0.15);
                        reasons.push(format!("{} co-sources", co_sources));
                    }

                    if score >= 0.4 {
                        candidates.push((
                            ea.name.clone(),
                            eb.name.clone(),
                            score,
                            reasons.join(", "),
                        ));
                    }
                }
            }
        }

        candidates.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap_or(std::cmp::Ordering::Equal));
        candidates.truncate(limit);
        Ok(candidates)
    }

    /// Find entities that are "almost connected" — they share 2+ hop paths through
    /// the graph but no direct edge. These are high-value hypothesis targets because
    /// a direct relation likely exists but wasn't extracted.
    /// Returns (entity_a, entity_b, path_count, shortest_path_len, suggested_predicate).
    pub fn find_near_miss_connections(
        &self,
        limit: usize,
    ) -> Result<Vec<(String, String, usize, usize, String)>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Build adjacency for meaningful entities only
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        let mut direct: HashSet<(i64, i64)> = HashSet::new();
        let mut pred_map: HashMap<(i64, i64), String> = HashMap::new();
        for r in &relations {
            if meaningful.contains(&r.subject_id) && meaningful.contains(&r.object_id) {
                adj.entry(r.subject_id).or_default().insert(r.object_id);
                adj.entry(r.object_id).or_default().insert(r.subject_id);
                let key = if r.subject_id < r.object_id {
                    (r.subject_id, r.object_id)
                } else {
                    (r.object_id, r.subject_id)
                };
                direct.insert(key);
                pred_map.insert(key, r.predicate.clone());
            }
        }

        // For each entity, do a 2-hop BFS to find entities reachable in exactly 2 hops
        // that share multiple 2-hop paths (strong indirect connection signal)
        let mut near_misses: Vec<(String, String, usize, usize, String)> = Vec::new();
        let entities: Vec<i64> = adj.keys().copied().collect();

        // Sample to avoid O(n²) on large graphs
        let sample_size = entities.len().min(200);
        let step = if entities.len() <= sample_size {
            1
        } else {
            entities.len() / sample_size
        };

        for &src in entities.iter().step_by(step).take(sample_size) {
            let src_nb = match adj.get(&src) {
                Some(s) => s,
                None => continue,
            };
            // Collect 2-hop targets with path counts
            let mut two_hop_count: HashMap<i64, usize> = HashMap::new();
            for &mid in src_nb {
                if let Some(mid_nb) = adj.get(&mid) {
                    for &target in mid_nb {
                        if target != src && !src_nb.contains(&target) {
                            let key = if src < target {
                                (src, target)
                            } else {
                                (target, src)
                            };
                            if !direct.contains(&key) {
                                *two_hop_count.entry(target).or_insert(0) += 1;
                            }
                        }
                    }
                }
            }

            // Entities reachable via 2+ distinct 2-hop paths are candidates
            for (&target, &count) in &two_hop_count {
                if count >= 2 {
                    let src_name = self.entity_name(src)?;
                    let tgt_name = self.entity_name(target)?;
                    if is_noise_name(&src_name) || is_noise_name(&tgt_name) {
                        continue;
                    }
                    // Suggest predicate from most common predicate between src's neighbors and target
                    let mut pred_freq: HashMap<String, usize> = HashMap::new();
                    for r in &relations {
                        if (r.subject_id == src || r.object_id == src)
                            && !is_generic_predicate(&r.predicate)
                        {
                            *pred_freq.entry(r.predicate.clone()).or_insert(0) += 1;
                        }
                    }
                    let suggested = pred_freq
                        .into_iter()
                        .max_by_key(|(_, c)| *c)
                        .map(|(p, _)| p)
                        .unwrap_or_else(|| "related_to".to_string());
                    near_misses.push((src_name, tgt_name, count, 2, suggested));
                }
            }
        }

        near_misses.sort_by(|a, b| b.2.cmp(&a.2));
        near_misses.truncate(limit);
        Ok(near_misses)
    }

    /// Generate hypotheses from near-miss connections (entities with multiple
    /// indirect paths but no direct edge).
    /// Generate hypotheses from Adamic-Adar link prediction.
    /// Uses network topology to predict the most likely missing links.
    pub fn generate_hypotheses_from_adamic_adar(&self) -> Result<Vec<Hypothesis>> {
        let predictions = crate::graph::adamic_adar_predict(self.brain, 30)?;
        let mut hypotheses = Vec::new();
        for (a_id, b_id, score) in &predictions {
            let a_name = self.entity_name(*a_id)?;
            let b_name = self.entity_name(*b_id)?;
            if is_noise_name(&a_name) || is_noise_name(&b_name) {
                continue;
            }
            let confidence = (0.4 + score * 0.1).min(0.85);
            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.clone(),
                predicate: "related_to".to_string(),
                object: b_name.clone(),
                confidence,
                evidence_for: vec![format!(
                    "Adamic-Adar score {:.3}: shared neighbors weighted by inverse log-degree",
                    score
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} and {} share multiple well-connected neighbors",
                        a_name, b_name
                    ),
                    format!("Adamic-Adar link prediction score: {:.3}", score),
                    "High AA score strongly predicts missing links in real networks".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "adamic_adar".to_string(),
            });
        }
        Ok(hypotheses)
    }

    /// Generate hypotheses using Resource Allocation Index — better for sparse graphs.
    /// RA uses 1/degree instead of 1/ln(degree), giving more weight to exclusive shared neighbors.
    pub fn generate_hypotheses_from_resource_allocation(&self) -> Result<Vec<Hypothesis>> {
        let predictions = crate::graph::resource_allocation_predict(self.brain, 30)?;
        let mut hypotheses = Vec::new();
        for (a_id, b_id, score) in &predictions {
            let a_name = self.entity_name(*a_id)?;
            let b_name = self.entity_name(*b_id)?;
            if is_noise_name(&a_name) || is_noise_name(&b_name) {
                continue;
            }
            let confidence = (0.4 + score * 0.15).min(0.85);
            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.clone(),
                predicate: "related_to".to_string(),
                object: b_name.clone(),
                confidence,
                evidence_for: vec![format!(
                    "Resource Allocation score {:.3}: shared neighbors weighted by inverse degree",
                    score
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} and {} share neighbors with low degree (exclusive connections)",
                        a_name, b_name
                    ),
                    format!("RA link prediction score: {:.3}", score),
                    "RA outperforms Adamic-Adar in sparse knowledge graphs".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "resource_allocation".to_string(),
            });
        }
        Ok(hypotheses)
    }

    /// Generate hypotheses using type-aware link prediction — boosts same-type entity pairs.
    pub fn generate_hypotheses_from_type_affinity(&self) -> Result<Vec<Hypothesis>> {
        let predictions = crate::graph::type_aware_link_predict(self.brain, 30)?;
        let mut hypotheses = Vec::new();
        for (a_id, b_id, score) in &predictions {
            let a_name = self.entity_name(*a_id)?;
            let b_name = self.entity_name(*b_id)?;
            if is_noise_name(&a_name) || is_noise_name(&b_name) {
                continue;
            }
            let a_type = self
                .brain
                .get_entity_by_id(*a_id)?
                .map(|e| e.entity_type)
                .unwrap_or_default();
            let b_type = self
                .brain
                .get_entity_by_id(*b_id)?
                .map(|e| e.entity_type)
                .unwrap_or_default();
            let type_info = if a_type == b_type {
                format!("both {} type", a_type)
            } else {
                format!("{} + {} types", a_type, b_type)
            };
            let confidence = (0.4 + score * 0.08).min(0.85);
            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.clone(),
                predicate: "related_to".to_string(),
                object: b_name.clone(),
                confidence,
                evidence_for: vec![format!(
                    "Type-aware CN score {:.3} ({}) — same-type entities sharing neighbors",
                    score, type_info
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!("{} and {} share common neighbors", a_name, b_name),
                    format!("Type affinity: {}", type_info),
                    "Same-type entities with shared neighbors are strongly correlated".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "type_affinity".to_string(),
            });
        }
        Ok(hypotheses)
    }

    /// Generate hypotheses using neighborhood overlap coefficient.
    /// Finds unconnected node pairs with high overlap in their neighborhoods.
    /// Overlap = |N(a) ∩ N(b)| / min(|N(a)|, |N(b)|) — robust to degree imbalance.
    pub fn generate_hypotheses_from_neighborhood_overlap(&self) -> Result<Vec<Hypothesis>> {
        let overlaps = crate::graph::neighborhood_overlap(self.brain, 0.3, 30)?;
        let mut hypotheses = Vec::new();
        for (a_id, b_id, score) in &overlaps {
            let a_name = self.entity_name(*a_id)?;
            let b_name = self.entity_name(*b_id)?;
            if is_noise_name(&a_name) || is_noise_name(&b_name) {
                continue;
            }
            let confidence = (0.45 + score * 0.4).min(0.90);
            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.clone(),
                predicate: "related_to".to_string(),
                object: b_name.clone(),
                confidence,
                evidence_for: vec![format!(
                    "Neighborhood overlap coefficient {:.3} — strong structural similarity",
                    score
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} and {} share a large fraction of their neighbors",
                        a_name, b_name
                    ),
                    format!("Overlap score: {:.2} (≥0.3 threshold)", score),
                    "High neighborhood overlap strongly predicts missing links".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "neighborhood_overlap".to_string(),
            });
        }
        Ok(hypotheses)
    }

    pub fn generate_hypotheses_from_near_misses(&self) -> Result<Vec<Hypothesis>> {
        let near_misses = self.find_near_miss_connections(50)?;
        let entities = self.brain.all_entities()?;
        let _id_type: HashMap<i64, String> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.clone()))
            .collect();
        let name_type: HashMap<String, String> = entities
            .iter()
            .map(|e| (e.name.to_lowercase(), e.entity_type.clone()))
            .collect();

        let mut hypotheses = Vec::new();
        for (a, b, path_count, _path_len, pred) in &near_misses {
            // Type compatibility check — skip incompatible pairs
            let a_type = name_type
                .get(&a.to_lowercase())
                .map(|s| s.as_str())
                .unwrap_or("unknown");
            let b_type = name_type
                .get(&b.to_lowercase())
                .map(|s| s.as_str())
                .unwrap_or("unknown");
            if !types_compatible(a_type, b_type) {
                continue;
            }

            // Require 3+ paths for cross-type pairs (higher bar for heterogeneous connections)
            if a_type != b_type && *path_count < 3 {
                continue;
            }

            // Use type-aware predicate inference when the suggested predicate is generic
            let predicate = if pred == "related_to" {
                infer_predicate(a_type, b_type, None).to_string()
            } else {
                pred.clone()
            };

            // Skip low-signal predicate/type combos that produce noise
            if predicate == "contemporary_of"
                || predicate == "references"
                || predicate == "related_to"
            {
                continue;
            }
            if predicate == "pioneered" && (a_type == "place" || b_type == "place") {
                continue;
            }
            // related_concept at 50% confirmation — require more evidence
            if predicate == "related_concept" && *path_count < 4 {
                continue;
            }
            // Skip single-word entities in near_miss (too ambiguous)
            if !a.contains(' ') && a.len() < 8 {
                continue;
            }
            if !b.contains(' ') && b.len() < 8 {
                continue;
            }

            // Scale confidence with path count, with a type-match bonus
            let type_bonus = if a_type == b_type { 0.05 } else { 0.0 };
            let base_conf = 0.45 + (*path_count as f64 * 0.05).min(0.3) + type_bonus;
            let confidence = self
                .calibrated_confidence("near_miss", base_conf)
                .unwrap_or(base_conf);

            hypotheses.push(Hypothesis {
                id: 0,
                subject: a.clone(),
                predicate,
                object: b.clone(),
                confidence,
                evidence_for: vec![format!(
                    "{} and {} connected via {} distinct 2-hop paths (types: {}↔{})",
                    a, b, path_count, a_type, b_type
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!("{} indirect paths between {} and {}", path_count, a, b),
                    format!("Type compatible: {} ↔ {}", a_type, b_type),
                    "Multiple indirect connections suggest a missing direct relation".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "near_miss".to_string(),
            });
        }
        hypotheses.truncate(30);
        Ok(hypotheses)
    }

    /// Triadic closure hypothesis generation: if A→B and B→C but not A→C,
    /// and A and C are of compatible types, hypothesize A→C.
    /// Weighted by number of distinct intermediaries (more paths = higher confidence).
    /// Generate hypotheses from semantic fingerprint similarity.
    /// Entities sharing the same (predicate, object) patterns are likely related
    /// even if not directly connected.
    pub fn generate_hypotheses_from_semantic_similarity(&self) -> Result<Vec<Hypothesis>> {
        // Two passes: strict (min_shared=2, top 80) and relaxed for high-value types (min_shared=1, top 40)
        let mut similar = crate::graph::semantic_fingerprint_similarity(self.brain, 2, 80)?;
        let relaxed = crate::graph::semantic_fingerprint_similarity(self.brain, 1, 40)?;
        // Merge relaxed results that aren't already present
        let existing: HashSet<(i64, i64)> = similar.iter().map(|(a, b, _, _)| (*a, *b)).collect();
        for item in relaxed {
            if !existing.contains(&(item.0, item.1)) && !existing.contains(&(item.1, item.0)) {
                similar.push(item);
            }
        }
        let meaningful = meaningful_ids(self.brain)?;
        let entities = self.brain.all_entities()?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Check direct connections
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            connected.insert(key);
        }

        let mut hypotheses = Vec::new();
        for (a, b, shared, jaccard) in &similar {
            if !meaningful.contains(a) || !meaningful.contains(b) {
                continue;
            }
            let key = if a < b { (*a, *b) } else { (*b, *a) };
            if connected.contains(&key) {
                continue;
            }
            let a_name = id_name.get(a).copied().unwrap_or("?");
            let b_name = id_name.get(b).copied().unwrap_or("?");
            if is_noise_name(a_name) || is_noise_name(b_name) {
                continue;
            }
            let a_type = id_type.get(a).copied().unwrap_or("?");
            let b_type = id_type.get(b).copied().unwrap_or("?");
            let predicate = infer_predicate(a_type, b_type, None);
            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.to_string(),
                predicate: predicate.to_string(),
                object: b_name.to_string(),
                confidence: 0.40 + (*jaccard * 0.4).min(0.4),
                evidence_for: vec![format!(
                    "Semantic fingerprint similarity: {} shared (pred,obj) patterns, Jaccard {:.2}",
                    shared, jaccard
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} and {} share {} predicate-object patterns",
                        a_name, b_name, shared
                    ),
                    format!("Jaccard similarity: {:.2}", jaccard),
                    "Entities with similar relational patterns are likely related".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "semantic_fingerprint".to_string(),
            });
        }
        Ok(hypotheses)
    }

    /// Generate hypotheses from type-level semantic fingerprint similarity.
    /// Entities sharing the same (predicate, object_type) patterns are likely
    /// related even if they connect to different specific entities.
    pub fn generate_hypotheses_from_type_level_fingerprint(&self) -> Result<Vec<Hypothesis>> {
        let similar = crate::graph::type_level_fingerprint_similarity(self.brain, 3, 100)?;
        let meaningful = meaningful_ids(self.brain)?;
        let entities = self.brain.all_entities()?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            connected.insert(key);
        }

        let mut hypotheses = Vec::new();
        for (a, b, shared, jaccard) in &similar {
            if !meaningful.contains(a) || !meaningful.contains(b) {
                continue;
            }
            let key = if a < b { (*a, *b) } else { (*b, *a) };
            if connected.contains(&key) {
                continue;
            }
            let a_name = id_name.get(a).copied().unwrap_or("?");
            let b_name = id_name.get(b).copied().unwrap_or("?");
            if is_noise_name(a_name) || is_noise_name(b_name) {
                continue;
            }
            let a_type = id_type.get(a).copied().unwrap_or("?");
            let b_type = id_type.get(b).copied().unwrap_or("?");
            let predicate = infer_predicate(a_type, b_type, None);
            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.to_string(),
                predicate: predicate.to_string(),
                object: b_name.to_string(),
                confidence: 0.35 + (*jaccard * 0.35).min(0.35),
                evidence_for: vec![format!(
                    "Type-level fingerprint: {} shared (pred,obj_type) patterns, Jaccard {:.2}",
                    shared, jaccard
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} and {} share {} (predicate, object_type) patterns",
                        a_name, b_name, shared
                    ),
                    format!("Type-level Jaccard similarity: {:.2}", jaccard),
                    "Entities with parallel relational structures are likely related".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "type_level_fingerprint".to_string(),
            });
        }
        Ok(hypotheses)
    }

    pub fn generate_hypotheses_from_triadic_closure(&self) -> Result<Vec<Hypothesis>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;
        let entities = self.brain.all_entities()?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Build adjacency with predicate info
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        let mut edge_preds: HashMap<(i64, i64), String> = HashMap::new();
        for r in &relations {
            if !meaningful.contains(&r.subject_id) || !meaningful.contains(&r.object_id) {
                continue;
            }
            adj.entry(r.subject_id).or_default().insert(r.object_id);
            adj.entry(r.object_id).or_default().insert(r.subject_id);
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            edge_preds.entry(key).or_insert_with(|| r.predicate.clone());
        }

        // Direct edge set
        let direct: HashSet<(i64, i64)> = edge_preds.keys().copied().collect();

        // Find open triads: A-B-C where A and C are not connected
        // For efficiency, only consider nodes with degree 3..100
        let candidates: Vec<i64> = adj
            .iter()
            .filter(|(_, nb)| nb.len() >= 3 && nb.len() <= 100)
            .map(|(&id, _)| id)
            .collect();

        // Count intermediaries for each (A,C) pair
        let mut pair_intermediaries: HashMap<(i64, i64), Vec<i64>> = HashMap::new();
        for &b in &candidates {
            let nb: Vec<i64> = adj[&b].iter().copied().collect();
            // Cap to avoid O(n²) within large neighborhoods
            if nb.len() > 80 {
                continue;
            }
            for i in 0..nb.len() {
                for j in (i + 1)..nb.len() {
                    let a = nb[i].min(nb[j]);
                    let c = nb[i].max(nb[j]);
                    if !direct.contains(&(a, c)) {
                        pair_intermediaries.entry((a, c)).or_default().push(b);
                    }
                }
            }
        }

        // Only generate hypotheses for pairs with ≥2 intermediaries (strong triadic signal)
        let mut scored: Vec<((i64, i64), usize)> = pair_intermediaries
            .iter()
            .filter(|(_, intermediaries)| intermediaries.len() >= 2)
            .map(|(&pair, intermediaries)| (pair, intermediaries.len()))
            .collect();
        scored.sort_by(|a, b| b.1.cmp(&a.1));
        scored.truncate(50);

        let mut hypotheses = Vec::new();
        for ((a, c), count) in scored {
            let a_name = id_name.get(&a).copied().unwrap_or("?");
            let c_name = id_name.get(&c).copied().unwrap_or("?");
            let a_type = id_type.get(&a).copied().unwrap_or("?");
            let c_type = id_type.get(&c).copied().unwrap_or("?");

            if is_noise_name(a_name) || is_noise_name(c_name) {
                continue;
            }
            if !types_compatible(a_type, c_type) {
                continue;
            }

            let predicate = infer_predicate(a_type, c_type, None);

            // Skip low-precision predicates (contemporary_of ~60%, related_to ~60%)
            if predicate == "contemporary_of" || predicate == "related_to" {
                continue;
            }
            // Skip trivial geo-geo associations
            if predicate == "associated_with"
                && (a_type == "place" || a_type == "location")
                && (c_type == "place" || c_type == "location")
            {
                continue;
            }

            let intermediary_names: Vec<String> = pair_intermediaries[&(a, c)]
                .iter()
                .take(5)
                .filter_map(|&id| id_name.get(&id).map(|n| n.to_string()))
                .collect();

            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.to_string(),
                predicate: predicate.to_string(),
                object: c_name.to_string(),
                confidence: 0.40 + (count as f64 * 0.08).min(0.35),
                evidence_for: vec![format!(
                    "{} and {} share {} mutual connections: {}",
                    a_name,
                    c_name,
                    count,
                    intermediary_names.join(", ")
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!("Triadic closure: {} mutual neighbors", count),
                    format!("Intermediaries: {}", intermediary_names.join(", ")),
                    "Open triads tend to close in real-world networks".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "triadic_closure".to_string(),
            });
        }
        Ok(hypotheses)
    }

    /// Jaccard coefficient link prediction: hypotheses from normalized neighbor overlap.
    /// Better than raw shared-neighbor count because it controls for hub-degree bias.
    pub fn generate_hypotheses_from_jaccard(&self) -> Result<Vec<Hypothesis>> {
        let predictions = crate::graph::jaccard_predict(self.brain, 30)?;
        let meaningful = meaningful_ids(self.brain)?;
        let entities = self.brain.all_entities()?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        let mut hypotheses = Vec::new();
        for (a, b, score) in &predictions {
            if !meaningful.contains(a) || !meaningful.contains(b) {
                continue;
            }
            let a_name = id_name.get(a).copied().unwrap_or("?");
            let b_name = id_name.get(b).copied().unwrap_or("?");
            if is_noise_name(a_name) || is_noise_name(b_name) {
                continue;
            }
            let a_type = id_type.get(a).copied().unwrap_or("unknown");
            let b_type = id_type.get(b).copied().unwrap_or("unknown");
            let predicate = infer_predicate(a_type, b_type, None);
            let confidence = self
                .calibrated_confidence("jaccard", 0.35 + (score * 0.5).min(0.45))
                .unwrap_or(0.5);
            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.to_string(),
                predicate: predicate.to_string(),
                object: b_name.to_string(),
                confidence,
                evidence_for: vec![format!("Jaccard neighbor similarity: {:.3}", score)],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} and {} have Jaccard coefficient {:.3}",
                        a_name, b_name, score
                    ),
                    "High normalized neighbor overlap suggests a missing link".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "jaccard".to_string(),
            });
        }
        Ok(hypotheses)
    }

    /// Generate hypotheses from "rising star" entities — high temporal momentum but low degree.
    /// These entities are rapidly gaining connections and likely have undiscovered links.
    /// Pairs rising stars with high-PageRank entities in the same type domain.
    pub fn generate_hypotheses_from_rising_stars(&self) -> Result<Vec<Hypothesis>> {
        let stars = crate::graph::rising_stars(self.brain, 20)?;
        if stars.is_empty() {
            return Ok(vec![]);
        }
        let meaningful = meaningful_ids(self.brain)?;
        let entities = self.brain.all_entities()?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Get PageRank scores for hub identification
        let pr = crate::graph::pagerank(self.brain, 0.85, 20)?;
        let mut type_hubs: HashMap<String, Vec<(i64, f64)>> = HashMap::new();
        for (&id, &score) in &pr {
            if !meaningful.contains(&id) {
                continue;
            }
            let etype = id_type.get(&id).copied().unwrap_or("unknown");
            type_hubs
                .entry(etype.to_string())
                .or_default()
                .push((id, score));
        }
        for hubs in type_hubs.values_mut() {
            hubs.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
            hubs.truncate(10);
        }

        // Build existing connections set
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            connected.insert(key);
        }

        // Pre-compute adjacency lists for shared-neighbor checks
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            adj.entry(r.subject_id).or_default().insert(r.object_id);
            adj.entry(r.object_id).or_default().insert(r.subject_id);
        }

        // Also pre-compute source URLs per entity for co-occurrence evidence
        let mut entity_sources: HashMap<i64, HashSet<String>> = HashMap::new();
        for r in &relations {
            let url = &r.source_url;
            if !url.is_empty() {
                entity_sources
                    .entry(r.subject_id)
                    .or_default()
                    .insert(url.clone());
                entity_sources
                    .entry(r.object_id)
                    .or_default()
                    .insert(url.clone());
            }
        }

        let mut hypotheses = Vec::new();
        for (star_id, star_name, momentum, degree) in &stars {
            if !meaningful.contains(star_id) || is_noise_name(star_name) {
                continue;
            }
            let star_type = id_type.get(star_id).copied().unwrap_or("unknown");
            let star_neighbors = adj.get(star_id).cloned().unwrap_or_default();
            let star_sources = entity_sources.get(star_id).cloned().unwrap_or_default();

            // Find top hubs in the same type that aren't already connected
            // but REQUIRE shared neighbors or source co-occurrence as evidence
            if let Some(hubs) = type_hubs.get(star_type) {
                for &(hub_id, hub_pr) in hubs.iter().take(5) {
                    if hub_id == *star_id {
                        continue;
                    }
                    let key = if *star_id < hub_id {
                        (*star_id, hub_id)
                    } else {
                        (hub_id, *star_id)
                    };
                    if connected.contains(&key) {
                        continue;
                    }
                    let hub_name = id_name.get(&hub_id).copied().unwrap_or("?");
                    if is_noise_name(hub_name) {
                        continue;
                    }

                    // Require at least 1 shared neighbor OR shared source URL
                    let hub_neighbors = adj.get(&hub_id).cloned().unwrap_or_default();
                    let shared_neighbors: Vec<i64> = star_neighbors
                        .intersection(&hub_neighbors)
                        .copied()
                        .collect();
                    let hub_sources = entity_sources.get(&hub_id).cloned().unwrap_or_default();
                    let shared_sources = star_sources.intersection(&hub_sources).count();

                    if shared_neighbors.is_empty() && shared_sources == 0 {
                        continue; // No evidence of relatedness — skip
                    }

                    let predicate = match star_type {
                        "person" => "contemporary_of",
                        "concept" => "related_concept",
                        "organization" | "company" => "affiliated_with",
                        "technology" => "builds_on",
                        _ => "related_to",
                    };

                    let neighbor_bonus = (shared_neighbors.len() as f64 * 0.05).min(0.2);
                    let source_bonus = (shared_sources as f64 * 0.03).min(0.1);
                    let confidence = self
                        .calibrated_confidence(
                            "rising_star",
                            (0.35
                                + (momentum * 0.1).min(0.3)
                                + neighbor_bonus
                                + source_bonus
                                + (hub_pr * 10.0).min(0.1))
                            .min(0.85),
                        )
                        .unwrap_or(0.45);

                    let mut evidence = vec![
                        format!(
                            "Rising star: momentum {:.3}, degree {} (rapidly growing)",
                            momentum, degree
                        ),
                        format!("Hub PageRank: {:.4}", hub_pr),
                    ];
                    if !shared_neighbors.is_empty() {
                        let shared_names: Vec<&str> = shared_neighbors
                            .iter()
                            .take(3)
                            .filter_map(|id| id_name.get(id).copied())
                            .collect();
                        evidence.push(format!(
                            "Shared {} neighbor(s): {}",
                            shared_neighbors.len(),
                            shared_names.join(", ")
                        ));
                    }
                    if shared_sources > 0 {
                        evidence.push(format!("Co-occur in {} source(s)", shared_sources));
                    }

                    hypotheses.push(Hypothesis {
                        id: 0,
                        subject: star_name.clone(),
                        predicate: predicate.to_string(),
                        object: hub_name.to_string(),
                        confidence,
                        evidence_for: evidence,
                        evidence_against: vec![],
                        reasoning_chain: vec![
                            format!(
                                "{} is a rising star ({} type) with high temporal momentum",
                                star_name, star_type
                            ),
                            format!(
                                "{} is a top hub in the same domain (PageRank {:.4})",
                                hub_name, hub_pr
                            ),
                            format!(
                                "Evidence: {} shared neighbors, {} shared sources",
                                shared_neighbors.len(),
                                shared_sources
                            ),
                        ],
                        status: HypothesisStatus::Proposed,
                        discovered_at: now_str(),
                        pattern_source: "rising_star".to_string(),
                    });
                }
            }
        }
        hypotheses.truncate(30);
        Ok(hypotheses)
    }

    /// Preferential attachment: high-degree nodes that aren't yet connected are
    /// likely to form connections (rich-get-richer effect in knowledge graphs).
    /// Uses the graph-level predictor and infers predicates from entity types.
    pub fn generate_hypotheses_from_preferential_attachment(&self) -> Result<Vec<Hypothesis>> {
        let predictions = crate::graph::preferential_attachment_predict(self.brain, 60)?;
        if predictions.is_empty() {
            return Ok(vec![]);
        }
        let meaningful = meaningful_ids(self.brain)?;
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Build adjacency for shared-neighbor check
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            adj.entry(r.subject_id).or_default().insert(r.object_id);
            adj.entry(r.object_id).or_default().insert(r.subject_id);
        }

        let mut hypotheses = Vec::new();
        for (a, b, score) in &predictions {
            if !meaningful.contains(a) || !meaningful.contains(b) {
                continue;
            }
            let a_name = match id_name.get(a) {
                Some(n) => *n,
                None => continue,
            };
            let b_name = match id_name.get(b) {
                Some(n) => *n,
                None => continue,
            };
            if is_noise_name(a_name) || is_noise_name(b_name) {
                continue;
            }
            let a_type = id_type.get(a).copied().unwrap_or("unknown");
            let b_type = id_type.get(b).copied().unwrap_or("unknown");

            // Skip incompatible type pairs (PA alone is too weak without type alignment)
            if is_type_incompatible(a_type, b_type, "related_to") {
                continue;
            }

            // Require at least 1 shared neighbor for PA hypotheses
            let a_neighbors = adj.get(a).cloned().unwrap_or_default();
            let b_neighbors = adj.get(b).cloned().unwrap_or_default();
            let shared: Vec<i64> = a_neighbors.intersection(&b_neighbors).copied().collect();
            if shared.is_empty() {
                continue;
            }

            let predicate = infer_predicate(a_type, b_type, None);

            // Skip contemporary_of — PA via hub connectivity produces noise
            if predicate == "contemporary_of" {
                continue;
            }

            // Normalize score: PA scores can be very large (degree_a * degree_b)
            let norm_score = score.ln().max(1.0) / 15.0;
            let neighbor_bonus = (shared.len() as f64 * 0.05).min(0.15);
            let base_conf = (0.3 + norm_score * 0.3 + neighbor_bonus).min(0.80);
            let confidence = self
                .calibrated_confidence("preferential_attachment", base_conf)
                .unwrap_or(base_conf);

            let shared_names: Vec<&str> = shared
                .iter()
                .take(3)
                .filter_map(|id| id_name.get(id).copied())
                .collect();

            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.to_string(),
                predicate: predicate.to_string(),
                object: b_name.to_string(),
                confidence,
                evidence_for: vec![
                    format!(
                        "Preferential attachment score: {:.0} (high-degree nodes not yet connected)",
                        score
                    ),
                    format!(
                        "Shared {} neighbor(s): {}",
                        shared.len(),
                        shared_names.join(", ")
                    ),
                ],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} ({}) and {} ({}) are both high-degree hubs",
                        a_name, a_type, b_name, b_type
                    ),
                    format!("Connected through {} shared neighbors", shared.len()),
                    "Preferential attachment + shared context: likely to form connection".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "preferential_attachment".to_string(),
            });
        }
        hypotheses.truncate(40);
        Ok(hypotheses)
    }

    /// Katz similarity link prediction: uses multi-hop path counts (weighted by α^depth)
    /// to find entities that are well-connected through the graph but lack a direct edge.
    /// Better than single-hop methods for discovering non-obvious connections.
    pub fn generate_hypotheses_from_katz_similarity(&self) -> Result<Vec<Hypothesis>> {
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;
        let relations = self.brain.all_relations()?;

        // Build existing edge set for quick lookup
        let mut existing_edges: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            existing_edges.insert((r.subject_id, r.object_id));
            existing_edges.insert((r.object_id, r.subject_id));
        }

        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Get top Katz-central entities as candidates (checking all pairs is O(n²))
        let katz_scores = crate::graph::katz_centrality(self.brain, 0.05, 4, 100)?;
        let top_n = 80.min(katz_scores.len());
        let top_ids: Vec<i64> = katz_scores[..top_n].iter().map(|&(id, _)| id).collect();

        // Build adjacency once to avoid reloading relations per pair
        let mut adj: HashMap<i64, Vec<i64>> = HashMap::new();
        for r in &relations {
            adj.entry(r.subject_id).or_default().push(r.object_id);
            adj.entry(r.object_id).or_default().push(r.subject_id);
        }

        let mut candidates: Vec<(i64, i64, f64)> = Vec::new();
        let alpha = 0.05_f64;
        for i in 0..top_ids.len() {
            for j in (i + 1)..top_ids.len() {
                let a = top_ids[i];
                let b = top_ids[j];
                if !meaningful.contains(&a) || !meaningful.contains(&b) {
                    continue;
                }
                if existing_edges.contains(&(a, b)) {
                    continue;
                }
                // Inline Katz similarity (BFS path counting, depth ≤ 4)
                let mut sim = 0.0_f64;
                let mut frontier: HashMap<i64, f64> = HashMap::new();
                frontier.insert(a, 1.0);
                for depth in 1..=4_usize {
                    let mut next: HashMap<i64, f64> = HashMap::new();
                    for (&node, &count) in &frontier {
                        if let Some(neighbors) = adj.get(&node) {
                            for &nbr in neighbors {
                                *next.entry(nbr).or_insert(0.0) += count;
                            }
                        }
                    }
                    if let Some(&paths) = next.get(&b) {
                        sim += alpha.powi(depth as i32) * paths;
                    }
                    frontier = next;
                }
                if sim > 0.001 {
                    candidates.push((a, b, sim));
                }
            }
        }
        candidates.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap_or(std::cmp::Ordering::Equal));
        candidates.truncate(40);

        let mut hypotheses = Vec::new();
        for (a, b, sim) in &candidates {
            let a_name = match id_name.get(a) {
                Some(n) => *n,
                None => continue,
            };
            let b_name = match id_name.get(b) {
                Some(n) => *n,
                None => continue,
            };
            if is_noise_name(a_name) || is_noise_name(b_name) {
                continue;
            }
            // Require at least 1 shared neighbor to reduce false positives
            let a_nbrs: HashSet<i64> = adj
                .get(a)
                .map(|v| v.iter().copied().collect())
                .unwrap_or_default();
            let b_nbrs: HashSet<i64> = adj
                .get(b)
                .map(|v| v.iter().copied().collect())
                .unwrap_or_default();
            let shared_neighbors = a_nbrs.intersection(&b_nbrs).count();
            if shared_neighbors < 3 {
                continue;
            }
            let a_type = id_type.get(a).copied().unwrap_or("unknown");
            let b_type = id_type.get(b).copied().unwrap_or("unknown");
            // Skip type-incompatible pairs (e.g., person↔concept via generic predicate)
            if !types_compatible(a_type, b_type) {
                continue;
            }
            let predicate = infer_predicate(a_type, b_type, None);

            // Skip low-signal predicates that Katz picks up via hub connectivity
            // but rarely produce useful knowledge:
            // - contemporary_of: 47% confirmation rate
            // - related_to: 32% confirmation rate
            // - associated_with between two places: trivially true geo connections
            if predicate == "contemporary_of" || predicate == "related_to" {
                continue;
            }
            if predicate == "associated_with"
                && (a_type == "place" || a_type == "location")
                && (b_type == "place" || b_type == "location")
            {
                continue;
            }

            // Boost confidence by shared neighbor count (starts at 2 now)
            let neighbor_boost = (shared_neighbors as f64 * 0.03).min(0.15);
            let base_conf = (0.35 + (sim.ln().max(-5.0) + 5.0) * 0.08 + neighbor_boost).min(0.75);
            let confidence = self
                .calibrated_confidence("katz_similarity", base_conf)
                .unwrap_or(base_conf);

            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.to_string(),
                predicate: predicate.to_string(),
                object: b_name.to_string(),
                confidence,
                evidence_for: vec![format!(
                    "Katz similarity: {:.6} (connected through multiple indirect paths)",
                    sim
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} ({}) and {} ({}) share many multi-hop paths",
                        a_name, a_type, b_name, b_type
                    ),
                    "Katz index: entities reachable through multiple short paths tend to be related"
                        .to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "katz_similarity".to_string(),
            });
        }
        Ok(hypotheses)
    }

    /// Predicate pattern transfer: if two same-type entities share most predicates,
    /// hypothesize that a predicate present in one but absent in the other should transfer.
    /// E.g., if Einstein has {pioneered, active_in, affiliated_with} and Bohr has
    /// {pioneered, active_in} but NOT affiliated_with, suggest Bohr → affiliated_with → ?.
    pub fn generate_hypotheses_from_predicate_transfer(&self) -> Result<Vec<Hypothesis>> {
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;
        let relations = self.brain.all_relations()?;

        // Build predicate profile per entity: entity_id → set of (predicate, direction)
        // Also track (predicate, object_name) for transfer
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Build outgoing predicate profiles
        let mut out_preds: HashMap<i64, HashSet<String>> = HashMap::new();
        let mut pred_objects: HashMap<(i64, String), Vec<String>> = HashMap::new();
        for r in &relations {
            if !meaningful.contains(&r.subject_id) || !meaningful.contains(&r.object_id) {
                continue;
            }
            if is_generic_predicate(&r.predicate) {
                continue;
            }
            out_preds
                .entry(r.subject_id)
                .or_default()
                .insert(r.predicate.clone());
            let obj_name = id_name.get(&r.object_id).copied().unwrap_or("?");
            pred_objects
                .entry((r.subject_id, r.predicate.clone()))
                .or_default()
                .push(obj_name.to_string());
        }

        // Group entities by type for comparison
        let mut type_groups: HashMap<&str, Vec<i64>> = HashMap::new();
        for e in &entities {
            if !meaningful.contains(&e.id)
                || is_noise_type(&e.entity_type)
                || is_noise_name(&e.name)
            {
                continue;
            }
            let preds = out_preds.get(&e.id);
            // Only consider entities with at least 2 predicates
            if preds.is_none_or(|p| p.len() < 2) {
                continue;
            }
            type_groups
                .entry(id_type.get(&e.id).copied().unwrap_or("?"))
                .or_default()
                .push(e.id);
        }

        // Build predicate prevalence per type group: only transfer predicates
        // that are common across the group (≥25% of entities have it).
        // Rare predicates are likely entity-specific, not transferable.
        let mut type_pred_prevalence: HashMap<String, HashMap<String, usize>> = HashMap::new();
        for (etype, ids) in &type_groups {
            let counts = type_pred_prevalence.entry(etype.to_string()).or_default();
            for &eid in ids {
                if let Some(preds) = out_preds.get(&eid) {
                    for p in preds {
                        *counts.entry(p.clone()).or_insert(0) += 1;
                    }
                }
            }
        }

        let mut hypotheses = Vec::new();
        let mut seen: HashSet<(i64, String)> = HashSet::new();

        for (etype, group) in &type_groups {
            let prevalence = type_pred_prevalence.get(*etype);
            let group_size = group.len();
            // Shuffle for diversity when group is large, then sample
            let group: Vec<i64> = if group.len() > 300 {
                // Deterministic but varied sampling: pick every Nth entity
                let step = group.len() / 300;
                group
                    .iter()
                    .step_by(step.max(1))
                    .copied()
                    .take(300)
                    .collect()
            } else {
                group.clone()
            };
            let group = &group[..];
            for i in 0..group.len() {
                if hypotheses.len() >= 50 {
                    break;
                }
                let a = group[i];
                let a_preds = match out_preds.get(&a) {
                    Some(p) => p,
                    None => continue,
                };
                for j in (i + 1)..group.len() {
                    let b = group[j];
                    let b_preds = match out_preds.get(&b) {
                        Some(p) => p,
                        None => continue,
                    };

                    let shared = a_preds.intersection(b_preds).count();
                    let total = a_preds.union(b_preds).count();
                    if total == 0 {
                        continue;
                    }
                    let jaccard = shared as f64 / total as f64;
                    // Tighter thresholds: need high overlap (>=0.65) and at least 5 shared predicates
                    // to reduce noise — this strategy had only ~4% confirmation rate with looser thresholds
                    if jaccard < 0.65 || shared < 5 {
                        continue;
                    }

                    // Find predicates in A but not B — only transfer prevalent predicates
                    for missing_pred in a_preds.difference(b_preds) {
                        // Skip predicates that aren't common in this type group (< 25% prevalence)
                        if let Some(prev) = prevalence {
                            let pred_count = prev.get(missing_pred.as_str()).copied().unwrap_or(0);
                            if group_size >= 10 && (pred_count as f64 / group_size as f64) < 0.25 {
                                continue;
                            }
                        }
                        let key = (b, missing_pred.clone());
                        if seen.contains(&key) {
                            continue;
                        }
                        seen.insert(key);
                        let b_name = id_name.get(&b).copied().unwrap_or("?");
                        let a_name = id_name.get(&a).copied().unwrap_or("?");
                        if is_noise_name(b_name) || is_noise_name(a_name) {
                            continue;
                        }
                        // Get an example object from the source entity
                        let example_obj = pred_objects
                            .get(&(a, missing_pred.clone()))
                            .and_then(|objs| objs.first())
                            .cloned()
                            .unwrap_or_else(|| "?".to_string());
                        let confidence = self
                            .calibrated_confidence(
                                "predicate_transfer",
                                (0.3 + jaccard * 0.3).min(0.75),
                            )
                            .unwrap_or(0.45);
                        // Analogical object inference: if A→pred→obj, try to predict
                        // that B→pred→obj (same object) or B→pred→type-compatible object.
                        // This dramatically improves validation vs object="?".
                        let all_objects = pred_objects
                            .get(&(a, missing_pred.clone()))
                            .cloned()
                            .unwrap_or_default();
                        // Pick the most specific object (longest name, not "?")
                        let best_object = all_objects
                            .iter()
                            .filter(|o| *o != "?")
                            .max_by_key(|o| o.len())
                            .cloned()
                            .unwrap_or_else(|| "?".to_string());

                        hypotheses.push(Hypothesis {
                            id: 0,
                            subject: b_name.to_string(),
                            predicate: missing_pred.clone(),
                            object: best_object.clone(),
                            confidence,
                            evidence_for: vec![format!(
                                "Predicate profile overlap {:.0}% with {} ({} shared predicates); {} has '{}' → {}; analogical prediction: {} → {} → {}",
                                jaccard * 100.0, a_name, shared, a_name, missing_pred, example_obj,
                                b_name, missing_pred, best_object
                            )],
                            evidence_against: vec![],
                            reasoning_chain: vec![
                                format!("{} and {} share {} of {} predicates (Jaccard {:.2})", b_name, a_name, shared, total, jaccard),
                                format!("{} has '{}' → '{}'; {} lacks this predicate", a_name, missing_pred, best_object, b_name),
                                format!("Analogical inference: similar entities often share the same predicate targets"),
                            ],
                            status: HypothesisStatus::Proposed,
                            discovered_at: now_str(),
                            pattern_source: "predicate_transfer".to_string(),
                        });
                        if hypotheses.len() >= 50 {
                            break;
                        }
                    }
                    // Also check B → A direction — only transfer prevalent predicates
                    for missing_pred in b_preds.difference(a_preds) {
                        if let Some(prev) = prevalence {
                            let pred_count = prev.get(missing_pred.as_str()).copied().unwrap_or(0);
                            if group_size >= 10 && (pred_count as f64 / group_size as f64) < 0.25 {
                                continue;
                            }
                        }
                        let key = (a, missing_pred.clone());
                        if seen.contains(&key) {
                            continue;
                        }
                        seen.insert(key);
                        let a_name = id_name.get(&a).copied().unwrap_or("?");
                        let b_name = id_name.get(&b).copied().unwrap_or("?");
                        if is_noise_name(a_name) || is_noise_name(b_name) {
                            continue;
                        }
                        let example_obj = pred_objects
                            .get(&(b, missing_pred.clone()))
                            .and_then(|objs| objs.first())
                            .cloned()
                            .unwrap_or_else(|| "?".to_string());
                        let confidence = self
                            .calibrated_confidence(
                                "predicate_transfer",
                                (0.3 + jaccard * 0.3).min(0.75),
                            )
                            .unwrap_or(0.45);
                        // Analogical object inference for B→A direction
                        let all_objects_b = pred_objects
                            .get(&(b, missing_pred.clone()))
                            .cloned()
                            .unwrap_or_default();
                        let best_object_b = all_objects_b
                            .iter()
                            .filter(|o| *o != "?")
                            .max_by_key(|o| o.len())
                            .cloned()
                            .unwrap_or_else(|| "?".to_string());

                        hypotheses.push(Hypothesis {
                            id: 0,
                            subject: a_name.to_string(),
                            predicate: missing_pred.clone(),
                            object: best_object_b.clone(),
                            confidence,
                            evidence_for: vec![format!(
                                "Predicate profile overlap {:.0}% with {} ({} shared predicates); {} has '{}' → {}; analogical prediction: {} → {} → {}",
                                jaccard * 100.0, b_name, shared, b_name, missing_pred, example_obj,
                                a_name, missing_pred, best_object_b
                            )],
                            evidence_against: vec![],
                            reasoning_chain: vec![
                                format!("{} and {} share {} of {} predicates (Jaccard {:.2})", a_name, b_name, shared, total, jaccard),
                                format!("{} has '{}' → '{}'; {} lacks this predicate", b_name, missing_pred, best_object_b, a_name),
                                format!("Analogical inference: similar entities often share the same predicate targets"),
                            ],
                            status: HypothesisStatus::Proposed,
                            discovered_at: now_str(),
                            pattern_source: "predicate_transfer".to_string(),
                        });
                        if hypotheses.len() >= 50 {
                            break;
                        }
                    }
                }
            }
        }
        hypotheses.truncate(30);
        Ok(hypotheses)
    }

    /// Isolated entity connector: find high-value isolated entities (no relations)
    /// and connect them to the graph via shared facts (same key-value pairs)
    /// or name substring matching with existing connected entities.
    /// Targets the 4600+ isolated nodes to reduce graph fragmentation.
    pub fn generate_hypotheses_from_isolated_connector(&self) -> Result<Vec<Hypothesis>> {
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;
        let relations = self.brain.all_relations()?;

        // Find connected entity IDs
        let mut connected_ids: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected_ids.insert(r.subject_id);
            connected_ids.insert(r.object_id);
        }

        // Find isolated high-value entities
        let isolated: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| {
                !connected_ids.contains(&e.id)
                    && meaningful.contains(&e.id)
                    && !is_noise_name(&e.name)
                    && !is_noise_type(&e.entity_type)
                    && HIGH_VALUE_TYPES.contains(&e.entity_type.as_str())
            })
            .take(200) // sample
            .collect();

        if isolated.is_empty() {
            return Ok(vec![]);
        }

        // Build fact index: (key, value) → list of entity_ids
        let mut fact_index: HashMap<(String, String), Vec<i64>> = HashMap::new();
        // Only index connected entities' facts for matching targets
        for &eid in &connected_ids {
            if !meaningful.contains(&eid) {
                continue;
            }
            let facts = self.brain.get_facts_for(eid)?;
            for f in facts {
                if f.key != "mention_count" && f.key != "url" && f.confidence >= 0.5 {
                    fact_index
                        .entry((f.key.clone(), f.value.to_lowercase()))
                        .or_default()
                        .push(eid);
                }
            }
        }

        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        let mut hypotheses = Vec::new();
        let mut seen_pairs: HashSet<(i64, i64)> = HashSet::new();

        for iso in &isolated {
            if hypotheses.len() >= 40 {
                break;
            }
            let iso_facts = self.brain.get_facts_for(iso.id)?;

            // Find connected entities sharing fact key-value pairs
            let mut candidates: HashMap<i64, Vec<String>> = HashMap::new();
            for f in &iso_facts {
                if f.key == "mention_count" || f.key == "url" || f.confidence < 0.5 {
                    continue;
                }
                let key = (f.key.clone(), f.value.to_lowercase());
                if let Some(matches) = fact_index.get(&key) {
                    for &match_id in matches {
                        if match_id == iso.id {
                            continue;
                        }
                        let pair = if iso.id < match_id {
                            (iso.id, match_id)
                        } else {
                            (match_id, iso.id)
                        };
                        if seen_pairs.contains(&pair) {
                            continue;
                        }
                        candidates
                            .entry(match_id)
                            .or_default()
                            .push(format!("{}={}", f.key, f.value));
                    }
                }
            }

            // Only consider candidates with at least 2 shared facts
            for (cand_id, shared_facts) in &candidates {
                if shared_facts.len() < 2 {
                    continue;
                }
                let pair = if iso.id < *cand_id {
                    (iso.id, *cand_id)
                } else {
                    (*cand_id, iso.id)
                };
                if !seen_pairs.insert(pair) {
                    continue;
                }
                let cand_name = id_name.get(cand_id).copied().unwrap_or("?");
                let cand_type = id_type.get(cand_id).copied().unwrap_or("unknown");
                if is_noise_name(cand_name) {
                    continue;
                }

                let predicate = infer_predicate(&iso.entity_type, cand_type, None);
                let base_conf = (0.4 + shared_facts.len() as f64 * 0.1).min(0.80);
                let confidence = self
                    .calibrated_confidence("isolated_connector", base_conf)
                    .unwrap_or(base_conf);

                hypotheses.push(Hypothesis {
                    id: 0,
                    subject: iso.name.clone(),
                    predicate: predicate.to_string(),
                    object: cand_name.to_string(),
                    confidence,
                    evidence_for: vec![
                        format!(
                            "Isolated entity {} shares {} facts with connected entity {}",
                            iso.name,
                            shared_facts.len(),
                            cand_name
                        ),
                        format!("Shared facts: {}", shared_facts.join(", ")),
                    ],
                    evidence_against: vec![],
                    reasoning_chain: vec![
                        format!(
                            "{} ({}) is isolated (no relations) but has matching facts",
                            iso.name, iso.entity_type
                        ),
                        format!(
                            "{} ({}) shares {} fact key-value pairs",
                            cand_name,
                            cand_type,
                            shared_facts.len()
                        ),
                        "Fact similarity suggests a real-world connection".to_string(),
                    ],
                    status: HypothesisStatus::Proposed,
                    discovered_at: now_str(),
                    pattern_source: "isolated_connector".to_string(),
                });
            }
        }
        hypotheses.truncate(30);
        Ok(hypotheses)
    }

    /// Generate hypotheses to enrich sparse concept entities.
    /// Concepts average <0.5 relations — find the most likely connections
    /// by matching concept names against entity names and fact values.
    /// Strategy: for each sparse concept (≤1 relation), find entities whose
    /// name contains the concept name or who share a source URL.
    pub fn generate_hypotheses_from_concept_enrichment(&self) -> Result<Vec<Hypothesis>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        // Count relations per entity
        let mut rel_count: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *rel_count.entry(r.subject_id).or_insert(0) += 1;
            *rel_count.entry(r.object_id).or_insert(0) += 1;
        }

        // Find sparse concepts
        let sparse_concepts: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| {
                e.entity_type == "concept"
                    && meaningful.contains(&e.id)
                    && !is_noise_name(&e.name)
                    && e.name.len() >= 4
                    && rel_count.get(&e.id).copied().unwrap_or(0) <= 1
            })
            .take(300)
            .collect();

        if sparse_concepts.is_empty() {
            return Ok(vec![]);
        }

        // Build source_url → entity_id index from facts
        let mut source_index: HashMap<String, Vec<i64>> = HashMap::new();
        for r in &relations {
            if !r.source_url.is_empty() {
                source_index
                    .entry(r.source_url.clone())
                    .or_default()
                    .push(r.subject_id);
                source_index
                    .entry(r.source_url.clone())
                    .or_default()
                    .push(r.object_id);
            }
        }

        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();

        // Non-concept entities with ≥2 relations (good connection targets)
        let connected_entities: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| {
                e.entity_type != "concept"
                    && meaningful.contains(&e.id)
                    && rel_count.get(&e.id).copied().unwrap_or(0) >= 2
                    && !is_noise_name(&e.name)
            })
            .collect();

        let mut hypotheses = Vec::new();
        let mut seen_pairs: HashSet<(String, String)> = HashSet::new();

        for concept in &sparse_concepts {
            if hypotheses.len() >= 50 {
                break;
            }
            let concept_lower = concept.name.to_lowercase();
            let concept_words: Vec<&str> = concept_lower.split_whitespace().collect();

            // Strategy 1: Find non-concept entities whose name contains this concept
            for target in &connected_entities {
                if hypotheses.len() >= 50 {
                    break;
                }
                let target_lower = target.name.to_lowercase();
                // Concept must be a meaningful substring (not just "a" in "Argentina")
                if concept_words.len() >= 2 && target_lower.contains(&concept_lower) {
                    let pair = (concept.name.clone(), target.name.clone());
                    if seen_pairs.insert(pair) {
                        hypotheses.push(Hypothesis {
                            id: 0,
                            subject: concept.name.clone(),
                            predicate: "related_concept".into(),
                            object: target.name.clone(),
                            confidence: 0.55,
                            evidence_for: vec![format!(
                                "concept '{}' appears in entity name '{}'",
                                concept.name, target.name
                            )],
                            evidence_against: vec![],
                            reasoning_chain: vec![format!(
                                "Name containment: '{}' is a concept that appears within '{}'",
                                concept.name, target.name
                            )],
                            status: HypothesisStatus::Testing,
                            discovered_at: now_str(),
                            pattern_source: "concept_enrichment".into(),
                        });
                    }
                }
            }

            // Strategy 2: Concept shares a source URL with connected entities
            let concept_rels: Vec<_> = relations
                .iter()
                .filter(|r| r.subject_id == concept.id || r.object_id == concept.id)
                .collect();
            for cr in &concept_rels {
                if !cr.source_url.is_empty() {
                    if let Some(co_entities) = source_index.get(&cr.source_url) {
                        for &eid in co_entities {
                            if eid == concept.id || hypotheses.len() >= 50 {
                                continue;
                            }
                            if rel_count.get(&eid).copied().unwrap_or(0) < 2 {
                                continue;
                            }
                            if let Some(&name) = id_name.get(&eid) {
                                if is_noise_name(name) {
                                    continue;
                                }
                                let pair = (concept.name.clone(), name.to_string());
                                if seen_pairs.insert(pair) {
                                    hypotheses.push(Hypothesis {
                                        id: 0,
                                        subject: concept.name.clone(),
                                        predicate: "related_concept".into(),
                                        object: name.to_string(),
                                        confidence: 0.50,
                                        evidence_for: vec![format!(
                                            "co-occurs in source: {}",
                                            cr.source_url
                                        )],
                                        evidence_against: vec![],
                                        reasoning_chain: vec![format!(
                                            "Source co-occurrence: '{}' and '{}' share source URL",
                                            concept.name, name
                                        )],
                                        status: HypothesisStatus::Testing,
                                        discovered_at: now_str(),
                                        pattern_source: "concept_enrichment".into(),
                                    });
                                }
                            }
                        }
                    }
                }
            }
        }

        Ok(hypotheses)
    }

    /// Generate hypotheses from predicate-profile similarity: entities that
    /// participate in similar distributions of predicate types (e.g., both have
    /// `pioneered`, `active_in`, `affiliated_with`) are structurally analogous
    /// and likely related, even without direct connections.
    pub fn generate_hypotheses_from_predicate_profile(&self) -> Result<Vec<Hypothesis>> {
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;
        let relations = self.brain.all_relations()?;

        // Build predicate profile per entity: {entity_id → {predicate → count}}
        let mut profiles: HashMap<i64, HashMap<String, usize>> = HashMap::new();
        let mut existing_edges: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            *profiles
                .entry(r.subject_id)
                .or_default()
                .entry(r.predicate.clone())
                .or_insert(0) += 1;
            *profiles
                .entry(r.object_id)
                .or_default()
                .entry(r.predicate.clone())
                .or_insert(0) += 1;
            existing_edges.insert((r.subject_id, r.object_id));
            existing_edges.insert((r.object_id, r.subject_id));
        }

        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Collect all predicate labels for vector indexing
        let all_predicates: Vec<String> = {
            let mut preds: HashSet<String> = HashSet::new();
            for prof in profiles.values() {
                for k in prof.keys() {
                    preds.insert(k.clone());
                }
            }
            let mut v: Vec<String> = preds.into_iter().collect();
            v.sort();
            v
        };
        let pred_idx: HashMap<&str, usize> = all_predicates
            .iter()
            .enumerate()
            .map(|(i, p)| (p.as_str(), i))
            .collect();
        let dim = all_predicates.len();
        if dim < 3 {
            return Ok(vec![]);
        }

        // Only consider entities with ≥3 distinct predicates (rich profiles)
        let candidates: Vec<i64> = profiles
            .iter()
            .filter(|(id, prof)| {
                prof.len() >= 3
                    && meaningful.contains(id)
                    && id_name.get(id).map(|n| !is_noise_name(n)).unwrap_or(false)
            })
            .map(|(id, _)| *id)
            .collect();

        if candidates.len() < 2 || candidates.len() > 5000 {
            // Too few or too many — skip
            return Ok(vec![]);
        }

        // Build normalized vectors
        let vectors: HashMap<i64, Vec<f64>> = candidates
            .iter()
            .filter_map(|&id| {
                let prof = profiles.get(&id)?;
                let mut vec = vec![0.0_f64; dim];
                for (pred, &count) in prof {
                    if let Some(&idx) = pred_idx.get(pred.as_str()) {
                        vec[idx] = count as f64;
                    }
                }
                let norm: f64 = vec.iter().map(|x| x * x).sum::<f64>().sqrt();
                if norm > 0.0 {
                    for x in vec.iter_mut() {
                        *x /= norm;
                    }
                    Some((id, vec))
                } else {
                    None
                }
            })
            .collect();

        // Sample pairs (full O(n²) too expensive for large n)
        let sample_ids: Vec<i64> = {
            let mut ids: Vec<i64> = vectors.keys().copied().collect();
            ids.sort(); // deterministic
            ids.truncate(500);
            ids
        };

        let mut scored: Vec<(i64, i64, f64)> = Vec::new();
        for i in 0..sample_ids.len() {
            let a = sample_ids[i];
            let va = match vectors.get(&a) {
                Some(v) => v,
                None => continue,
            };
            for j in (i + 1)..sample_ids.len() {
                let b = sample_ids[j];
                if existing_edges.contains(&(a, b)) {
                    continue;
                }
                // Must be same type for meaningful profile comparison
                let at = id_type.get(&a).copied().unwrap_or("");
                let bt = id_type.get(&b).copied().unwrap_or("");
                if at != bt {
                    continue;
                }
                let vb = match vectors.get(&b) {
                    Some(v) => v,
                    None => continue,
                };
                // Cosine similarity (vectors already normalized)
                let cosine: f64 = va.iter().zip(vb.iter()).map(|(x, y)| x * y).sum();
                // Concept-concept pairs need higher threshold (historically 88% rejection)
                let threshold = if at == "concept" { 0.93 } else { 0.85 };
                if cosine >= threshold {
                    scored.push((a, b, cosine));
                }
            }
        }

        scored.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap_or(std::cmp::Ordering::Equal));
        scored.truncate(30);

        let mut hypotheses = Vec::new();
        for (a, b, cosine) in &scored {
            let a_name = match id_name.get(a) {
                Some(n) => *n,
                None => continue,
            };
            let b_name = match id_name.get(b) {
                Some(n) => *n,
                None => continue,
            };
            if is_noise_name(a_name) || is_noise_name(b_name) {
                continue;
            }
            let a_type = id_type.get(a).copied().unwrap_or("unknown");
            let predicate = infer_predicate(a_type, a_type, None);
            let base_conf = 0.35 + (cosine - 0.85) * 2.0; // 0.85→0.35, 1.0→0.65
            let confidence = self
                .calibrated_confidence("predicate_profile", base_conf)
                .unwrap_or(base_conf);

            // Describe which predicates they share
            let a_prof = profiles.get(a).cloned().unwrap_or_default();
            let b_prof = profiles.get(b).cloned().unwrap_or_default();
            let shared_preds: Vec<String> = a_prof
                .keys()
                .filter(|k| b_prof.contains_key(*k))
                .take(5)
                .cloned()
                .collect();

            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.to_string(),
                predicate: predicate.to_string(),
                object: b_name.to_string(),
                confidence,
                evidence_for: vec![
                    format!("Predicate-profile cosine similarity: {:.3}", cosine),
                    format!("Shared predicate types: {}", shared_preds.join(", ")),
                ],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} and {} have highly similar relation profiles ({} shared predicates)",
                        a_name,
                        b_name,
                        shared_preds.len()
                    ),
                    "Entities with analogous structural roles are likely to be related".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "predicate_profile".to_string(),
            });
        }

        Ok(hypotheses)
    }

    /// Generate hypotheses from temporal co-discovery: isolated entities that
    /// were first_seen in the same narrow time window (≤2 hours) likely came
    /// from the same article/topic and should be connected.
    pub fn generate_hypotheses_from_temporal_co_discovery(&self) -> Result<Vec<Hypothesis>> {
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;
        let relations = self.brain.all_relations()?;

        // Build connected set
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Collect high-value isolated entities with timestamps
        let mut isolated: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| {
                !connected.contains(&e.id)
                    && meaningful.contains(&e.id)
                    && !is_noise_name(&e.name)
                    && !is_noise_type(&e.entity_type)
                    && HIGH_VALUE_TYPES.contains(&e.entity_type.as_str())
            })
            .collect();

        if isolated.len() < 2 {
            return Ok(vec![]);
        }

        // Sort by first_seen for efficient window scanning
        isolated.sort_by_key(|e| e.first_seen);

        let mut hypotheses = Vec::new();
        let mut seen: HashSet<(i64, i64)> = HashSet::new();
        let window = chrono::Duration::minutes(30);

        // Sliding window: O(n) scan for entities within 30min of each other
        // (tightened from 2h — wider windows had only ~16% confirmation rate)
        let mut start = 0;
        for end in 0..isolated.len() {
            // Advance start past the window
            while start < end && (isolated[end].first_seen - isolated[start].first_seen) > window {
                start += 1;
            }
            // Pair isolated[end] with all in [start..end)
            for j in start..end {
                if hypotheses.len() >= 40 {
                    break;
                }
                let a = isolated[j];
                let b = isolated[end];
                let pair = if a.id < b.id {
                    (a.id, b.id)
                } else {
                    (b.id, a.id)
                };
                if !seen.insert(pair) {
                    continue;
                }
                // Same-name skip
                if a.name.to_lowercase() == b.name.to_lowercase() {
                    continue;
                }
                // Must be same or compatible types
                if a.entity_type != b.entity_type
                    && !types_compatible(&a.entity_type, &b.entity_type)
                {
                    continue;
                }
                let delta_min = (b.first_seen - a.first_seen).num_minutes().unsigned_abs();
                let predicate = infer_predicate(&a.entity_type, &b.entity_type, None);
                let base_conf = if delta_min <= 5 {
                    0.60
                } else if delta_min <= 15 {
                    0.50
                } else {
                    0.40
                };
                let confidence = self
                    .calibrated_confidence("temporal_co_discovery", base_conf)
                    .unwrap_or(base_conf);

                hypotheses.push(Hypothesis {
                    id: 0,
                    subject: a.name.clone(),
                    predicate: predicate.to_string(),
                    object: b.name.clone(),
                    confidence,
                    evidence_for: vec![format!(
                        "Both isolated, discovered within {} minutes of each other",
                        delta_min
                    )],
                    evidence_against: vec![],
                    reasoning_chain: vec![
                        format!(
                            "{} ({}) and {} ({}) are both isolated entities",
                            a.name, a.entity_type, b.name, b.entity_type
                        ),
                        format!(
                            "First seen {} min apart — likely from same crawl topic",
                            delta_min
                        ),
                    ],
                    status: HypothesisStatus::Proposed,
                    discovered_at: now_str(),
                    pattern_source: "temporal_co_discovery".to_string(),
                });
            }
            if hypotheses.len() >= 40 {
                break;
            }
        }

        hypotheses.truncate(40);
        Ok(hypotheses)
    }

    /// Shared-predicate-object inverse inference: if A→pred→X and B→pred→X
    /// (multiple subjects share the same predicate+object), infer A and B
    /// are related. Higher signal than co-occurrence because it's predicate-specific.
    pub fn generate_hypotheses_from_shared_predicate_object(&self) -> Result<Vec<Hypothesis>> {
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;
        let entities = self.brain.all_entities()?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Group: (predicate, object_id) → Vec<subject_id>
        let mut pred_obj_subjects: HashMap<(String, i64), Vec<i64>> = HashMap::new();
        for r in &relations {
            if !meaningful.contains(&r.subject_id) || !meaningful.contains(&r.object_id) {
                continue;
            }
            if is_generic_predicate(&r.predicate) {
                continue;
            }
            pred_obj_subjects
                .entry((r.predicate.clone(), r.object_id))
                .or_default()
                .push(r.subject_id);
        }

        // Build existing-edge set for dedup
        let mut existing: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            let pair = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            existing.insert(pair);
        }

        let mut hypotheses = Vec::new();
        let mut seen: HashSet<(i64, i64)> = HashSet::new();

        for ((predicate, obj_id), subjects) in &pred_obj_subjects {
            if subjects.len() < 2 || subjects.len() > 50 {
                continue; // Too small or too generic
            }
            let obj_name = id_name.get(obj_id).copied().unwrap_or("?");
            if is_noise_name(obj_name) {
                continue;
            }
            // Generate pairs from subjects
            for i in 0..subjects.len().min(20) {
                for j in (i + 1)..subjects.len().min(20) {
                    if hypotheses.len() >= 40 {
                        break;
                    }
                    let a = subjects[i];
                    let b = subjects[j];
                    let pair = if a < b { (a, b) } else { (b, a) };
                    if existing.contains(&pair) || !seen.insert(pair) {
                        continue;
                    }
                    let a_name = id_name.get(&a).copied().unwrap_or("?");
                    let b_name = id_name.get(&b).copied().unwrap_or("?");
                    if is_noise_name(a_name) || is_noise_name(b_name) {
                        continue;
                    }
                    let a_type = id_type.get(&a).copied().unwrap_or("?");
                    let b_type = id_type.get(&b).copied().unwrap_or("?");
                    // Count how many objects they share (across all predicates)
                    let mut shared_count = 0usize;
                    for ((_, oid), subs) in &pred_obj_subjects {
                        if subs.contains(&a) && subs.contains(&b) {
                            shared_count += 1;
                            if *oid != *obj_id {
                                // Extra signal: multiple shared objects
                            }
                        }
                    }
                    let base_conf = (0.35 + 0.10 * shared_count as f64).min(0.75);
                    let confidence = self
                        .calibrated_confidence("shared_predicate_object", base_conf)
                        .unwrap_or(base_conf);
                    let inferred_pred = infer_predicate(a_type, b_type, None);
                    hypotheses.push(Hypothesis {
                        id: 0,
                        subject: a_name.to_string(),
                        predicate: inferred_pred.to_string(),
                        object: b_name.to_string(),
                        confidence,
                        evidence_for: vec![format!(
                            "Both {} '{}' → {} (shared {} predicate-object pairs)",
                            predicate, obj_name, predicate, shared_count
                        )],
                        evidence_against: vec![],
                        reasoning_chain: vec![
                            format!("{} → {} → {}", a_name, predicate, obj_name),
                            format!("{} → {} → {}", b_name, predicate, obj_name),
                            format!(
                                "Shared predicate-object implies {} relationship",
                                inferred_pred
                            ),
                        ],
                        status: HypothesisStatus::Proposed,
                        discovered_at: now_str(),
                        pattern_source: "shared_predicate_object".to_string(),
                    });
                }
                if hypotheses.len() >= 40 {
                    break;
                }
            }
        }

        hypotheses.truncate(40);
        Ok(hypotheses)
    }

    /// Overlap coefficient link prediction: identifies entity pairs where the
    /// smaller entity's neighborhood is largely contained in the larger's.
    /// This asymmetric measure catches "subset" relationships missed by Jaccard.
    pub fn generate_hypotheses_from_overlap_coefficient(&self) -> Result<Vec<Hypothesis>> {
        let predictions = crate::graph::overlap_coefficient_predict(self.brain, 60)?;
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        let mut hypotheses = Vec::new();
        for (a, b, score) in &predictions {
            if !meaningful.contains(a) || !meaningful.contains(b) {
                continue;
            }
            let a_name = id_name.get(a).copied().unwrap_or("?");
            let b_name = id_name.get(b).copied().unwrap_or("?");
            if is_noise_name(a_name) || is_noise_name(b_name) {
                continue;
            }
            let a_type = id_type.get(a).copied().unwrap_or("?");
            let b_type = id_type.get(b).copied().unwrap_or("?");
            // Skip concept-concept pairs (100% historical rejection for related_concept)
            if a_type == "concept" && b_type == "concept" {
                continue;
            }
            let predicate = infer_predicate(a_type, b_type, None);
            let base_conf = 0.35 + (score - 0.3) * 0.5; // 0.3→0.35, 1.0→0.70
            let confidence = self
                .calibrated_confidence("overlap_coefficient", base_conf)
                .unwrap_or(base_conf);

            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.to_string(),
                predicate: predicate.to_string(),
                object: b_name.to_string(),
                confidence,
                evidence_for: vec![format!(
                    "Overlap coefficient {:.3}: smaller neighborhood largely contained in larger",
                    score
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "{} and {} share significant neighbor overlap",
                        a_name, b_name
                    ),
                    format!("Overlap coefficient: {:.3} (asymmetric containment)", score),
                    "High overlap suggests hierarchical or associative relationship".to_string(),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "overlap_coefficient".to_string(),
            });
        }
        hypotheses.truncate(40);
        Ok(hypotheses)
    }

    /// Inverse relation inference: if A→pred→B exists, hypothesize B→inverse(pred)→A.
    /// Many predicates have natural inverses (e.g., "influenced" ↔ "influenced_by",
    /// "founded" ↔ "founded_by", "part_of" ↔ "contains").
    pub fn generate_hypotheses_from_inverse_relations(&self) -> Result<Vec<Hypothesis>> {
        let inverse_map: HashMap<&str, &str> = [
            ("influenced", "influenced_by"),
            ("influenced_by", "influenced"),
            ("founded", "founded_by"),
            ("founded_by", "founded"),
            ("part_of", "contains"),
            ("contains", "part_of"),
            ("taught", "studied_under"),
            ("studied_under", "taught"),
            ("preceded", "succeeded"),
            ("succeeded", "preceded"),
            ("parent_of", "child_of"),
            ("child_of", "parent_of"),
            ("employed_by", "employs"),
            ("employs", "employed_by"),
            ("mentor_of", "mentored_by"),
            ("mentored_by", "mentor_of"),
            ("discovered", "discovered_by"),
            ("discovered_by", "discovered"),
            ("invented", "invented_by"),
            ("invented_by", "invented"),
            ("collaborated_with", "collaborated_with"),
            ("defeated", "defeated_by"),
            ("defeated_by", "defeated"),
            ("inspired", "inspired_by"),
            ("inspired_by", "inspired"),
            ("based_in", "headquarters_of"),
            ("headquarters_of", "based_in"),
            // Additional inverses for higher coverage
            ("contributed_to", "received_contribution_from"),
            ("received_contribution_from", "contributed_to"),
            ("pioneered", "pioneered_by"),
            ("pioneered_by", "pioneered"),
            ("named_after", "namesake_of"),
            ("namesake_of", "named_after"),
            ("birthplace_of", "born_in"),
            ("born_in", "birthplace_of"),
            ("active_in", "home_to"),
            ("home_to", "active_in"),
            ("affiliated_with", "has_member"),
            ("has_member", "affiliated_with"),
            ("wrote", "written_by"),
            ("written_by", "wrote"),
            ("conquered", "conquered_by"),
            ("conquered_by", "conquered"),
            ("ruled", "ruled_by"),
            ("ruled_by", "ruled"),
            ("subclass_of", "superclass_of"),
            ("superclass_of", "subclass_of"),
            ("works_on", "worked_on_by"),
            ("worked_on_by", "works_on"),
            // Symmetric relations (A↔B implies B↔A)
            ("contemporary_of", "contemporary_of"),
            ("located_near", "located_near"),
            ("partner_of", "partner_of"),
            ("associated_with", "associated_with"),
            ("geographically_related_to", "geographically_related_to"),
        ]
        .iter()
        .copied()
        .collect();

        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();

        // Build existing relation set for dedup
        let existing: HashSet<(i64, String, i64)> = relations
            .iter()
            .map(|r| (r.subject_id, r.predicate.clone(), r.object_id))
            .collect();

        let mut hypotheses = Vec::new();
        let mut seen = HashSet::new();

        for r in &relations {
            if !meaningful.contains(&r.subject_id) || !meaningful.contains(&r.object_id) {
                continue;
            }
            let pred = r.predicate.as_str();
            if let Some(&inverse) = inverse_map.get(pred) {
                // Check if inverse relation already exists
                if existing.contains(&(r.object_id, inverse.to_string(), r.subject_id)) {
                    continue;
                }
                let pair = (
                    r.object_id.min(r.subject_id),
                    r.object_id.max(r.subject_id),
                    inverse,
                );
                if !seen.insert(pair) {
                    continue;
                }

                let subj_name = id_name.get(&r.object_id).copied().unwrap_or("?");
                let obj_name = id_name.get(&r.subject_id).copied().unwrap_or("?");
                if is_noise_name(subj_name) || is_noise_name(obj_name) {
                    continue;
                }

                let base_conf = 0.65 + r.confidence * 0.2; // Inverse relations are high-confidence
                let confidence = self
                    .calibrated_confidence("inverse_relation", base_conf)
                    .unwrap_or(base_conf);

                hypotheses.push(Hypothesis {
                    id: 0,
                    subject: subj_name.to_string(),
                    predicate: inverse.to_string(),
                    object: obj_name.to_string(),
                    confidence,
                    evidence_for: vec![format!(
                        "Inverse of existing relation: {} {} {}",
                        obj_name, pred, subj_name
                    )],
                    evidence_against: vec![],
                    reasoning_chain: vec![
                        format!("Known: {} {} {}", obj_name, pred, subj_name),
                        format!("'{}' implies inverse '{}'", pred, inverse),
                        "Bidirectional relations strengthen graph connectivity".to_string(),
                    ],
                    status: HypothesisStatus::Proposed,
                    discovered_at: now_str(),
                    pattern_source: "inverse_relation".to_string(),
                });
            }
        }
        hypotheses.truncate(150);
        Ok(hypotheses)
    }

    /// Co-fact hypothesis: entities sharing the same (key, value) fact pairs are
    /// likely related. For example, if both "Alan Turing" and "Bletchley Park"
    /// have fact (location, "England"), they may be connected.
    pub fn generate_hypotheses_from_co_facts(&self) -> Result<Vec<Hypothesis>> {
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Build fact-value → entity index
        let mut fact_groups: HashMap<(String, String), Vec<i64>> = HashMap::new();
        self.brain.with_conn(|conn| {
            let mut stmt =
                conn.prepare("SELECT entity_id, key, value FROM facts WHERE confidence >= 0.5")?;
            let rows = stmt.query_map([], |row| {
                Ok((
                    row.get::<_, i64>(0)?,
                    row.get::<_, String>(1)?,
                    row.get::<_, String>(2)?,
                ))
            })?;
            for row in rows {
                let (eid, key, value) = row?;
                if meaningful.contains(&eid)
                    && !key.is_empty()
                    && !value.is_empty()
                    && value.len() > 2
                {
                    // Skip overly generic fact keys
                    let generic_keys = ["type", "description", "name", "source", "url"];
                    if !generic_keys.contains(&key.as_str()) {
                        fact_groups.entry((key, value)).or_default().push(eid);
                    }
                }
            }
            Ok(())
        })?;

        // Build existing relation set
        let relations = self.brain.all_relations()?;
        let connected: HashSet<(i64, i64)> = relations
            .iter()
            .flat_map(|r| [(r.subject_id, r.object_id), (r.object_id, r.subject_id)])
            .collect();

        let mut hypotheses = Vec::new();
        let mut seen = HashSet::new();

        for ((key, value), eids) in &fact_groups {
            if eids.len() < 2 || eids.len() > 20 {
                continue; // Skip too-small or too-common groups
            }
            for i in 0..eids.len().min(10) {
                for j in (i + 1)..eids.len().min(10) {
                    let a = eids[i];
                    let b = eids[j];
                    if connected.contains(&(a, b)) {
                        continue;
                    }
                    let pair = (a.min(b), a.max(b));
                    if !seen.insert(pair) {
                        continue;
                    }
                    let a_name = id_name.get(&a).copied().unwrap_or("?");
                    let b_name = id_name.get(&b).copied().unwrap_or("?");
                    if is_noise_name(a_name) || is_noise_name(b_name) {
                        continue;
                    }
                    let a_type = id_type.get(&a).copied().unwrap_or("?");
                    let b_type = id_type.get(&b).copied().unwrap_or("?");
                    let predicate = infer_predicate(a_type, b_type, None);

                    let base_conf = 0.40 + (1.0 / eids.len() as f64) * 0.2; // Rarer shared facts = higher conf
                    let confidence = self
                        .calibrated_confidence("co_fact", base_conf)
                        .unwrap_or(base_conf);

                    hypotheses.push(Hypothesis {
                        id: 0,
                        subject: a_name.to_string(),
                        predicate: predicate.to_string(),
                        object: b_name.to_string(),
                        confidence,
                        evidence_for: vec![format!(
                            "Shared fact: {}={} (group size {})",
                            key,
                            value,
                            eids.len()
                        )],
                        evidence_against: vec![],
                        reasoning_chain: vec![
                            format!("{} and {} both have fact {}={}", a_name, b_name, key, value),
                            "Shared factual attributes suggest a real-world connection".to_string(),
                        ],
                        status: HypothesisStatus::Proposed,
                        discovered_at: now_str(),
                        pattern_source: "co_fact".to_string(),
                    });
                }
            }
        }
        hypotheses.truncate(50);
        Ok(hypotheses)
    }

    /// Predicate covariance gaps: if predicates A and B co-occur frequently
    /// (high Jaccard) and an entity has A but not B, hypothesize B should exist.
    /// Leverages graph::predicate_gap_entities for efficient detection.
    pub fn generate_hypotheses_from_predicate_covariance(&self) -> Result<Vec<Hypothesis>> {
        let gaps = crate::graph::predicate_gap_entities(self.brain, 0.3, 200)?;
        let meaningful = meaningful_ids(self.brain)?;
        let mut hypotheses = Vec::new();

        for (eid, ename, missing_pred, has_pred, jaccard) in &gaps {
            if !meaningful.contains(eid) {
                continue;
            }
            if is_noise_name(ename) || is_generic_predicate(missing_pred) {
                continue;
            }
            let base_conf = (0.30 + jaccard * 0.4).min(0.70);
            let confidence = self
                .calibrated_confidence("predicate_covariance", base_conf)
                .unwrap_or(base_conf);

            hypotheses.push(Hypothesis {
                id: 0,
                subject: ename.clone(),
                predicate: missing_pred.clone(),
                object: "?".to_string(),
                confidence,
                evidence_for: vec![format!(
                    "Has '{}' but missing '{}' (predicate covariance Jaccard={:.2})",
                    has_pred, missing_pred, jaccard
                )],
                evidence_against: vec![],
                reasoning_chain: vec![
                    format!(
                        "Predicates '{}' and '{}' co-occur with J={:.2}",
                        has_pred, missing_pred, jaccard
                    ),
                    format!(
                        "{} has '{}' → expect '{}' too",
                        ename, has_pred, missing_pred
                    ),
                ],
                status: HypothesisStatus::Proposed,
                discovered_at: now_str(),
                pattern_source: "predicate_covariance".to_string(),
            });
            if hypotheses.len() >= 30 {
                break;
            }
        }
        Ok(hypotheses)
    }

    /// Multi-hop transitive inference: if A→p1→B and B→p2→C exist but A↛C,
    /// hypothesize A→inferred_pred→C.  Focuses on high-value entity types and
    /// avoids noise by requiring both endpoints to be meaningful.
    pub fn generate_hypotheses_from_multi_hop(&self) -> Result<Vec<Hypothesis>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let id_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Build adjacency: subject → [(object, predicate)]
        let mut outgoing: HashMap<i64, Vec<(i64, &str)>> = HashMap::new();
        let mut connected_pairs: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            outgoing
                .entry(r.subject_id)
                .or_default()
                .push((r.object_id, &r.predicate));
            outgoing
                .entry(r.object_id)
                .or_default()
                .push((r.subject_id, &r.predicate));
            connected_pairs.insert((r.subject_id, r.object_id));
            connected_pairs.insert((r.object_id, r.subject_id));
        }

        let mut hypotheses = Vec::new();
        let mut seen: HashSet<(i64, i64)> = HashSet::new();

        // Sample high-value entities as starting points (limit computation)
        let high_value: Vec<i64> = entities
            .iter()
            .filter(|e| {
                meaningful.contains(&e.id)
                    && HIGH_VALUE_TYPES.contains(&e.entity_type.as_str())
                    && e.confidence >= 0.7
            })
            .map(|e| e.id)
            .collect();

        for &a in high_value.iter().take(500) {
            let a_neighbors = match outgoing.get(&a) {
                Some(ns) => ns,
                None => continue,
            };
            for &(b, p1) in a_neighbors.iter().take(20) {
                if !meaningful.contains(&b) {
                    continue;
                }
                let b_neighbors = match outgoing.get(&b) {
                    Some(ns) => ns,
                    None => continue,
                };
                for &(c, p2) in b_neighbors.iter().take(20) {
                    if c == a || !meaningful.contains(&c) {
                        continue;
                    }
                    if connected_pairs.contains(&(a, c)) {
                        continue;
                    }
                    if seen.contains(&(a, c)) || seen.contains(&(c, a)) {
                        continue;
                    }
                    let a_name = id_name.get(&a).copied().unwrap_or("?");
                    let c_name = id_name.get(&c).copied().unwrap_or("?");
                    if is_noise_name(a_name) || is_noise_name(c_name) {
                        continue;
                    }
                    if is_generic_predicate(p1) || is_generic_predicate(p2) {
                        continue;
                    }
                    let a_type = id_type.get(&a).copied().unwrap_or("?");
                    let c_type = id_type.get(&c).copied().unwrap_or("?");
                    if !types_compatible(a_type, c_type) {
                        continue;
                    }

                    let b_name = id_name.get(&b).copied().unwrap_or("?");
                    let predicate = infer_predicate(a_type, c_type, None);
                    let base_conf = 0.40;
                    let confidence = self
                        .calibrated_confidence("multi_hop", base_conf)
                        .unwrap_or(base_conf);

                    seen.insert((a, c));
                    hypotheses.push(Hypothesis {
                        id: 0,
                        subject: a_name.to_string(),
                        predicate: predicate.to_string(),
                        object: c_name.to_string(),
                        confidence,
                        evidence_for: vec![format!(
                            "Transitive: {}→{}→{} via {} ({}→{}, {}→{})",
                            a_name, b_name, c_name, b_name, p1, b_name, p2, c_name
                        )],
                        evidence_against: vec![],
                        reasoning_chain: vec![
                            format!("{} --[{}]--> {}", a_name, p1, b_name),
                            format!("{} --[{}]--> {}", b_name, p2, c_name),
                            format!(
                                "No direct link {} ↔ {} — infer transitive {}",
                                a_name, c_name, predicate
                            ),
                        ],
                        status: HypothesisStatus::Proposed,
                        discovered_at: now_str(),
                        pattern_source: "multi_hop".to_string(),
                    });
                    if hypotheses.len() >= 50 {
                        return Ok(hypotheses);
                    }
                }
            }
        }
        hypotheses.truncate(50);
        Ok(hypotheses)
    }

    /// Strategy: Structural similarity link prediction.
    /// Finds entity pairs with highly overlapping typed neighborhoods (same predicates
    /// pointing to same neighbors) that are NOT directly connected.
    /// Uses cosine similarity on predicate-typed neighbor vectors from graph.rs.
    pub fn generate_hypotheses_from_structural_similarity(&self) -> Result<Vec<Hypothesis>> {
        let pairs = crate::graph::structural_similarity_pairs(self.brain, 3, 0.20, 400)?;
        let meaningful = meaningful_ids(self.brain)?;
        let entities = self.brain.all_entities()?;
        let name_to_type: HashMap<String, String> = entities
            .iter()
            .map(|e| (e.name.clone(), e.entity_type.clone()))
            .collect();

        let mut hypotheses = Vec::new();
        let mut seen: HashSet<(String, String)> = HashSet::new();

        for (id_a, name_a, id_b, name_b, sim) in &pairs {
            if !meaningful.contains(id_a) || !meaningful.contains(id_b) {
                continue;
            }
            if is_noise_name(name_a) || is_noise_name(name_b) {
                continue;
            }
            let key = if name_a < name_b {
                (name_a.clone(), name_b.clone())
            } else {
                (name_b.clone(), name_a.clone())
            };
            if seen.contains(&key) {
                continue;
            }
            seen.insert(key);

            let a_type = name_to_type.get(name_a).map(|s| s.as_str()).unwrap_or("?");
            let b_type = name_to_type.get(name_b).map(|s| s.as_str()).unwrap_or("?");
            if !types_compatible(a_type, b_type) {
                continue;
            }
            // Skip concept-concept (100% historical rejection)
            if a_type == "concept" && b_type == "concept" {
                continue;
            }

            let predicate = infer_predicate(a_type, b_type, None);
            let base_conf = 0.30 + (sim - 0.20) * 0.45; // 0.30 at min sim, up to ~0.66
            let confidence = self
                .calibrated_confidence("structural_similarity", base_conf)
                .unwrap_or(base_conf);

            hypotheses.push(Hypothesis {
                id: 0,
                subject: name_a.clone(),
                predicate: predicate.to_string(),
                object: name_b.clone(),
                confidence,
                evidence_for: vec![format!(
                    "Structural similarity {:.3}: share typed neighborhood patterns",
                    sim
                )],
                evidence_against: vec![],
                reasoning_chain: vec![format!(
                    "{} and {} have overlapping predicate-neighbor profiles (cosine+jaccard={:.3})",
                    name_a, name_b, sim
                )],
                status: HypothesisStatus::Testing,
                discovered_at: now_str(),
                pattern_source: "structural_similarity".to_string(),
            });
        }

        hypotheses.truncate(50);
        Ok(hypotheses)
    }

    /// Strategy: Fact-gap — find entities with many facts but few relations.
    /// These are knowledge-rich nodes that the graph hasn't connected yet.
    /// Pair them with entities sharing fact key-value pairs to surface hidden links.
    pub fn generate_hypotheses_from_fact_gaps(&self) -> Result<Vec<Hypothesis>> {
        let meaningful = meaningful_ids(self.brain)?;
        let entities = self.brain.all_entities()?;
        let name_map: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let type_map: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Find entities with ≥2 facts but ≤1 relation (fact-rich, relation-poor)
        let fact_rich: Vec<(i64, Vec<(String, String)>)> = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT f.entity_id, f.key, f.value FROM facts f
                 JOIN entities e ON e.id = f.entity_id
                 WHERE f.entity_id IN (
                     SELECT entity_id FROM facts GROUP BY entity_id HAVING COUNT(*) >= 2
                 )
                 AND (SELECT COUNT(*) FROM relations WHERE subject_id = f.entity_id OR object_id = f.entity_id) <= 1
                 ORDER BY f.entity_id"
            )?;
            let rows = stmt.query_map([], |row| {
                Ok((row.get::<_, i64>(0)?, row.get::<_, String>(1)?, row.get::<_, String>(2)?))
            })?;
            let mut grouped: HashMap<i64, Vec<(String, String)>> = HashMap::new();
            for r in rows.filter_map(|r| r.ok()) {
                grouped.entry(r.0).or_default().push((r.1, r.2));
            }
            Ok(grouped.into_iter().collect::<Vec<_>>())
        })?;

        if fact_rich.is_empty() {
            return Ok(Vec::new());
        }

        // Build inverted index: (key, value) → entity ids
        let mut kv_index: HashMap<(String, String), Vec<i64>> = HashMap::new();
        for (eid, facts) in &fact_rich {
            for (k, v) in facts {
                kv_index
                    .entry((k.clone(), v.clone()))
                    .or_default()
                    .push(*eid);
            }
        }
        // Also add well-connected entities' facts to the index
        let connected_facts: Vec<(i64, String, String)> = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT f.entity_id, f.key, f.value FROM facts f
                 WHERE f.entity_id IN (
                     SELECT entity_id FROM facts GROUP BY entity_id HAVING COUNT(*) >= 2
                 )
                 AND (SELECT COUNT(*) FROM relations WHERE subject_id = f.entity_id OR object_id = f.entity_id) >= 3"
            )?;
            let rows = stmt.query_map([], |row| {
                Ok((row.get::<_, i64>(0)?, row.get::<_, String>(1)?, row.get::<_, String>(2)?))
            })?;
            Ok(rows.filter_map(|r| r.ok()).collect::<Vec<_>>())
        })?;
        for (eid, k, v) in &connected_facts {
            kv_index
                .entry((k.clone(), v.clone()))
                .or_default()
                .push(*eid);
        }

        let mut hypotheses = Vec::new();
        let mut seen: HashSet<(String, String)> = HashSet::new();

        for (eid, facts) in &fact_rich {
            if !meaningful.contains(eid) {
                continue;
            }
            let name = match name_map.get(eid) {
                Some(n) => *n,
                None => continue,
            };
            if is_noise_name(name) {
                continue;
            }
            let etype = type_map.get(eid).copied().unwrap_or("?");
            // Skip single-word entities (too ambiguous for fact-gap inference)
            if !name.contains(' ') && name.len() < 10 {
                continue;
            }

            // Find entities sharing fact key-value pairs
            let mut candidates: HashMap<i64, usize> = HashMap::new();
            for (k, v) in facts {
                if let Some(others) = kv_index.get(&(k.clone(), v.clone())) {
                    for &other_id in others {
                        if other_id != *eid {
                            *candidates.entry(other_id).or_insert(0) += 1;
                        }
                    }
                }
            }

            // Pick top candidate by shared facts
            let mut sorted: Vec<_> = candidates.into_iter().collect();
            sorted.sort_by(|a, b| b.1.cmp(&a.1));

            for (other_id, shared_count) in sorted.into_iter().take(2) {
                // Require ≥2 shared facts to reduce noise (single-fact overlap is weak signal)
                if shared_count < 2 {
                    continue;
                }
                let other_name = match name_map.get(&other_id) {
                    Some(n) => *n,
                    None => continue,
                };
                if is_noise_name(other_name) {
                    continue;
                }
                if !other_name.contains(' ') && other_name.len() < 10 {
                    continue;
                }
                let other_type = type_map.get(&other_id).copied().unwrap_or("?");
                if !types_compatible(etype, other_type) {
                    continue;
                }
                let key = if name < other_name {
                    (name.to_string(), other_name.to_string())
                } else {
                    (other_name.to_string(), name.to_string())
                };
                if seen.contains(&key) {
                    continue;
                }
                seen.insert(key);

                let predicate = infer_predicate(etype, other_type, None);
                let base_conf = 0.45 + 0.10 * (shared_count as f64).min(3.0);
                let confidence = self
                    .calibrated_confidence("fact_gap", base_conf)
                    .unwrap_or(base_conf);
                // Skip low-confidence fact_gap hypotheses — historically ~59% confirmation
                // rate means we should only emit higher-quality candidates
                if confidence < 0.45 {
                    continue;
                }

                hypotheses.push(Hypothesis {
                    id: 0,
                    subject: name.to_string(),
                    predicate: predicate.to_string(),
                    object: other_name.to_string(),
                    confidence,
                    evidence_for: vec![format!(
                        "Share {} fact key-value pairs; {} is fact-rich but relation-poor",
                        shared_count, name
                    )],
                    evidence_against: vec![],
                    reasoning_chain: vec![format!(
                        "Fact-gap: {} has {} facts but ≤1 relation; shares facts with {}",
                        name,
                        facts.len(),
                        other_name
                    )],
                    status: HypothesisStatus::Testing,
                    discovered_at: now_str(),
                    pattern_source: "fact_gap".to_string(),
                });
            }

            if hypotheses.len() >= 60 {
                break;
            }
        }

        hypotheses.truncate(60);
        Ok(hypotheses)
    }

    /// Strategy: Open triangle completion — find pairs of entities (A, C) that share
    /// multiple intermediary neighbors B but lack a direct connection.  This implements
    /// the strong triadic closure principle: if A-B and B-C are strong ties,
    /// then A-C should exist.  Uses the graph-level `open_triangle_predict` for
    /// efficient enumeration of candidate pairs.
    pub fn generate_hypotheses_from_open_triangles(&self) -> Result<Vec<Hypothesis>> {
        let predictions = crate::graph::open_triangle_predict(self.brain, 2, 200)?;

        if predictions.is_empty() {
            return Ok(Vec::new());
        }

        let entities = self.brain.all_entities()?;
        let name_map: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let type_map: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();
        let meaningful = meaningful_ids(self.brain)?;

        let mut hypotheses = Vec::new();
        let mut seen: HashSet<(String, String)> = HashSet::new();

        for (a_id, c_id, shared_count, weight) in &predictions {
            if !meaningful.contains(a_id) && !meaningful.contains(c_id) {
                continue;
            }
            let a_name = match name_map.get(a_id) {
                Some(n) => *n,
                None => continue,
            };
            let c_name = match name_map.get(c_id) {
                Some(n) => *n,
                None => continue,
            };
            if is_noise_name(a_name) || is_noise_name(c_name) {
                continue;
            }
            let a_type = type_map.get(a_id).copied().unwrap_or("?");
            let c_type = type_map.get(c_id).copied().unwrap_or("?");
            if !types_compatible(a_type, c_type) {
                continue;
            }
            let key = if a_name < c_name {
                (a_name.to_string(), c_name.to_string())
            } else {
                (c_name.to_string(), a_name.to_string())
            };
            if seen.contains(&key) {
                continue;
            }
            seen.insert(key);

            let predicate = infer_predicate(a_type, c_type, None);
            // Base confidence scales with intermediary count: 3→0.55, 5→0.65, 10→0.75
            let base_conf = (0.45 + 0.05 * (*shared_count as f64).min(6.0)).min(0.80);
            let confidence = self
                .calibrated_confidence("open_triangle", base_conf)
                .unwrap_or(base_conf);

            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.to_string(),
                predicate: predicate.to_string(),
                object: c_name.to_string(),
                confidence,
                evidence_for: vec![format!(
                    "{} shared intermediary neighbors (weight {:.3})",
                    shared_count, weight
                )],
                evidence_against: vec![],
                reasoning_chain: vec![format!(
                    "Open triangle: {} and {} share {} neighbors but have no direct link",
                    a_name, c_name, shared_count
                )],
                status: HypothesisStatus::Testing,
                discovered_at: now_str(),
                pattern_source: "open_triangle".to_string(),
            });

            if hypotheses.len() >= 80 {
                break;
            }
        }

        Ok(hypotheses)
    }

    /// Strategy: Heat-kernel diffusion link prediction.
    /// Uses random walk diffusion to find entity pairs that are "close" in the
    /// graph's diffusion geometry but not directly connected. Works well in sparse
    /// graphs where common-neighbor methods fail due to low overlap.
    pub fn generate_hypotheses_from_heat_kernel(&self) -> Result<Vec<Hypothesis>> {
        let predictions = crate::graph::heat_kernel_predict(self.brain, 2.0, 200)?;
        if predictions.is_empty() {
            return Ok(Vec::new());
        }

        let entities = self.brain.all_entities()?;
        let name_map: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();
        let type_map: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();
        let meaningful = meaningful_ids(self.brain)?;

        let mut hypotheses = Vec::new();
        let mut seen: HashSet<(String, String)> = HashSet::new();

        // Normalize scores: use max score for relative confidence
        let max_score = predictions.iter().map(|p| p.2).fold(0.0_f64, f64::max);
        if max_score <= 0.0 {
            return Ok(Vec::new());
        }

        for (a_id, b_id, score) in &predictions {
            if !meaningful.contains(a_id) && !meaningful.contains(b_id) {
                continue;
            }
            let a_name = match name_map.get(a_id) {
                Some(n) => *n,
                None => continue,
            };
            let b_name = match name_map.get(b_id) {
                Some(n) => *n,
                None => continue,
            };
            if is_noise_name(a_name) || is_noise_name(b_name) {
                continue;
            }
            let a_type = type_map.get(a_id).copied().unwrap_or("?");
            let b_type = type_map.get(b_id).copied().unwrap_or("?");
            if !types_compatible(a_type, b_type) {
                continue;
            }
            let key = if a_name < b_name {
                (a_name.to_string(), b_name.to_string())
            } else {
                (b_name.to_string(), a_name.to_string())
            };
            if seen.contains(&key) {
                continue;
            }
            seen.insert(key);

            let predicate = infer_predicate(a_type, b_type, None);
            let norm_score = score / max_score;
            let base_conf = (0.40 + 0.35 * norm_score).min(0.75);
            let confidence = self
                .calibrated_confidence("heat_kernel", base_conf)
                .unwrap_or(base_conf);

            hypotheses.push(Hypothesis {
                id: 0,
                subject: a_name.to_string(),
                predicate: predicate.to_string(),
                object: b_name.to_string(),
                confidence,
                evidence_for: vec![format!(
                    "Heat-kernel diffusion score: {:.3} (normalized: {:.3})",
                    score, norm_score
                )],
                evidence_against: vec![],
                reasoning_chain: vec![format!(
                    "Diffusion proximity: {} and {} are close in random-walk geometry despite no direct link",
                    a_name, b_name
                )],
                status: HypothesisStatus::Testing,
                discovered_at: now_str(),
                pattern_source: "heat_kernel".to_string(),
            });

            if hypotheses.len() >= 60 {
                break;
            }
        }

        Ok(hypotheses)
    }

    /// Strategy: Hypothesis recycling — re-evaluate previously rejected hypotheses
    /// from high-performing strategies. The graph may have grown since rejection,
    /// providing new evidence that could flip the verdict.
    pub fn recycle_rejected_hypotheses(&self) -> Result<(usize, usize)> {
        // Only recycle from strategies with > 70% confirmation rate
        let weights: Vec<(String, f64)> = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT pattern_type, weight FROM pattern_weights WHERE confirmations > 20 AND weight > 0.7"
            )?;
            let rows = stmt.query_map([], |row| {
                Ok((row.get::<_, String>(0)?, row.get::<_, f64>(1)?))
            })?;
            Ok(rows.filter_map(|r| r.ok()).collect::<Vec<_>>())
        })?;

        if weights.is_empty() {
            return Ok((0, 0));
        }

        let strategy_names: HashSet<String> = weights.iter().map(|(n, _)| n.clone()).collect();

        // Find rejected hypotheses from these strategies, rejected > 3 days ago
        let cutoff = (Utc::now() - chrono::Duration::days(3))
            .format("%Y-%m-%d %H:%M:%S")
            .to_string();
        let candidates: Vec<(i64, String, String, String, f64, String)> =
            self.brain.with_conn(|conn| {
                let mut stmt = conn.prepare(
                    "SELECT id, subject, predicate, object, confidence, pattern_source \
                 FROM hypotheses WHERE status = 'rejected' AND discovered_at < ?1 \
                 ORDER BY confidence DESC LIMIT 100",
                )?;
                let rows = stmt.query_map(params![cutoff], |row| {
                    Ok((
                        row.get::<_, i64>(0)?,
                        row.get::<_, String>(1)?,
                        row.get::<_, String>(2)?,
                        row.get::<_, String>(3)?,
                        row.get::<_, f64>(4)?,
                        row.get::<_, String>(5)?,
                    ))
                })?;
                Ok(rows
                    .filter_map(|r| r.ok())
                    .filter(|(_, _, _, _, _, src)| strategy_names.contains(src))
                    .collect::<Vec<_>>())
            })?;

        let mut recycled = 0usize;
        let mut promoted = 0usize;

        for (id, subject, predicate, object, old_conf, source) in &candidates {
            // Re-validate: check if new evidence exists
            let mut h = Hypothesis {
                id: *id,
                subject: subject.clone(),
                predicate: predicate.clone(),
                object: object.clone(),
                confidence: 0.30, // reset to base — must earn its way back
                evidence_for: vec![format!("Recycled from {} (was {:.2})", source, old_conf)],
                evidence_against: vec![],
                reasoning_chain: vec![],
                status: HypothesisStatus::Testing,
                discovered_at: now_str(),
                pattern_source: source.clone(),
            };

            self.validate_hypothesis(&mut h)?;

            if h.confidence >= CONFIRMATION_THRESHOLD {
                // New evidence confirms — promote!
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE hypotheses SET status = 'confirmed', confidence = ?1 WHERE id = ?2",
                        params![h.confidence, id],
                    )?;
                    Ok(())
                })?;
                self.record_outcome(source, true)?;
                promoted += 1;
                recycled += 1;
            } else if h.confidence >= 0.45 {
                // Promising but not confirmed — put back to testing
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE hypotheses SET status = 'testing', confidence = ?1 WHERE id = ?2",
                        params![h.confidence, id],
                    )?;
                    Ok(())
                })?;
                recycled += 1;
            }
            // Otherwise leave as rejected
        }

        Ok((recycled, promoted))
    }

    /// Compute surprise/novelty score for a hypothesis.
    /// Cross-domain hypotheses (different entity types, different communities) get higher scores.
    /// Returns value 0.0 (expected) to 1.0 (highly surprising).
    pub fn novelty_score(&self, h: &Hypothesis) -> Result<f64> {
        let entities = self.brain.all_entities()?;
        let name_to_ent: HashMap<String, &crate::db::Entity> = entities
            .iter()
            .map(|e| (e.name.to_lowercase(), e))
            .collect();

        let mut score = 0.0;

        let s_ent = name_to_ent.get(&h.subject.to_lowercase());
        let o_ent = name_to_ent.get(&h.object.to_lowercase());

        // Different entity types → more surprising
        if let (Some(s), Some(o)) = (s_ent, o_ent) {
            if s.entity_type != o.entity_type {
                score += 0.3;
            }
            // Check if in different communities
            let communities = crate::graph::louvain_communities(self.brain)?;
            if let (Some(&cs), Some(&co)) = (communities.get(&s.id), communities.get(&o.id)) {
                if cs != co {
                    score += 0.3;
                }
            }
            // Low shared-neighbor count → more surprising
            let all_rels = self.brain.all_relations()?;
            let s_rels: HashSet<i64> = all_rels
                .iter()
                .filter(|r| r.subject_id == s.id || r.object_id == s.id)
                .flat_map(|r| [r.subject_id, r.object_id])
                .collect();
            let o_rels: HashSet<i64> = all_rels
                .iter()
                .filter(|r| r.subject_id == o.id || r.object_id == o.id)
                .flat_map(|r| [r.subject_id, r.object_id])
                .collect();
            let shared = s_rels.intersection(&o_rels).count();
            if shared == 0 {
                score += 0.3;
            } else if shared <= 2 {
                score += 0.1;
            }
        }

        // Novel predicate (rare in the graph) → more surprising
        let pred_count: i64 = self.brain.with_conn(|conn| {
            conn.query_row(
                "SELECT COUNT(*) FROM relations WHERE predicate = ?1",
                params![h.predicate],
                |row| row.get(0),
            )
        })?;
        let total_rels: i64 = self.brain.with_conn(|conn| {
            conn.query_row("SELECT COUNT(*) FROM relations", [], |row| row.get(0))
        })?;
        if total_rels > 0 {
            let pred_freq = pred_count as f64 / total_rels as f64;
            if pred_freq < 0.01 {
                score += 0.1; // rare predicate
            }
        }

        Ok((score as f64).min(1.0))
    }

    /// Aggressively purge single-word island entities that are clearly generic English words,
    /// citation surnames, adverbs, or adjectives. These entities have zero relations and
    /// zero facts — they contribute nothing to the knowledge graph.
    ///
    /// Heuristic: a single-word island is "generic" if it matches common English word patterns
    /// Merge connected entities where one name fully contains the other.
    /// Targets patterns like "Byzantine Empire Christianity" connected to "Byzantine Empire"
    /// — the longer form is usually a sentence-fragment extraction, not a real entity.
    /// Only merges when the shorter form has higher or equal degree and names share the same type.
    /// Returns count of merges performed.
    pub fn merge_connected_containment(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        // Build adjacency and degree
        let mut adj: HashSet<(i64, i64)> = HashSet::new();
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            adj.insert((r.subject_id, r.object_id));
            adj.insert((r.object_id, r.subject_id));
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build name→entity index for meaningful entities only
        let meaningful: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| !is_noise_type(&e.entity_type) && e.name.len() >= 3)
            .collect();

        // Index by first word for efficient lookup
        let mut by_first_word: HashMap<String, Vec<&crate::db::Entity>> = HashMap::new();
        for e in &meaningful {
            if let Some(first) = e.name.split_whitespace().next() {
                by_first_word
                    .entry(first.to_lowercase())
                    .or_default()
                    .push(e);
            }
        }

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        for e in &meaningful {
            if absorbed.contains(&e.id) {
                continue;
            }
            let e_words: Vec<&str> = e.name.split_whitespace().collect();
            if e_words.len() < 2 {
                continue;
            }
            let first_lower = e_words[0].to_lowercase();
            // Find potential shorter forms that this entity's name contains
            if let Some(candidates) = by_first_word.get(&first_lower) {
                for &cand in candidates {
                    if cand.id == e.id || absorbed.contains(&cand.id) {
                        continue;
                    }
                    // Check: is one name contained in the other?
                    let (shorter, longer) = if cand.name.len() < e.name.len() {
                        (cand, *e)
                    } else if cand.name.len() > e.name.len() {
                        (*e, cand)
                    } else {
                        continue;
                    };
                    let shorter_lower = shorter.name.to_lowercase();
                    let longer_lower = longer.name.to_lowercase();
                    // The longer name must start with or end with the shorter name
                    if !longer_lower.starts_with(&shorter_lower)
                        && !longer_lower.ends_with(&shorter_lower)
                    {
                        continue;
                    }
                    // Must be same type, or shorter has 3x+ degree (NLP mistype)
                    let short_deg = degree.get(&shorter.id).copied().unwrap_or(0);
                    let long_deg = degree.get(&longer.id).copied().unwrap_or(0);
                    if shorter.entity_type != longer.entity_type && short_deg < long_deg.max(1) * 3
                    {
                        continue;
                    }
                    // Must be connected (directly related)
                    let connected_pair = adj.contains(&(shorter.id, longer.id));
                    if !connected_pair {
                        continue;
                    }
                    // Shorter form should have >= degree of longer form
                    if short_deg < long_deg {
                        continue;
                    }
                    // The extra words in the longer name should be "noise-like"
                    let shorter_words: Vec<&str> = shorter.name.split_whitespace().collect();
                    let longer_words: Vec<&str> = longer.name.split_whitespace().collect();
                    let extra_words: Vec<&&str> = longer_words
                        .iter()
                        .filter(|w| !shorter_words.contains(w))
                        .collect();
                    // At least one extra word, and all extra words should be short or generic
                    if extra_words.is_empty() {
                        continue;
                    }

                    // Merge longer into shorter
                    self.brain.merge_entities(longer.id, shorter.id)?;
                    absorbed.insert(longer.id);
                    merged += 1;
                }
            }
        }
        Ok(merged)
    }

    /// Aggressive same-type prefix deduplication: merge entities whose name starts
    /// with a known shorter entity name of the SAME type. Unlike suffix_strip which
    /// only handles islands, this works on all entities and doesn't require the
    /// extra words to be in a noise list. For example:
    /// - "Byzantine Empire Diocletian" (concept) → "Byzantine Empire" (concept)
    /// - "Emmy Noether APSNews" (person) → "Emmy Noether" (person)
    /// - "Ada Lovelace WIRED" (person) → "Ada Lovelace" (person)
    /// Only merges when the target (shorter name) has strictly more connections.
    pub fn aggressive_prefix_dedup(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build (lowercase_name → (id, degree, word_count, type)) for entities with 2+ words
        let mut name_index: HashMap<String, (i64, usize, usize, String)> = HashMap::new();
        for e in &entities {
            if is_noise_type(&e.entity_type) {
                continue;
            }
            let lower = e.name.to_lowercase();
            let wc = lower.split_whitespace().count();
            if wc < 2 {
                continue;
            }
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            let existing = name_index.get(&lower);
            if existing.is_none() || existing.is_some_and(|(_, d, _, _)| deg > *d) {
                name_index.insert(lower, (e.id, deg, wc, e.entity_type.clone()));
            }
        }

        // For each entity with 3+ words, check if a 2-word prefix exists with same type
        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        for e in &entities {
            if absorbed.contains(&e.id) || is_noise_type(&e.entity_type) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 3 {
                continue;
            }
            let my_deg = degree.get(&e.id).copied().unwrap_or(0);

            // Try progressively shorter prefixes (keep at least 2 words)
            for take in (2..words.len()).rev() {
                let prefix: String = words[..take].join(" ").to_lowercase();
                if let Some(&(target_id, target_deg, _, ref target_type)) = name_index.get(&prefix)
                {
                    if target_id == e.id || absorbed.contains(&target_id) {
                        continue;
                    }
                    // Type check: same type required, OR target has 3x+ degree
                    // (high-degree target with matching prefix = NLP mistyped variant)
                    if target_type != &e.entity_type && target_deg < my_deg.max(1) * 3 {
                        continue;
                    }
                    // Target must be more connected (canonical)
                    if target_deg <= my_deg && my_deg > 0 {
                        continue;
                    }
                    self.brain.merge_entities(e.id, target_id)?;
                    absorbed.insert(e.id);
                    merged += 1;
                    break;
                }
            }
        }
        Ok(merged)
    }

    /// Merge any entity whose name is "KnownEntity + suffix" into KnownEntity,
    /// regardless of type or connectivity, when KnownEntity has 5x+ the degree.
    /// This catches NLP artifacts like "Ada Lovelace WIRED", "Byzantine Empire Rhomaioi" etc.
    pub fn merge_high_confidence_prefix_variants(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build index: lowercase name → (id, degree) for entities with 2+ words and degree >= 5
        let mut canonical: HashMap<String, (i64, usize)> = HashMap::new();
        for e in &entities {
            if is_noise_type(&e.entity_type) {
                continue;
            }
            let wc = e.name.split_whitespace().count();
            if wc < 2 {
                continue;
            }
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            if deg < 5 {
                continue;
            }
            let lower = e.name.to_lowercase();
            let existing_deg = canonical.get(&lower).map(|(_, d)| *d).unwrap_or(0);
            if deg > existing_deg {
                canonical.insert(lower, (e.id, deg));
            }
        }

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        for e in &entities {
            if absorbed.contains(&e.id) || is_noise_type(&e.entity_type) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 3 {
                continue;
            }
            let my_deg = degree.get(&e.id).copied().unwrap_or(0);

            // Try progressively shorter prefixes
            for take in (2..words.len()).rev() {
                let prefix = words[..take].join(" ").to_lowercase();
                if let Some(&(target_id, target_deg)) = canonical.get(&prefix) {
                    if target_id == e.id || absorbed.contains(&target_id) {
                        continue;
                    }
                    // Require target to have 3x+ degree (high confidence)
                    if target_deg < my_deg.max(1) * 3 {
                        continue;
                    }
                    self.brain.merge_entities(e.id, target_id)?;
                    absorbed.insert(e.id);
                    merged += 1;
                    break;
                }
            }
        }
        Ok(merged)
    }

    /// (adverbs ending in -ly, adjectives ending in -ous/-ive/-ful/-less, common nouns,
    /// past participles ending in -ed, gerunds ending in -ing, plurals of abstract concepts).
    /// We preserve single-word islands that are clearly proper nouns with known-entity signals.
    pub fn purge_generic_single_word_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        let mut removed = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let name = e.name.trim();
            let word_count = name.split_whitespace().count();
            if word_count != 1 {
                continue;
            }
            if should_purge_single_word(name, &e.entity_type) {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    Ok(())
                })?;
                removed += 1;
            }
        }
        Ok(removed)
    }

    /// Refine over-generic `associated_with` predicates using entity type pairs.
    /// E.g., person→place becomes "active_in", person→organization becomes "affiliated_with",
    /// concept→concept becomes "related_concept", person→person becomes "contemporary_of", etc.
    /// Returns count of relations updated.
    pub fn refine_associated_with(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        let mut updated = 0usize;
        for r in &relations {
            if r.predicate != "associated_with" {
                continue;
            }
            let s_type = id_to_type.get(&r.subject_id).copied().unwrap_or("unknown");
            let o_type = id_to_type.get(&r.object_id).copied().unwrap_or("unknown");
            let new_pred = match (s_type, o_type) {
                ("person", "place") | ("place", "person") => "active_in",
                ("person", "organization") | ("organization", "person") => "affiliated_with",
                ("person", "person") => "collaborated_with",
                ("person", "concept") => "contributed_to",
                ("concept", "person") => "pioneered_by",
                ("person", "event") | ("event", "person") => "participated_in",
                ("organization", "place") | ("place", "organization") => "based_in",
                ("organization", "organization") => "partner_of",
                ("organization", "concept") => "works_on",
                ("concept", "concept") => "related_concept",
                ("concept", "place") | ("place", "concept") => "relevant_to",
                ("event", "place") | ("place", "event") => "held_in",
                _ => continue,
            };
            let ok = self.brain.with_conn(|conn| {
                // Try UPDATE; if unique constraint fails (refined predicate already exists),
                // just DELETE the redundant associated_with row.
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                updated += 1;
            }
        }
        Ok(updated)
    }

    /// Refine remaining `associated_with` predicates using neighborhood context.
    /// For type pairs not handled by `refine_associated_with` (e.g., place-place,
    /// technology-technology), infer the predicate by looking at what predicates
    /// the subject entity most commonly uses with similar-type objects.
    /// Falls back to type-pair heuristics for common cases.
    pub fn refine_associated_with_contextual(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Build subject→[(predicate, object_type)] for non-associated_with relations
        let mut subject_pred_patterns: HashMap<i64, Vec<(&str, &str)>> = HashMap::new();
        for r in &relations {
            if r.predicate != "associated_with" && r.predicate != "related_to" {
                if let Some(&otype) = id_to_type.get(&r.object_id) {
                    subject_pred_patterns
                        .entry(r.subject_id)
                        .or_default()
                        .push((&r.predicate, otype));
                }
            }
        }

        let mut updated = 0usize;
        for r in &relations {
            if r.predicate != "associated_with" {
                continue;
            }
            let s_type = id_to_type.get(&r.subject_id).copied().unwrap_or("unknown");
            let o_type = id_to_type.get(&r.object_id).copied().unwrap_or("unknown");

            // Only handle type pairs that refine_associated_with skips
            let new_pred = match (s_type, o_type) {
                ("place", "place") => "located_near",
                ("technology", "technology") => "related_technology",
                ("technology", "concept") | ("concept", "technology") => "implements",
                ("technology", "person") => "created_by",
                ("technology", "organization") | ("organization", "technology") => "develops",
                ("event", "event") => "concurrent_with",
                ("product", "person") => "created_by",
                ("product", "organization") => "produced_by",
                ("product", "technology") | ("technology", "product") => "uses",
                ("company", "person") | ("person", "company") => "affiliated_with",
                ("company", "place") | ("place", "company") => "headquartered_in",
                ("company", "company") => "partner_of",
                ("company", "concept") | ("concept", "company") => "works_on",
                _ => {
                    // Context-based: if subject has a dominant predicate for this object type, use it
                    if let Some(patterns) = subject_pred_patterns.get(&r.subject_id) {
                        let type_matches: Vec<&&str> = patterns
                            .iter()
                            .filter(|(_, ot)| *ot == o_type)
                            .map(|(p, _)| p)
                            .collect();
                        if !type_matches.is_empty() {
                            // Pick most common predicate for this type pair
                            let mut freq: HashMap<&&str, usize> = HashMap::new();
                            for p in &type_matches {
                                *freq.entry(p).or_insert(0) += 1;
                            }
                            if let Some((&&best, &count)) = freq.iter().max_by_key(|(_, &c)| c) {
                                if count >= 2 && best != "associated_with" {
                                    best
                                } else {
                                    continue;
                                }
                            } else {
                                continue;
                            }
                        } else {
                            continue;
                        }
                    } else {
                        continue;
                    }
                }
            };
            let ok = self.brain.with_conn(|conn| {
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                updated += 1;
            }
        }
        Ok(updated)
    }

    /// Score and classify island entities by crawl priority.
    /// Returns Vec of (entity_name, entity_type, priority_score, reason) sorted by priority.
    /// High-priority islands are well-named entities of valuable types that would
    /// benefit most from being connected to the graph via targeted crawling.
    pub fn classify_island_priority(&self) -> Result<Vec<(String, String, f64, String)>> {
        let islands = self.find_island_entities()?;
        let mut scored: Vec<(String, String, f64, String)> = Vec::new();

        for (name, etype) in &islands {
            let mut score = 0.0_f64;
            let mut reasons = Vec::new();

            // Type value
            let type_score = match etype.as_str() {
                "person" => 0.8,
                "organization" | "company" => 0.7,
                "concept" | "technology" => 0.6,
                "place" => 0.4,
                "event" | "product" => 0.5,
                _ => 0.1,
            };
            score += type_score;
            reasons.push(format!("type:{}", etype));

            // Name quality: multi-word proper nouns are higher quality
            let word_count = name.split_whitespace().count();
            if word_count >= 2 && word_count <= 4 {
                score += 0.3;
                reasons.push("good-name-length".into());
            } else if word_count == 1 && name.len() >= 4 {
                score += 0.1;
            }

            // Capitalization: properly capitalized suggests real entity
            let first_char_upper = name.chars().next().is_some_and(|c| c.is_uppercase());
            if first_char_upper {
                score += 0.1;
            }

            // Skip noise
            if is_noise_name(name) || is_noise_type(etype) {
                continue;
            }

            scored.push((name.clone(), etype.clone(), score, reasons.join(", ")));
        }

        scored.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap_or(std::cmp::Ordering::Equal));
        Ok(scored)
    }

    /// Refine overly generic `related_to` predicates into more specific ones
    /// based on entity type pairs. `related_to` is the most common predicate in
    /// confirmed hypotheses — specializing it dramatically improves graph semantics.
    pub fn refine_related_to(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        let mut updated = 0usize;
        for r in &relations {
            if r.predicate != "related_to" {
                continue;
            }
            let s_type = id_to_type.get(&r.subject_id).copied().unwrap_or("unknown");
            let o_type = id_to_type.get(&r.object_id).copied().unwrap_or("unknown");
            let new_pred = match (s_type, o_type) {
                ("person", "person") => "contemporary_of",
                ("person", "place") | ("place", "person") => "active_in",
                ("person", "organization") | ("organization", "person") => "affiliated_with",
                ("person", "concept") => "contributed_to",
                ("concept", "person") => "pioneered_by",
                ("person", "event") | ("event", "person") => "participated_in",
                ("organization", "place") | ("place", "organization") => "based_in",
                ("organization", "organization") => "partner_of",
                ("organization", "concept") | ("concept", "organization") => "works_on",
                ("concept", "concept") => "related_concept",
                ("concept", "place") | ("place", "concept") => "relevant_to",
                ("place", "place") => "located_near",
                ("event", "place") | ("place", "event") => "held_in",
                ("event", "event") => "concurrent_with",
                ("technology", "person") | ("person", "technology") => "created_by",
                ("technology", "concept") | ("concept", "technology") => "implements",
                ("technology", "organization") | ("organization", "technology") => "develops",
                ("company", "person") | ("person", "company") => "affiliated_with",
                ("company", "place") | ("place", "company") => "headquartered_in",
                ("company", "company") => "partner_of",
                _ => continue,
            };
            let ok = self.brain.with_conn(|conn| {
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                updated += 1;
            }
        }
        Ok(updated)
    }

    /// Refine overly generic `contributed_to` predicates into more specific ones
    /// based on entity types. `contributed_to` makes up ~33% of all relations —
    /// specializing it improves semantic precision and discovery quality.
    pub fn refine_contributed_to(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        let mut updated = 0usize;
        for r in &relations {
            if r.predicate != "contributed_to" {
                continue;
            }
            let s_type = id_to_type.get(&r.subject_id).copied().unwrap_or("unknown");
            let o_type = id_to_type.get(&r.object_id).copied().unwrap_or("unknown");
            let new_pred = match (s_type, o_type) {
                ("person", "concept") => "pioneered",
                ("person", "organization") => "affiliated_with",
                ("person", "place") => "active_in",
                ("person", "event") => "participated_in",
                ("organization", "concept") => "works_on",
                ("organization", "event") => "organized",
                ("concept", "concept") => "influenced",
                ("place", "concept") | ("concept", "place") => "relevant_to",
                _ => continue,
            };
            let ok = self.brain.with_conn(|conn| {
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                updated += 1;
            }
        }
        Ok(updated)
    }

    /// Refine `contemporary_of` predicates into more specific relationships
    /// by analyzing shared neighbors. If two persons share an organization → "colleagues_at",
    /// share a concept → "co_researchers", share a place → "co_located_in", etc.
    /// Only refines when evidence is clear (≥1 shared typed neighbor).
    pub fn refine_contemporary_of(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Build neighbor-by-type map for each entity
        let mut neighbors_by_type: HashMap<i64, HashMap<&str, usize>> = HashMap::new();
        for r in &relations {
            if r.predicate == "contemporary_of" {
                continue; // Don't count contemporary_of as evidence
            }
            if let Some(&otype) = id_to_type.get(&r.object_id) {
                *neighbors_by_type
                    .entry(r.subject_id)
                    .or_default()
                    .entry(otype)
                    .or_insert(0) += 1;
            }
            if let Some(&stype) = id_to_type.get(&r.subject_id) {
                *neighbors_by_type
                    .entry(r.object_id)
                    .or_default()
                    .entry(stype)
                    .or_insert(0) += 1;
            }
        }

        // Build shared-neighbor sets for contemporary_of pairs
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            if r.predicate == "contemporary_of" {
                continue;
            }
            adj.entry(r.subject_id).or_default().insert(r.object_id);
            adj.entry(r.object_id).or_default().insert(r.subject_id);
        }

        let contemporary_rels: Vec<_> = relations
            .iter()
            .filter(|r| r.predicate == "contemporary_of")
            .collect();

        let mut updated = 0usize;
        for r in &contemporary_rels {
            let s_type = id_to_type.get(&r.subject_id).copied().unwrap_or("unknown");
            let o_type = id_to_type.get(&r.object_id).copied().unwrap_or("unknown");

            // Only refine person-person contemporary_of
            if s_type != "person" || o_type != "person" {
                continue;
            }

            let s_adj = adj.get(&r.subject_id);
            let o_adj = adj.get(&r.object_id);
            let shared: Vec<i64> = match (s_adj, o_adj) {
                (Some(a), Some(b)) => a.intersection(b).copied().collect(),
                _ => continue,
            };
            if shared.is_empty() {
                continue;
            }

            // Count shared neighbor types
            let mut shared_types: HashMap<&str, usize> = HashMap::new();
            for &nid in &shared {
                if let Some(&ntype) = id_to_type.get(&nid) {
                    *shared_types.entry(ntype).or_insert(0) += 1;
                }
            }

            // Pick the most specific predicate based on shared neighbor types
            let new_pred = if shared_types.get("organization").copied().unwrap_or(0) > 0 {
                "colleagues_at"
            } else if shared_types.get("concept").copied().unwrap_or(0) >= 2 {
                "co_researchers"
            } else if shared_types.get("event").copied().unwrap_or(0) > 0 {
                "co_participants"
            } else if shared_types.get("place").copied().unwrap_or(0) > 0 && shared_types.len() == 1
            {
                "co_located_in"
            } else {
                continue; // Not enough evidence to refine
            };

            let ok = self.brain.with_conn(|conn| {
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                updated += 1;
            }
        }
        // Second pass: refine based on shared predicate-object pairs
        // If A contemporary_of B and both A→pioneered→X and B→pioneered→X, refine to co_pioneers
        let remaining_contemporary: Vec<_> = self
            .brain
            .all_relations()?
            .into_iter()
            .filter(|r| r.predicate == "contemporary_of")
            .collect();

        // Build entity→[(predicate, object_id)] index (excluding contemporary_of)
        let all_rels2 = self.brain.all_relations()?;
        let mut pred_obj_map: HashMap<i64, Vec<(&str, i64)>> = HashMap::new();
        // Need to store predicates from all_rels2 which lives long enough
        for r in &all_rels2 {
            if r.predicate == "contemporary_of" {
                continue;
            }
            pred_obj_map
                .entry(r.subject_id)
                .or_default()
                .push((&r.predicate, r.object_id));
        }

        for r in &remaining_contemporary {
            let s_type = id_to_type.get(&r.subject_id).copied().unwrap_or("unknown");
            let o_type = id_to_type.get(&r.object_id).copied().unwrap_or("unknown");
            if s_type != "person" || o_type != "person" {
                continue;
            }

            let s_po = match pred_obj_map.get(&r.subject_id) {
                Some(v) => v,
                None => continue,
            };
            let o_po = match pred_obj_map.get(&r.object_id) {
                Some(v) => v,
                None => continue,
            };

            // Find shared (predicate, object) pairs
            let s_set: HashSet<(&str, i64)> = s_po.iter().copied().collect();
            let shared_po: Vec<(&str, i64)> = o_po
                .iter()
                .filter(|po| s_set.contains(po))
                .copied()
                .collect();

            if shared_po.is_empty() {
                continue;
            }

            // Count shared predicates
            let mut shared_preds: HashMap<&str, usize> = HashMap::new();
            for (pred, _) in &shared_po {
                *shared_preds.entry(pred).or_insert(0) += 1;
            }

            let new_pred = if shared_preds.get("pioneered").copied().unwrap_or(0) > 0 {
                "co_pioneers"
            } else if shared_preds.get("contributed_to").copied().unwrap_or(0) > 0 {
                "co_contributors"
            } else if shared_preds.get("works_on").copied().unwrap_or(0) > 0 {
                "co_researchers"
            } else if shared_preds.get("affiliated_with").copied().unwrap_or(0) > 0 {
                "colleagues_at"
            } else if shared_preds.get("active_in").copied().unwrap_or(0) > 0 {
                "co_practitioners"
            } else {
                continue;
            };

            let ok = self.brain.with_conn(|conn| {
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                updated += 1;
            }
        }

        // Third pass: refine based on shared facts (field, nationality, era)
        let remaining3: Vec<_> = self
            .brain
            .all_relations()?
            .into_iter()
            .filter(|r| r.predicate == "contemporary_of")
            .collect();

        // Build entity→facts index
        let mut entity_facts: HashMap<i64, Vec<(String, String)>> = HashMap::new();
        for r in &remaining3 {
            for eid in [r.subject_id, r.object_id] {
                if !entity_facts.contains_key(&eid) {
                    if let Ok(facts) = self.brain.get_facts_for(eid) {
                        entity_facts.insert(
                            eid,
                            facts
                                .into_iter()
                                .map(|f| (f.key, f.value.to_lowercase()))
                                .collect(),
                        );
                    }
                }
            }
        }

        for r in &remaining3 {
            let s_type = id_to_type.get(&r.subject_id).copied().unwrap_or("unknown");
            let o_type = id_to_type.get(&r.object_id).copied().unwrap_or("unknown");
            if s_type != "person" || o_type != "person" {
                continue;
            }
            let s_facts = match entity_facts.get(&r.subject_id) {
                Some(f) => f,
                None => continue,
            };
            let o_facts = match entity_facts.get(&r.object_id) {
                Some(f) => f,
                None => continue,
            };

            // Check for shared field/discipline facts
            let s_fields: HashSet<&str> = s_facts
                .iter()
                .filter(|(k, _)| {
                    k == "field" || k == "discipline" || k == "domain" || k == "known_for"
                })
                .map(|(_, v)| v.as_str())
                .collect();
            let o_fields: HashSet<&str> = o_facts
                .iter()
                .filter(|(k, _)| {
                    k == "field" || k == "discipline" || k == "domain" || k == "known_for"
                })
                .map(|(_, v)| v.as_str())
                .collect();
            let shared_fields: Vec<&&str> = s_fields.intersection(&o_fields).collect();

            let new_pred = if !shared_fields.is_empty() {
                "co_practitioners"
            } else {
                // Check for shared nationality/origin
                let s_nat: HashSet<&str> = s_facts
                    .iter()
                    .filter(|(k, _)| k == "nationality" || k == "country" || k == "origin")
                    .map(|(_, v)| v.as_str())
                    .collect();
                let o_nat: HashSet<&str> = o_facts
                    .iter()
                    .filter(|(k, _)| k == "nationality" || k == "country" || k == "origin")
                    .map(|(_, v)| v.as_str())
                    .collect();
                if s_nat.intersection(&o_nat).next().is_some() {
                    "compatriots"
                } else {
                    continue;
                }
            };

            let ok = self.brain.with_conn(|conn| {
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                updated += 1;
            }
        }

        Ok(updated)
    }

    /// Refine overused `pioneered` predicates into more specific relationships
    /// based on entity type pairs. "pioneered" is semantically correct only for
    /// person→concept; all other type pairs get domain-specific predicates.
    /// Also refines person→concept "pioneered" when neighborhood evidence suggests
    /// a more specific predicate (e.g., "founded" for organizations, "invented" for tech).
    pub fn refine_pioneered(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();
        let id_to_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();

        let mut updated = 0usize;

        // Phase 1: Fix type-mismatched "pioneered" relations
        for r in &relations {
            if r.predicate != "pioneered" {
                continue;
            }
            let s_type = id_to_type.get(&r.subject_id).copied().unwrap_or("unknown");
            let o_type = id_to_type.get(&r.object_id).copied().unwrap_or("unknown");

            // person→concept is the only valid "pioneered" — skip it for phase 1
            if s_type == "person" && o_type == "concept" {
                continue;
            }

            let new_pred = match (s_type, o_type) {
                ("person", "person") => "contemporary_of",
                ("person", "place") => "active_in",
                ("person", "organization") => "affiliated_with",
                ("person", "event") => "participated_in",
                ("person", "technology") => "invented",
                ("concept", "concept") => "related_concept",
                ("concept", "person") => "pioneered_by",
                ("concept", "place") | ("place", "concept") => "relevant_to",
                ("place", "place") => "located_near",
                ("place", "person") => "birthplace_of",
                ("organization", "concept") => "works_on",
                ("organization", "person") => "employed",
                ("organization", "place") => "based_in",
                ("organization", "organization") => "partner_of",
                _ => "associated_with",
            };

            let ok = self.brain.with_conn(|conn| {
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                updated += 1;
            }
        }

        // Phase 2: Specialize person→concept "pioneered" using object name heuristics
        // Concepts containing certain keywords suggest more precise predicates
        let remaining: Vec<_> = self
            .brain
            .all_relations()?
            .into_iter()
            .filter(|r| r.predicate == "pioneered")
            .collect();

        for r in &remaining {
            let o_name = match id_to_name.get(&r.object_id) {
                Some(n) => n.to_lowercase(),
                None => continue,
            };

            // Keyword-based specialization
            let new_pred = if o_name.contains("theorem")
                || o_name.contains("conjecture")
                || o_name.contains("equation")
                || o_name.contains("formula")
                || o_name.contains("law of")
                || o_name.contains("principle of")
            {
                "formulated"
            } else if o_name.contains("algorithm")
                || o_name.contains("method")
                || o_name.contains("technique")
                || o_name.contains("protocol")
            {
                "developed"
            } else if o_name.contains("theory")
                || o_name.contains("hypothesis")
                || o_name.contains("model of")
            {
                "theorized"
            } else if o_name.contains("machine")
                || o_name.contains("engine")
                || o_name.contains("device")
                || o_name.contains("instrument")
                || o_name.contains("computer")
                || o_name.contains("processor")
            {
                "invented"
            } else if o_name.contains("language")
                || o_name.contains("notation")
                || o_name.contains("syntax")
                || o_name.contains("calculus")
            {
                "created"
            } else if o_name.contains("movement")
                || o_name.contains("school of")
                || o_name.contains("philosophy")
                || o_name.contains("ism")
            {
                "founded"
            } else if o_name.contains("discovery")
                || o_name.contains("element")
                || o_name.contains("particle")
                || o_name.contains("radiation")
            {
                "discovered"
            } else if o_name.contains("architecture")
                || o_name.contains("framework")
                || o_name.contains("system")
                || o_name.contains("standard")
            {
                "designed"
            } else {
                continue; // Keep as "pioneered" — it's the right generic
            };

            let ok = self.brain.with_conn(|conn| {
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                updated += 1;
            }
        }

        Ok(updated)
    }

    /// Refine "active_in" relations where entity types don't match the expected
    /// person→place pattern. Fixes misclassified relations from NLP extraction.
    pub fn refine_active_in(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        let mut updated = 0usize;

        for r in &relations {
            if r.predicate != "active_in" {
                continue;
            }
            let s_type = id_to_type.get(&r.subject_id).copied().unwrap_or("unknown");
            let o_type = id_to_type.get(&r.object_id).copied().unwrap_or("unknown");

            // person→place is correct — skip
            if s_type == "person" && o_type == "place" {
                continue;
            }

            let new_pred = match (s_type, o_type) {
                ("place", "person") => "birthplace_of",
                ("place", "place") => "located_near",
                ("person", "person") => "contemporary_of",
                ("person", "organization") => "affiliated_with",
                ("person", "concept") => "works_on",
                ("organization", "place") => "based_in",
                ("concept", "place") => "relevant_to",
                ("place", "concept") => "relevant_to",
                ("organization", "person") => "employed",
                ("organization", "organization") => "partner_of",
                _ => "associated_with",
            };

            let ok = self.brain.with_conn(|conn| {
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                updated += 1;
            }
        }

        Ok(updated)
    }

    /// Demote weak `contemporary_of` relations: reduce confidence of person-person
    /// contemporary_of relations that have no shared neighbors (no corroborating evidence).
    /// These were typically inferred from co-occurrence in the same article without
    /// any deeper structural connection in the knowledge graph.
    pub fn demote_weak_contemporary(&self) -> Result<usize> {
        let demoted: usize = self.brain.with_conn(|conn| {
            // Find contemporary_of relations between persons with no shared non-contemporary neighbors
            let count = conn.execute(
                "UPDATE relations SET confidence = confidence * 0.8
                 WHERE predicate = 'contemporary_of'
                 AND confidence > 0.3
                 AND subject_id IN (SELECT id FROM entities WHERE entity_type = 'person')
                 AND object_id IN (SELECT id FROM entities WHERE entity_type = 'person')
                 AND NOT EXISTS (
                     SELECT 1 FROM relations r2
                     JOIN relations r3 ON (r2.object_id = r3.object_id OR r2.object_id = r3.subject_id)
                     WHERE r2.subject_id = relations.subject_id
                     AND (r3.subject_id = relations.object_id OR r3.object_id = relations.object_id)
                     AND r2.predicate != 'contemporary_of'
                     AND r3.predicate != 'contemporary_of'
                     AND r2.id != r3.id
                 )",
                [],
            )?;
            Ok(count)
        })?;
        // Delete contemporary_of relations that have decayed below threshold
        let pruned: usize = self.brain.with_conn(|conn| {
            let count = conn.execute(
                "DELETE FROM relations WHERE predicate = 'contemporary_of' AND confidence < 0.2",
                [],
            )?;
            Ok(count)
        })?;
        if pruned > 0 {
            eprintln!(
                "[PROMETHEUS] Pruned {} sub-threshold contemporary_of relations",
                pruned
            );
        }
        Ok(demoted + pruned)
    }

    /// Remove redundant `contemporary_of` relations where a more specific predicate
    /// already connects the same entity pair. Also delete `contemporary_of` between
    /// non-person entities (places/concepts can't be "contemporaries"), and refine
    /// person-person `contemporary_of` by inferring predicates from shared domain
    /// context (e.g., both pioneered same concept → "co_researchers").
    pub fn prune_redundant_contemporary(&self) -> Result<usize> {
        let mut total = 0usize;

        // 1. Delete contemporary_of where a better predicate already exists for the same pair
        total += self.brain.with_conn(|conn| {
            let count = conn.execute(
                "DELETE FROM relations WHERE predicate = 'contemporary_of'
                 AND EXISTS (
                     SELECT 1 FROM relations r2
                     WHERE r2.predicate != 'contemporary_of'
                     AND ((r2.subject_id = relations.subject_id AND r2.object_id = relations.object_id)
                       OR (r2.subject_id = relations.object_id AND r2.object_id = relations.subject_id))
                 )",
                [],
            )?;
            Ok(count)
        })?;

        // 2. Delete contemporary_of between non-person entities
        total += self.brain.with_conn(|conn| {
            let count = conn.execute(
                "DELETE FROM relations WHERE predicate = 'contemporary_of'
                 AND (subject_id NOT IN (SELECT id FROM entities WHERE entity_type = 'person')
                   OR object_id NOT IN (SELECT id FROM entities WHERE entity_type = 'person'))",
                [],
            )?;
            Ok(count)
        })?;

        // 3. Refine remaining contemporary_of using shared predicate-object patterns:
        //    if both persons share the same (predicate, object) pair via other relations,
        //    infer a more meaningful relationship.
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_type: HashMap<i64, &str> = entities
            .iter()
            .map(|e| (e.id, e.entity_type.as_str()))
            .collect();

        // Build predicate-object sets per entity (excluding contemporary_of itself)
        let mut pred_obj: HashMap<i64, Vec<(&str, i64)>> = HashMap::new();
        for r in &relations {
            if r.predicate == "contemporary_of" {
                continue;
            }
            pred_obj
                .entry(r.subject_id)
                .or_default()
                .push((&r.predicate, r.object_id));
        }

        let contemporary_rels: Vec<_> = relations
            .iter()
            .filter(|r| r.predicate == "contemporary_of")
            .collect();

        for r in &contemporary_rels {
            let s_type = id_to_type.get(&r.subject_id).copied().unwrap_or("unknown");
            let o_type = id_to_type.get(&r.object_id).copied().unwrap_or("unknown");
            if s_type != "person" || o_type != "person" {
                continue;
            }

            let s_po = pred_obj.get(&r.subject_id);
            let o_po = pred_obj.get(&r.object_id);
            let (s_po, o_po) = match (s_po, o_po) {
                (Some(a), Some(b)) => (a, b),
                _ => continue,
            };

            // Find shared (predicate, object) pairs
            let s_set: HashSet<(&str, i64)> = s_po.iter().copied().collect();
            let shared: Vec<(&str, i64)> = o_po
                .iter()
                .filter(|po| s_set.contains(po))
                .copied()
                .collect();
            if shared.is_empty() {
                continue;
            }

            // Determine best predicate from shared context
            let shared_preds: HashSet<&str> = shared.iter().map(|(p, _)| *p).collect();
            let new_pred = if shared_preds.contains("pioneered")
                || shared_preds.contains("works_on")
                || shared_preds.contains("contributed_to")
            {
                "co_researchers"
            } else if shared_preds.contains("affiliated_with") {
                "colleagues_at"
            } else if shared_preds.contains("active_in") || shared_preds.contains("based_in") {
                "co_located_in"
            } else {
                continue;
            };

            let ok = self.brain.with_conn(|conn| {
                let res = conn.execute(
                    "UPDATE relations SET predicate = ?1 WHERE id = ?2",
                    params![new_pred, r.id],
                );
                match res {
                    Ok(_) => Ok(true),
                    Err(rusqlite::Error::SqliteFailure(e, _))
                        if e.code == rusqlite::ErrorCode::ConstraintViolation =>
                    {
                        conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                        Ok(true)
                    }
                    Err(e) => Err(e),
                }
            })?;
            if ok {
                total += 1;
            }
        }

        Ok(total)
    }

    /// Promote high-confidence testing hypotheses to confirmed if they've been
    /// testing for over `min_days` and confidence is above `min_conf`.
    /// This prevents hypothesis limbo — if nothing contradicts a plausible hypothesis
    /// after several discovery cycles, it's likely valid.
    pub fn promote_mature_hypotheses(&self, min_days: i64, min_conf: f64) -> Result<usize> {
        let cutoff = (Utc::now() - chrono::Duration::days(min_days))
            .naive_utc()
            .format("%Y-%m-%d %H:%M:%S")
            .to_string();
        // First, collect IDs of hypotheses that will be promoted so we can record discoveries
        let to_promote: Vec<(i64, String)> = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT id, pattern_source FROM hypotheses WHERE status = 'testing' AND confidence >= ?1 AND discovered_at < ?2",
            )?;
            let rows = stmt.query_map(params![min_conf, cutoff], |row| {
                Ok((row.get::<_, i64>(0)?, row.get::<_, String>(1)?))
            })?;
            rows.collect::<Result<Vec<_>>>()
        })?;
        let count = self.brain.with_conn(|conn| {
            let updated = conn.execute(
                "UPDATE hypotheses SET status = 'confirmed' WHERE status = 'testing' AND confidence >= ?1 AND discovered_at < ?2",
                params![min_conf, cutoff],
            )?;
            Ok(updated)
        })?;
        // Record discoveries and outcomes for promoted hypotheses
        for (id, source) in &to_promote {
            let _ = self.save_discovery(
                *id,
                &[format!(
                    "Mature promotion (>{} days, conf>={:.2})",
                    min_days, min_conf
                )],
            );
            let _ = self.record_outcome(source, true);
        }
        Ok(count)
    }

    /// Accelerated promotion for high-ROI strategies: strategies with a historical
    /// confirmation rate >= `min_strategy_rate` get their testing hypotheses promoted
    /// at a lower confidence threshold (`min_conf`) after fewer days (`min_days`).
    /// This rewards reliable strategies with faster hypothesis resolution.
    pub fn promote_high_roi_mature_hypotheses(
        &self,
        min_days: i64,
        min_conf: f64,
        min_strategy_rate: f64,
    ) -> Result<usize> {
        // Find strategies with sufficient track record and high confirmation rate
        let rates = self.strategy_confirmation_rates(30)?;
        let elite_strategies: HashSet<String> = rates
            .into_iter()
            .filter(|(_, (total, _, rate))| *total >= 30 && *rate >= min_strategy_rate)
            .map(|(s, _)| s)
            .collect();

        if elite_strategies.is_empty() {
            return Ok(0);
        }

        let cutoff = (Utc::now() - chrono::Duration::days(min_days))
            .naive_utc()
            .format("%Y-%m-%d %H:%M:%S")
            .to_string();

        // Collect eligible hypotheses
        let candidates: Vec<(i64, String)> = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT id, pattern_source FROM hypotheses \
                 WHERE status = 'testing' AND confidence >= ?1 AND discovered_at < ?2",
            )?;
            let rows = stmt.query_map(params![min_conf, cutoff], |row| {
                Ok((row.get::<_, i64>(0)?, row.get::<_, String>(1)?))
            })?;
            rows.collect::<Result<Vec<_>>>()
        })?;

        let mut promoted = 0usize;
        for (id, source) in &candidates {
            if !elite_strategies.contains(source) {
                continue;
            }
            self.brain.with_conn(|conn| {
                conn.execute(
                    "UPDATE hypotheses SET status = 'confirmed' WHERE id = ?1",
                    params![id],
                )?;
                Ok(())
            })?;
            let _ = self.save_discovery(
                *id,
                &[format!(
                    "Accelerated promotion: high-ROI strategy '{}' (rate>={:.0}%, >{}d, conf>={:.2})",
                    source, min_strategy_rate * 100.0, min_days, min_conf
                )],
            );
            let _ = self.record_outcome(source, true);
            promoted += 1;
        }
        Ok(promoted)
    }

    /// Decay confidence of stale testing hypotheses that haven't been confirmed.
    /// Hypotheses older than `min_days` with confidence below `max_conf` get a
    /// multiplicative decay applied, eventually auto-rejecting at floor threshold.
    /// This prevents hypothesis accumulation — the system should not carry thousands
    /// of low-confidence "testing" hypotheses indefinitely.
    pub fn decay_stale_hypotheses(
        &self,
        min_days: i64,
        decay_factor: f64,
        reject_floor: f64,
    ) -> Result<(usize, usize)> {
        let cutoff = (Utc::now() - chrono::Duration::days(min_days))
            .naive_utc()
            .format("%Y-%m-%d %H:%M:%S")
            .to_string();

        // Collect stale testing hypotheses
        let stale: Vec<(i64, f64, String)> = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT id, confidence, pattern_source FROM hypotheses \
                 WHERE status = 'testing' AND discovered_at < ?1",
            )?;
            let rows = stmt.query_map(params![cutoff], |row| {
                Ok((
                    row.get::<_, i64>(0)?,
                    row.get::<_, f64>(1)?,
                    row.get::<_, String>(2)?,
                ))
            })?;
            rows.collect::<Result<Vec<_>>>()
        })?;

        let mut decayed = 0usize;
        let mut rejected = 0usize;

        for (id, conf, source) in &stale {
            let new_conf = conf * decay_factor;
            if new_conf < reject_floor {
                // Auto-reject
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE hypotheses SET status = 'rejected', confidence = ?1 WHERE id = ?2",
                        params![new_conf, id],
                    )?;
                    Ok(())
                })?;
                let _ = self.record_outcome(source, false);
                rejected += 1;
            } else {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE hypotheses SET confidence = ?1 WHERE id = ?2",
                        params![new_conf, id],
                    )?;
                    Ok(())
                })?;
                decayed += 1;
            }
        }

        Ok((decayed, rejected))
    }

    /// Analyze strategy diversity: returns distribution of hypothesis sources
    /// and identifies over-represented or under-performing strategies.
    /// Useful for meta-learning: if one strategy dominates hypothesis generation
    /// but has low confirmation rate, its weight should drop.
    pub fn strategy_diversity_report(
        &self,
    ) -> Result<Vec<(String, usize, usize, usize, f64, f64)>> {
        // (strategy, total, confirmed, rejected, confirmation_rate, share_of_total)
        self.brain.with_conn(|conn| {
            let total_hyps: i64 =
                conn.query_row("SELECT COUNT(*) FROM hypotheses", [], |row| row.get(0))?;
            let total = total_hyps.max(1) as f64;

            let mut stmt = conn.prepare(
                "SELECT pattern_source, \
                        COUNT(*) as cnt, \
                        SUM(CASE WHEN status='confirmed' THEN 1 ELSE 0 END) as conf, \
                        SUM(CASE WHEN status='rejected' THEN 1 ELSE 0 END) as rej \
                 FROM hypotheses \
                 GROUP BY pattern_source \
                 ORDER BY cnt DESC",
            )?;
            let rows = stmt.query_map([], |row| {
                let source: String = row.get(0)?;
                let cnt: i64 = row.get(1)?;
                let conf: i64 = row.get(2)?;
                let rej: i64 = row.get(3)?;
                let decided = (conf + rej).max(1) as f64;
                let rate = conf as f64 / decided;
                let share = cnt as f64 / total;
                Ok((
                    source,
                    cnt as usize,
                    conf as usize,
                    rej as usize,
                    rate,
                    share,
                ))
            })?;
            rows.collect()
        })
    }

    /// Decompose concatenated entity names: entities like "Caucasus Crimea Balkans"
    /// or "South-East Asia Africa" where multiple entity names got concatenated
    /// during NLP extraction. Splits them and creates relations to component entities.
    /// Returns count of relations created + entities cleaned up.
    pub fn split_concatenated_entities(&self) -> Result<(usize, usize)> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build name→id lookup for known entities (case-insensitive)
        let mut name_to_id: HashMap<String, i64> = HashMap::new();
        for e in &entities {
            if !is_noise_name(&e.name) && !is_noise_type(&e.entity_type) {
                name_to_id.entry(e.name.to_lowercase()).or_insert(e.id);
            }
        }

        let mut rels_created = 0usize;
        let mut entities_cleaned = 0usize;

        for e in &entities {
            if is_noise_name(&e.name) || is_noise_type(&e.entity_type) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 3 {
                continue;
            }

            // Try all possible splits into 2+ known entity names
            // E.g., "Caucasus Crimea Balkans" → try "Caucasus" + "Crimea Balkans",
            //   "Caucasus Crimea" + "Balkans", etc.
            let mut found_components: Vec<(i64, String)> = Vec::new();
            for split_at in 1..words.len() {
                let left: String = words[..split_at].join(" ");
                let right: String = words[split_at..].join(" ");
                let left_lower = left.to_lowercase();
                let right_lower = right.to_lowercase();

                // Both parts must be known entities (different from this entity)
                if let (Some(&lid), Some(&rid)) =
                    (name_to_id.get(&left_lower), name_to_id.get(&right_lower))
                {
                    if lid != e.id && rid != e.id && lid != rid {
                        found_components.push((lid, left));
                        found_components.push((rid, right));
                        break;
                    }
                }
            }

            if found_components.len() >= 2 {
                // Create relations from this entity to its components
                for (comp_id, _comp_name) in &found_components {
                    self.brain
                        .upsert_relation(e.id, "references", *comp_id, "")?;
                    rels_created += 1;
                }
                // If this entity is an island, merge it away
                if !connected.contains(&e.id) {
                    let facts = self.brain.get_facts_for(e.id)?;
                    if facts.is_empty() {
                        self.brain.with_conn(|conn| {
                            conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                            Ok(())
                        })?;
                        entities_cleaned += 1;
                    }
                }
            }
        }
        Ok((rels_created, entities_cleaned))
    }

    /// Dissolve single-word name fragment hubs.
    ///
    /// Entities like "Charles" (concept, 35 relations) are NLP extraction artifacts
    /// that absorb connections meant for real entities ("Charles Babbage", "Charles Darwin").
    /// This method:
    /// 1. Finds single-word entities with high degree that look like name fragments
    /// 2. For each, checks if multi-word entities exist containing that name
    /// 3. If so, merges the fragment into the most connected matching entity
    /// 4. Or if the fragment is just a hub linking unrelated "Charles *" entities,
    ///    deletes it and its relations (they're noise)
    ///
    /// Returns count of fragments dissolved (merged or removed).
    pub fn dissolve_name_fragment_hubs(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Known real single-word entities that should NOT be dissolved
        let real_single_words: HashSet<&str> = [
            // Countries & major places
            "China",
            "France",
            "Germany",
            "Japan",
            "India",
            "Russia",
            "Italy",
            "Spain",
            "Brazil",
            "Canada",
            "Mexico",
            "Egypt",
            "Iran",
            "Iraq",
            "Turkey",
            "Greece",
            "Sweden",
            "Norway",
            "Finland",
            "Denmark",
            "Poland",
            "Austria",
            "Belgium",
            "Portugal",
            "Romania",
            "Hungary",
            "Ireland",
            "Scotland",
            "England",
            "Wales",
            "Africa",
            "Asia",
            "Europe",
            "America",
            "Antarctica",
            "Australia",
            "Paris",
            "London",
            "Berlin",
            "Rome",
            "Tokyo",
            "Moscow",
            "Vienna",
            "Prague",
            "Madrid",
            "Lisbon",
            "Athens",
            "Cairo",
            "Baghdad",
            "Tehran",
            "Delhi",
            "Zurich",
            "Geneva",
            "Bern",
            "Basel",
            "Lausanne",
            "Lucerne",
            "Svalbard",
            "Crimea",
            "Balkans",
            "Caucasus",
            "Anatolia",
            "Mesopotamia",
            // Major concepts/things
            "Internet",
            "Bitcoin",
            "Linux",
            "Wikipedia",
            "Amazon",
            "Google",
            "Apple",
            "Microsoft",
            "Tesla",
            "Netflix",
            "Facebook",
            "Twitter",
            "YouTube",
            "DNA",
            "RNA",
            "CRISPR",
            "NATO",
            "UNESCO",
            "UNICEF",
            // Historical entities
            "Renaissance",
            "Reformation",
            "Enlightenment",
        ]
        .iter()
        .copied()
        .collect();

        // Build lookup: lowercase_name → Vec<(id, degree, name)> for multi-word entities
        let mut multiword_by_word: HashMap<String, Vec<(i64, usize, String)>> = HashMap::new();
        for e in &entities {
            if is_noise_type(&e.entity_type) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() >= 2 {
                let deg = degree.get(&e.id).copied().unwrap_or(0);
                for w in &words {
                    multiword_by_word
                        .entry(w.to_lowercase())
                        .or_default()
                        .push((e.id, deg, e.name.clone()));
                }
            }
        }

        let mut dissolved = 0usize;

        // Debug: count how many single-word entities with deg>=5 we find
        let mut dbg_count = 0usize;
        for e in &entities {
            let name = e.name.trim();
            if name.split_whitespace().count() == 1 && !is_noise_type(&e.entity_type) {
                let deg = degree.get(&e.id).copied().unwrap_or(0);
                if deg >= 5 && !real_single_words.contains(name) {
                    dbg_count += 1;
                    if dbg_count <= 5 {
                        eprintln!(
                            "  [dissolve-debug] '{}' type={} deg={}",
                            name, e.entity_type, deg
                        );
                    }
                }
            }
        }
        // eprintln!("  [dissolve] total candidates with deg>=5: {}", dbg_count);

        for e in &entities {
            if is_noise_type(&e.entity_type) {
                continue;
            }
            let name = e.name.trim();
            let word_count = name.split_whitespace().count();
            if word_count != 1 {
                continue;
            }
            // Skip known real entities
            if real_single_words.contains(name) {
                continue;
            }
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            if deg < 5 {
                continue; // Only target high-degree fragments
            }

            // Check: does this look like a name fragment?
            let lower = name.to_lowercase();
            let matching = multiword_by_word.get(&lower).cloned().unwrap_or_default();

            if name == "Charles" || name == "Noether" || name == "Lovelace" {
                eprintln!(
                    "  [dissolve-trace] '{}' deg={} matching.len()={}",
                    name,
                    deg,
                    matching.len()
                );
            }

            if matching.len() < 2 {
                continue; // Not enough evidence it's a fragment
            }
            eprintln!(
                "  [dissolve] candidate: '{}' (deg={}, {} matching multi-word entities)",
                name,
                deg,
                matching.len()
            );

            // This is a fragment hub. Find the best target to merge into.
            // Best = most connected multi-word entity containing this name.
            let mut best_target: Option<(i64, usize, String)> = None;
            for (mid, mdeg, mname) in &matching {
                if *mid == e.id {
                    continue;
                }
                if best_target.is_none() || mdeg > &best_target.as_ref().unwrap().1 {
                    best_target = Some((*mid, *mdeg, mname.clone()));
                }
            }

            if let Some((target_id, _target_deg, _target_name)) = best_target {
                // Merge fragment into best matching multi-word entity
                self.brain.merge_entities(e.id, target_id)?;
                dissolved += 1;
            }
        }
        Ok(dissolved)
    }

    /// Strip leading adjectives/demonyms from entity names.
    /// E.g., "American Eli Whitney" → merge into "Eli Whitney",
    /// "French Marie Curie" → merge into "Marie Curie".
    /// Only acts when the stripped version exists as a higher-degree entity.
    pub fn strip_leading_adjectives(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build name → (id, degree) lookup
        let mut name_lookup: HashMap<String, (i64, usize)> = HashMap::new();
        for e in &entities {
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            let lower = e.name.to_lowercase();
            // Keep highest-degree version
            let entry = name_lookup.entry(lower).or_insert((e.id, deg));
            if deg > entry.1 {
                *entry = (e.id, deg);
            }
        }

        // Leading words that are likely adjectives/demonyms, not part of proper names
        let leading_adjectives: HashSet<&str> = [
            "american",
            "british",
            "french",
            "german",
            "italian",
            "spanish",
            "russian",
            "chinese",
            "japanese",
            "indian",
            "canadian",
            "australian",
            "dutch",
            "swiss",
            "swedish",
            "norwegian",
            "danish",
            "finnish",
            "polish",
            "austrian",
            "belgian",
            "portuguese",
            "hungarian",
            "irish",
            "scottish",
            "english",
            "welsh",
            "greek",
            "turkish",
            "persian",
            "arab",
            "african",
            "asian",
            "european",
            "latin",
            "western",
            "eastern",
            "northern",
            "southern",
            "central",
            "ancient",
            "modern",
            "medieval",
            "classical",
            "early",
            "late",
            "young",
            "old",
            "great",
            "little",
            "big",
            "new",
            "former",
            "professor",
            "dr",
            "sir",
            "lord",
            "king",
            "queen",
            "emperor",
            "admiral",
            "general",
            "generals",
            "colonel",
            "captain",
            "marshal",
            "field-marshal",
            "sergeant",
            "lieutenant",
            "commander",
            "commodore",
            "minister",
            "governor",
            "archbishop",
            "bishop",
            "sultan",
            "kaiser",
            "tsar",
            "czar",
            "shah",
            "prince",
            "princess",
            "duke",
            "duchess",
            "count",
            "countess",
            "baron",
            "baroness",
            "cardinal",
            "pope",
            "saint",
            "san",
            "scientific",
            "royal",
            "imperial",
            "national",
            "international",
        ]
        .iter()
        .copied()
        .collect();

        // Country/region names that appear as prefixes in NLP extraction errors
        // e.g. "Netherlands Oskar Klein", "Switzerland CERN", "Japan Toyota"
        let country_prefixes: HashSet<&str> = [
            "netherlands",
            "switzerland",
            "germany",
            "france",
            "italy",
            "spain",
            "portugal",
            "belgium",
            "austria",
            "sweden",
            "norway",
            "denmark",
            "finland",
            "poland",
            "hungary",
            "romania",
            "bulgaria",
            "serbia",
            "croatia",
            "greece",
            "turkey",
            "russia",
            "ukraine",
            "japan",
            "korea",
            "taiwan",
            "thailand",
            "vietnam",
            "indonesia",
            "malaysia",
            "singapore",
            "philippines",
            "brazil",
            "argentina",
            "mexico",
            "colombia",
            "chile",
            "peru",
            "egypt",
            "israel",
            "iran",
            "iraq",
            "pakistan",
            "bangladesh",
            "nigeria",
            "kenya",
            "ethiopia",
            "tanzania",
            "australia",
            "zealand",
            "ireland",
            "scotland",
            "england",
            "wales",
            "canada",
            "cuba",
            "prussia",
            "saxony",
            "bavaria",
            "bohemia",
            "catalonia",
            "lombardy",
            "tuscany",
            "andalusia",
            "normandy",
        ]
        .iter()
        .copied()
        .collect();

        // Combine all strippable prefixes
        let all_prefixes: HashSet<&str> = leading_adjectives
            .union(&country_prefixes)
            .copied()
            .collect();

        let mut merged = 0usize;
        for e in &entities {
            let words: Vec<&str> = e.name.split_whitespace().collect();
            // For person entities, allow stripping to 2-word result (e.g. "Netherlands Oskar Klein" → "Oskar Klein")
            // For other types, still require 3+ words
            let min_words = if e.entity_type == "person" { 2 } else { 3 };
            if words.len() < min_words + 1 {
                continue;
            }
            let first_lower = words[0].to_lowercase();
            if !all_prefixes.contains(first_lower.as_str()) {
                continue;
            }
            // Try stripping the first word
            let stripped = words[1..].join(" ");
            let stripped_lower = stripped.to_lowercase();
            if let Some(&(target_id, _target_deg)) = name_lookup.get(&stripped_lower) {
                if target_id == e.id {
                    continue;
                }
                let _my_deg = degree.get(&e.id).copied().unwrap_or(0);
                // Merge the adjective-prefixed form into the canonical stripped form.
                // The stripped form is the correct name regardless of degree.
                eprintln!(
                    "  [adj-strip] merging '{}' (id={}) → '{}' (id={})",
                    e.name, e.id, stripped, target_id
                );
                self.brain.merge_entities(e.id, target_id)?;
                merged += 1;
            }
        }
        Ok(merged)
    }

    /// Information-theoretic entity scoring: rank entities by how much they contribute
    /// to the knowledge graph's information content. Uses a combination of:
    /// - Structural importance (betweenness centrality proxy via degree * clustering)
    /// - Uniqueness (inverse of how many similar entities exist)
    /// - Connectivity quality (ratio of diverse predicates to total degree)
    /// Returns (entity_name, entity_type, info_score) sorted descending.
    pub fn information_content_ranking(&self, limit: usize) -> Result<Vec<(String, String, f64)>> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut degree: HashMap<i64, usize> = HashMap::new();
        let mut predicates: HashMap<i64, HashSet<String>> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
            predicates
                .entry(r.subject_id)
                .or_default()
                .insert(r.predicate.clone());
            predicates
                .entry(r.object_id)
                .or_default()
                .insert(r.predicate.clone());
        }

        // Type frequency for uniqueness scoring
        let mut type_count: HashMap<String, usize> = HashMap::new();
        for e in &entities {
            *type_count.entry(e.entity_type.clone()).or_insert(0) += 1;
        }

        let total_entities = entities.len().max(1) as f64;
        let mut scores: Vec<(String, String, f64)> = Vec::new();

        for e in &entities {
            if is_noise_type(&e.entity_type) || is_noise_name(&e.name) {
                continue;
            }
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            if deg == 0 {
                continue;
            }

            let pred_diversity = predicates.get(&e.id).map(|s| s.len()).unwrap_or(0) as f64;
            let type_freq = type_count.get(&e.entity_type).copied().unwrap_or(1) as f64;

            // Information score components:
            // 1. Connectivity: log(degree + 1) — diminishing returns for hubs
            let connectivity = (deg as f64 + 1.0).ln();
            // 2. Predicate diversity: more diverse connections = more informative
            let diversity = pred_diversity / (deg as f64).max(1.0);
            // 3. Type uniqueness: rarer types are more informative
            let uniqueness = (total_entities / type_freq).ln().max(0.1);
            // 4. Fact richness
            let facts = self.brain.get_facts_for(e.id)?.len() as f64;
            let fact_bonus = (facts + 1.0).ln() * 0.5;

            let info_score = connectivity * diversity * uniqueness + fact_bonus;
            scores.push((e.name.clone(), e.entity_type.clone(), info_score));
        }

        scores.sort_by(|a, b| b.2.partial_cmp(&a.2).unwrap_or(std::cmp::Ordering::Equal));
        scores.truncate(limit);
        Ok(scores)
    }

    /// Token-based island reconnection: match island entities to connected entities
    /// via significant shared name tokens. E.g., island "Möngke Khan" connects to
    /// "Genghis Khan" via shared token "Khan". Uses TF-IDF-like weighting — rare
    /// tokens that appear in few entity names are more informative than common ones.
    /// Only connects when the shared tokens are significant (not stopwords, not too common).
    pub fn reconnect_islands_by_tokens(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build direct-connection set
        let mut edges: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            edges.insert(key);
        }

        // Tokenize all entity names, compute document frequency per token
        let stopwords: HashSet<&str> = [
            "the",
            "of",
            "and",
            "in",
            "on",
            "for",
            "to",
            "by",
            "from",
            "with",
            "at",
            "an",
            "or",
            "a",
            "is",
            "was",
            "are",
            "were",
            "be",
            "been",
            "has",
            "had",
            "have",
            "do",
            "does",
            "did",
            "not",
            "no",
            "but",
            "if",
            "as",
            "it",
            "its",
            "his",
            "her",
            "he",
            "she",
            "they",
            "their",
            "this",
            "that",
            "which",
            "who",
            "whom",
            "de",
            "la",
            "le",
            "les",
            "du",
            "des",
            "von",
            "van",
            "der",
            "den",
            "di",
            "el",
            // Geographic/descriptive terms that cause spurious token matches
            "sea",
            "basin",
            "island",
            "islands",
            "cape",
            "bay",
            "gulf",
            "strait",
            "river",
            "lake",
            "mountain",
            "mount",
            "valley",
            "desert",
            "forest",
            "park",
            "port",
            "north",
            "south",
            "east",
            "west",
            "northern",
            "southern",
            "eastern",
            "western",
            "new",
            "old",
            "great",
            "big",
            "little",
            "upper",
            "lower",
            "central",
            // Academic/publication terms
            "university",
            "college",
            "institute",
            "school",
            "academy",
            "society",
            "journal",
            "review",
            "press",
            "news",
            "radio",
            "time",
            "standard",
            // Common descriptors that link unrelated entities
            "national",
            "international",
            "royal",
            "imperial",
            "federal",
            "state",
            "general",
            "special",
            "grand",
            "holy",
            "sacred",
            "ancient",
            "modern",
            // Common surname-like words that cause false person matches
            "deep",
            "ridge",
            "dover",
            "london",
            "paris",
            "berlin",
            "prize",
            "award",
            "medal",
            "order",
            "courier",
            "herald",
            // Common English words that cause spurious token matches
            "you",
            "your",
            "all",
            "one",
            "two",
            "three",
            "first",
            "last",
            "next",
            "only",
            "just",
            "more",
            "most",
            "some",
            "many",
            "any",
            "each",
            "every",
            "other",
            "such",
            "same",
            "way",
            "day",
            "year",
            "man",
            "men",
            "out",
            "into",
            "over",
            "also",
            "after",
            "before",
            "how",
            "why",
            "what",
            "when",
            "where",
            "there",
            "here",
            "about",
            "between",
            "through",
            "during",
            "under",
            "along",
            "both",
            "well",
            "back",
            "even",
            "still",
            "then",
            "than",
            "very",
            "too",
            "much",
            "now",
            "long",
            "made",
            "make",
            "like",
            "will",
            "can",
            "may",
            "could",
            "would",
            "should",
            "use",
            "used",
            "core",
            "easy",
            "fast",
            "high",
            "low",
            "left",
            "right",
            "real",
            "true",
            "full",
            "good",
            "best",
            "need",
            "take",
            "give",
            "find",
            "know",
            "come",
            "part",
            "work",
            "world",
            "life",
            "being",
            "place",
            "thing",
            "point",
            "small",
            "large",
            "early",
            "late",
            "half",
            "end",
            "side",
            "line",
            "land",
            "head",
            "hand",
            "eye",
            "face",
            "book",
            "war",
            "bury",
            "compare",
            // Common nouns that cause spurious cross-domain matches
            "cube",
            "theory",
            "problem",
            "system",
            "model",
            "code",
            "map",
            "maps",
            "guide",
            "lost",
            "red",
            "blue",
            "green",
            "black",
            "white",
            "golden",
            "silver",
            "dark",
            "light",
            "star",
            "sun",
            "moon",
            "fire",
            "ice",
            "iron",
            "steel",
            "stone",
            "rock",
            "sand",
            "snow",
            "storm",
            "wind",
            "rain",
            "cloud",
            "wave",
            "twin",
            "double",
            "triple",
            "super",
            "mega",
            "ultra",
            "mini",
            "micro",
            "nano",
            "digital",
            "analog",
            "hybrid",
            "memory",
            "power",
            "energy",
            "force",
            "speed",
            "test",
            "trial",
            "game",
            "play",
            "race",
            "match",
            "fight",
            "rule",
            "rules",
            "act",
            "finally",
            "scientist",
            "tri",
            "bin",
            "hasan",
            "webb",
            "revenge",
            "encoder",
            "belt",
            "road",
        ]
        .into_iter()
        .collect();

        // Token → entity IDs (only for meaningful, non-noise entities)
        let mut token_entities: HashMap<String, Vec<i64>> = HashMap::new();
        let mut entity_tokens: HashMap<i64, Vec<String>> = HashMap::new();
        let entity_map: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        for e in &entities {
            if !meaningful.contains(&e.id)
                || is_noise_name(&e.name)
                || is_noise_type(&e.entity_type)
            {
                continue;
            }
            let tokens: Vec<String> = e
                .name
                .split_whitespace()
                .filter(|w| w.len() >= 3 && !stopwords.contains(&w.to_lowercase().as_str()))
                .map(|w| w.to_lowercase())
                .collect();
            for t in &tokens {
                token_entities.entry(t.clone()).or_default().push(e.id);
            }
            entity_tokens.insert(e.id, tokens);
        }

        let total_entities = entities.len().max(1) as f64;

        // For each island entity, find best connected match via shared significant tokens
        let mut reconnected = 0usize;
        let islands: Vec<i64> = entities
            .iter()
            .filter(|e| {
                meaningful.contains(&e.id)
                    && !connected.contains(&e.id)
                    && !is_noise_name(&e.name)
                    && !is_noise_type(&e.entity_type)
                    && e.name.split_whitespace().count() >= 2 // At least 2-word names for token matching
            })
            .map(|e| e.id)
            .collect();

        for &island_id in &islands {
            let island_toks = match entity_tokens.get(&island_id) {
                Some(t) if !t.is_empty() => t,
                _ => continue,
            };

            // Score each connected entity by TF-IDF-like shared token weight
            let mut candidates: HashMap<i64, f64> = HashMap::new();
            for tok in island_toks {
                let doc_freq = token_entities.get(tok).map(|v| v.len()).unwrap_or(0) as f64;
                if doc_freq < 2.0 || doc_freq > total_entities * 0.1 {
                    continue; // Too rare (unique to island) or too common
                }
                let idf = (total_entities / doc_freq).ln();
                if let Some(matching_ids) = token_entities.get(tok) {
                    for &mid in matching_ids {
                        if mid == island_id || !connected.contains(&mid) {
                            continue;
                        }
                        let key = if island_id < mid {
                            (island_id, mid)
                        } else {
                            (mid, island_id)
                        };
                        if edges.contains(&key) {
                            continue;
                        }
                        *candidates.entry(mid).or_insert(0.0) += idf;
                    }
                }
            }

            // Find best candidate — require significant shared tokens
            if let Some((&best_id, &best_score)) = candidates
                .iter()
                .max_by(|a, b| a.1.partial_cmp(b.1).unwrap_or(std::cmp::Ordering::Equal))
            {
                // Require minimum score (lower for person-type since surname sharing is strong signal)
                let min_score = if entity_map.get(&island_id).map(|e| e.entity_type.as_str())
                    == Some("person")
                {
                    4.0
                } else {
                    5.0
                };
                if best_score < min_score {
                    continue;
                }
                // Count actual shared tokens (not just score)
                let best_toks = entity_tokens.get(&best_id);
                let shared_count = island_toks
                    .iter()
                    .filter(|t| best_toks.map(|bt| bt.contains(t)).unwrap_or(false))
                    .count();
                // Require ≥2 shared tokens for non-person entities to avoid
                // single-word spurious matches like "Rubik Cube" ↔ "Hybrid Memory Cube"
                let is_person =
                    entity_map.get(&island_id).map(|e| e.entity_type.as_str()) == Some("person");
                if !is_person && shared_count < 2 && island_toks.len() >= 2 {
                    continue;
                }
                let island_name = entity_map
                    .get(&island_id)
                    .map(|e| e.name.as_str())
                    .unwrap_or("?");
                let target_name = entity_map
                    .get(&best_id)
                    .map(|e| e.name.as_str())
                    .unwrap_or("?");
                let island_type = entity_map
                    .get(&island_id)
                    .map(|e| e.entity_type.as_str())
                    .unwrap_or("?");
                let target_type = entity_map
                    .get(&best_id)
                    .map(|e| e.entity_type.as_str())
                    .unwrap_or("?");

                // For person-person matches, require shared LAST name (not just first name).
                // "Frank Sinatra" and "Denis Frank" share "frank" but are unrelated.
                // Require that the shared tokens include the last token of at least one name.
                if island_type == "person" && target_type == "person" {
                    let island_last = island_name
                        .split_whitespace()
                        .last()
                        .unwrap_or("")
                        .to_lowercase();
                    let target_last = target_name
                        .split_whitespace()
                        .last()
                        .unwrap_or("")
                        .to_lowercase();
                    let target_toks = entity_tokens.get(&best_id);
                    let shared_toks: Vec<&String> = island_toks
                        .iter()
                        .filter(|t| target_toks.map(|tt| tt.contains(t)).unwrap_or(false))
                        .collect();
                    // Must share a last name token, not just first names
                    let shares_surname = shared_toks
                        .iter()
                        .any(|t| **t == island_last || **t == target_last);
                    if !shares_surname {
                        continue;
                    }
                    // Require ≥2 shared tokens for person-person (avoid "John X" ↔ "John Y")
                    if shared_toks.len() < 2 && island_toks.len() >= 2 {
                        continue;
                    }
                }

                // For person ↔ non-person matches, require that shared tokens
                // include the person's surname (last word), not just first name.
                // Avoids "Don Backer" ↔ "Don River" (shares "don" = first name only).
                if (island_type == "person") != (target_type == "person") {
                    let (person_name, _person_toks) = if island_type == "person" {
                        (island_name, island_toks.as_slice())
                    } else {
                        (target_name, best_toks.map(|t| t.as_slice()).unwrap_or(&[]))
                    };
                    if person_name.split_whitespace().count() >= 2 {
                        let person_last = person_name
                            .split_whitespace()
                            .last()
                            .unwrap_or("")
                            .to_lowercase();
                        let other_toks = if island_type == "person" {
                            best_toks.map(|t| t.as_slice()).unwrap_or(&[])
                        } else {
                            island_toks.as_slice()
                        };
                        let shares_surname = other_toks.iter().any(|t| *t == person_last);
                        if !shares_surname {
                            continue;
                        }
                    }
                }

                // Skip connections where one entity name contains noise words suggesting
                // it's a sentence fragment, not a real entity (e.g., "Stalin Carpet",
                // "Dead Christ", "Good-bye David Bowie")
                let noise_context_words = [
                    "carpet",
                    "dead",
                    "goodbye",
                    "good-bye",
                    "let",
                    "wealth",
                    "textbooks",
                    "traces",
                    "fever",
                    "goddamn",
                    "god",
                    "particle",
                    "wall",
                    "came",
                    "goes",
                    "down",
                    "preceded",
                    "succeeded",
                    "mint",
                    "re-explained",
                    "piled",
                    "higher",
                    "next-level",
                ];
                let island_lower = island_name.to_lowercase();
                let target_lower = target_name.to_lowercase();
                let has_noise_context = island_lower
                    .split_whitespace()
                    .any(|w| noise_context_words.contains(&w))
                    || target_lower
                        .split_whitespace()
                        .any(|w| noise_context_words.contains(&w));
                if has_noise_context {
                    continue;
                }

                // Determine predicate based on types
                let predicate = infer_predicate(island_type, target_type, None);

                eprintln!(
                    "  [token-reconnect] {} → {} (score: {:.2}, pred: {})",
                    island_name, target_name, best_score, predicate
                );
                self.brain.upsert_relation(
                    island_id,
                    predicate,
                    best_id,
                    "prometheus:token_reconnect",
                )?;
                reconnected += 1;
                if reconnected >= 200 {
                    break; // Cap per run
                }
            }
        }
        Ok(reconnected)
    }

    /// Connect island entities to connected entities of the same type where the
    /// island name is a substring of the connected entity name (or vice versa),
    /// weighted by name overlap ratio. E.g., island "Fourier Transform" → connected
    /// "Discrete Fourier Transform" with high confidence.
    /// Returns count of new relations created.
    pub fn reconnect_islands_by_name_containment(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        let mut connected: HashSet<i64> = HashSet::new();
        let mut edges: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            edges.insert(key);
        }

        // Build type → connected entities index
        let mut type_connected: HashMap<String, Vec<(i64, String)>> = HashMap::new();
        for e in &entities {
            if meaningful.contains(&e.id)
                && connected.contains(&e.id)
                && !is_noise_type(&e.entity_type)
                && !is_noise_name(&e.name)
                && e.name.split_whitespace().count() >= 2
            {
                type_connected
                    .entry(e.entity_type.clone())
                    .or_default()
                    .push((e.id, e.name.to_lowercase()));
            }
        }

        let mut reconnected = 0usize;
        for e in &entities {
            if connected.contains(&e.id)
                || !meaningful.contains(&e.id)
                || is_noise_type(&e.entity_type)
                || is_noise_name(&e.name)
                || e.name.split_whitespace().count() < 2
            {
                continue;
            }
            let island_lower = e.name.to_lowercase();
            let island_words: usize = island_lower.split_whitespace().count();

            // Look for same-type connected entities where one name contains the other
            if let Some(candidates) = type_connected.get(&e.entity_type) {
                let mut best: Option<(i64, f64)> = None;
                for (cid, cname) in candidates {
                    let key = if e.id < *cid {
                        (e.id, *cid)
                    } else {
                        (*cid, e.id)
                    };
                    if edges.contains(&key) {
                        continue;
                    }
                    let cand_words: usize = cname.split_whitespace().count();

                    // One must contain the other, and the shorter must be ≥2 words
                    let (shorter, longer): (&str, &str) = if island_words <= cand_words {
                        (island_lower.as_str(), cname.as_str())
                    } else {
                        (cname.as_str(), island_lower.as_str())
                    };
                    let shorter_words = shorter.split_whitespace().count();
                    if shorter_words < 2 {
                        continue;
                    }
                    if longer.contains(shorter) {
                        // Overlap ratio: how much of the longer name is covered
                        let ratio = shorter.len() as f64 / longer.len().max(1) as f64;
                        if ratio > 0.4 && (best.is_none() || ratio > best.unwrap().1) {
                            best = Some((*cid, ratio));
                        }
                    }
                }

                if let Some((target_id, _ratio)) = best {
                    let predicate = if island_words
                        < type_connected
                            .get(&e.entity_type)
                            .map(|v| {
                                v.iter()
                                    .find(|(id, _)| *id == target_id)
                                    .map(|(_, n)| n.split_whitespace().count())
                                    .unwrap_or(0)
                            })
                            .unwrap_or(0)
                    {
                        "broader_form_of"
                    } else {
                        "specific_form_of"
                    };
                    self.brain.upsert_relation(
                        e.id,
                        predicate,
                        target_id,
                        "prometheus:name_containment_reconnect",
                    )?;
                    reconnected += 1;
                    if reconnected >= 150 {
                        break;
                    }
                }
            }
        }
        Ok(reconnected)
    }

    /// Reconnect single-word island entities to connected entities whose name contains
    /// that word as a significant token. E.g., island "Entropy" → connected "Shannon Entropy".
    /// Uses type-aware scoring: same-type matches are preferred, and high-value types
    /// (person, place, concept, technology) get a boost.
    /// Only reconnects when the match is unambiguous (one clearly best candidate).
    /// Merge entities in different connected components that share the same name.
    /// This directly reduces graph fragmentation by unifying duplicate entities
    /// that ended up in separate components due to different source pages.
    pub fn merge_cross_component_duplicates(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let components = crate::graph::connected_components(self.brain)?;
        if components.len() < 2 {
            return Ok(0);
        }

        // Map entity_id → component_index
        let mut entity_component: HashMap<i64, usize> = HashMap::new();
        for (ci, comp) in components.iter().enumerate() {
            for &eid in comp {
                entity_component.insert(eid, ci);
            }
        }

        // Group entities by lowercase name
        let mut name_groups: HashMap<String, Vec<(i64, usize)>> = HashMap::new();
        for e in &entities {
            if is_noise_type(&e.entity_type) || is_noise_name(&e.name) || e.name.len() < 3 {
                continue;
            }
            let lower = e.name.to_lowercase();
            let comp_idx = entity_component.get(&e.id).copied().unwrap_or(usize::MAX);
            name_groups.entry(lower).or_default().push((e.id, comp_idx));
        }

        let mut merged = 0usize;
        for group in name_groups.values() {
            if group.len() < 2 {
                continue;
            }
            // Find the entity in the largest component (most connected)
            let best = group
                .iter()
                .min_by_key(|(_, ci)| {
                    // Lower component index = larger component (sorted desc by size)
                    *ci
                })
                .copied();
            if let Some((keep_id, keep_comp)) = best {
                for &(eid, comp_idx) in group {
                    if eid != keep_id && comp_idx != keep_comp {
                        self.brain.merge_entities(eid, keep_id)?;
                        merged += 1;
                    }
                }
            }
        }
        Ok(merged)
    }

    pub fn reconnect_single_word_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        let mut connected: HashSet<i64> = HashSet::new();
        let mut edges: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            edges.insert(key);
        }

        // Build inverted index: lowercase token → list of connected entity IDs
        let entity_map: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();
        let mut token_to_connected: HashMap<String, Vec<i64>> = HashMap::new();
        for e in &entities {
            if !meaningful.contains(&e.id)
                || !connected.contains(&e.id)
                || is_noise_type(&e.entity_type)
                || is_noise_name(&e.name)
            {
                continue;
            }
            // Index each significant word token
            for word in e.name.split_whitespace() {
                let lower = word.to_lowercase();
                if lower.len() >= 3 {
                    token_to_connected.entry(lower).or_default().push(e.id);
                }
            }
        }

        // Collect single-word island entities that are meaningful
        let islands: Vec<&crate::db::Entity> = entities
            .iter()
            .filter(|e| {
                meaningful.contains(&e.id)
                    && !connected.contains(&e.id)
                    && !is_noise_type(&e.entity_type)
                    && !is_noise_name(&e.name)
                    && e.name.split_whitespace().count() == 1
                    && e.name.len() >= 3
            })
            .collect();

        let mut reconnected = 0usize;
        for island in &islands {
            let island_lower = island.name.to_lowercase();

            // Find connected entities containing this word
            let candidates = match token_to_connected.get(&island_lower) {
                Some(c) if !c.is_empty() => c,
                _ => continue,
            };

            // Too many matches = too generic, skip
            if candidates.len() > 20 {
                continue;
            }

            // Score candidates: prefer same type, prefer shorter names (more specific match)
            let mut scored: Vec<(i64, f64)> = Vec::new();
            for &cid in candidates {
                let key = if island.id < cid {
                    (island.id, cid)
                } else {
                    (cid, island.id)
                };
                if edges.contains(&key) {
                    continue;
                }
                let cand = match entity_map.get(&cid) {
                    Some(e) => e,
                    None => continue,
                };
                let mut score = 1.0_f64;
                // Same type bonus
                if cand.entity_type == island.entity_type {
                    score += 2.0;
                }
                // High-value type bonus
                if HIGH_VALUE_TYPES.contains(&cand.entity_type.as_str()) {
                    score += 0.5;
                }
                // Shorter name = more specific match (word is bigger fraction of name)
                let name_words = cand.name.split_whitespace().count().max(1) as f64;
                score += 1.0 / name_words;
                // Exact word-boundary match bonus (not just substring)
                let cand_tokens: Vec<String> = cand
                    .name
                    .split_whitespace()
                    .map(|w| w.to_lowercase())
                    .collect();
                if cand_tokens.contains(&island_lower) {
                    score += 1.0;
                }
                scored.push((cid, score));
            }

            if scored.is_empty() {
                continue;
            }
            scored.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

            // Only reconnect if best candidate is clearly better than runner-up
            let best_score = scored[0].1;
            let runner_up = scored.get(1).map(|s| s.1).unwrap_or(0.0);
            if scored.len() > 1 && best_score < runner_up * 1.3 {
                continue; // Ambiguous — skip
            }

            let target_id = scored[0].0;
            let target_name = entity_map
                .get(&target_id)
                .map(|e| e.name.as_str())
                .unwrap_or("?");
            let predicate = match (
                island.entity_type.as_str(),
                entity_map
                    .get(&target_id)
                    .map(|e| e.entity_type.as_str())
                    .unwrap_or("?"),
            ) {
                ("person", "person") => "related_person",
                ("place", "place") => "associated_with",
                ("concept", "concept") => "related_concept",
                ("technology", "technology") => "related_technology",
                _ => "associated_with",
            };

            eprintln!(
                "  [single-word-reconnect] {} ({}) → {} (score: {:.2}, pred: {})",
                island.name, island.entity_type, target_name, best_score, predicate
            );
            self.brain.upsert_relation(
                island.id,
                predicate,
                target_id,
                "prometheus:single_word_reconnect",
            )?;
            reconnected += 1;
            if reconnected >= 300 {
                break;
            }
        }
        Ok(reconnected)
    }

    /// Purge island entities mistyped as "person" that are clearly not people.
    /// Catches patterns like "Gravitational Waves" (concept), "Modern Japan" (place/concept),
    /// "Collected Works" (concept), etc.
    pub fn purge_mistyped_person_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Non-person indicators: words that strongly suggest the entity is NOT a person
        let concept_words: HashSet<&str> = [
            "waves",
            "theory",
            "theorem",
            "equation",
            "equations",
            "method",
            "methods",
            "algorithm",
            "process",
            "effect",
            "phenomenon",
            "principle",
            "law",
            "laws",
            "paradox",
            "conjecture",
            "hypothesis",
            "model",
            "formula",
            "series",
            "function",
            "transform",
            "distribution",
            "constant",
            "number",
            "numbers",
            "problem",
            "space",
            "group",
            "ring",
            "field",
            "operator",
            "integral",
            "differential",
            "system",
            "systems",
            "machine",
            "engine",
            "device",
            "network",
            "protocol",
            "code",
            "language",
            "architecture",
            "framework",
            "structure",
            "works",
            "collection",
            "collected",
            "correspondence",
            "letters",
            "papers",
            "manuscript",
            "text",
            "book",
            "edition",
            "volume",
            "treaty",
            "declaration",
            "manifesto",
            "charter",
            "revolution",
            "movement",
            "campaign",
            "battle",
            "war",
            "siege",
            "invasion",
            "expedition",
            "conquest",
            "migration",
            "diaspora",
            "genocide",
            "massacre",
            "crisis",
            "reform",
            "policy",
            "agreement",
            "alliance",
            "coalition",
            "prices",
            "trade",
            "market",
            "economy",
            "currency",
            "values",
            "money",
            "modern",
            "ancient",
            "medieval",
            "classical",
            "contemporary",
            "early",
            "late",
            "online",
            "digital",
            "virtual",
            "mobile",
            "wireless",
            "syndrome",
            "doctrine",
            "ideology",
            "heresy",
            "orthodoxy",
            "islam",
            "christianity",
            "buddhism",
            "hinduism",
            "judaism",
            "crown",
            "throne",
            "dynasty",
            "braid",
            "golden",
            "eternal",
            "sacred",
            "holy",
            "divine",
            "celestial",
            "infernal",
            "captains",
            "great",
            "grand",
            "royal",
            "imperial",
            "nuovo",
            "cimento",
            "prinzipien",
            "physikalische",
            "friedhöfen",
            "minería",
            "sublemma",
            "raspberry",
            "powerful",
            "dissenting",
            "academies",
            "academy",
            "penny",
            "post",
            "exchange",
            "steam",
            "power",
            "fire",
            "trials",
            "brothers",
            // German institutional suffixes
            "gesellschaft",
            "hochschule",
            "nachrichten",
            "jahrbuch",
            "akademie",
            "verein",
            "zeitschrift",
            "institut",
            "bibliothek",
            "archiv",
            "anzeiger",
            "berichte",
            "abhandlungen",
            "kommmission",
            "kommission",
            "cultusgemeinde",
            "gemeinde",
            "figuren",
            "handbuch",
            "wörterbuch",
            "lexikon",
            // Latin document/treaty terms
            "instrumentum",
            "pacis",
            "universalis",
            "cosmographia",
            "encyclopedia",
            "dictionary",
            // Past participles indicating sentence fragments
            "disappeared",
            "commissioned",
            "collected",
            "completed",
            "established",
            "organized",
            "published",
            "translated",
            "compiled",
            "ratified",
            "succeeded",
            // Cross-cultural / descriptive terms
            "cross-cultural",
            "influences",
            "philanthropy",
            "educational",
            "electronic",
            "historical",
            // Common non-person terms found in NLP extraction errors
            "access",
            "user",
            "plans",
            "prewar",
            "staff",
            "ego",
            "productions",
            "smashers",
            "graves",
            "mass",
            "sex",
            "gentle",
            "imprecision",
            "trees",
            "counted",
            "ministerium",
            "bildung",
            "stationen",
            "dokumente",
            "elenco",
            "cronologico",
            "eclogae",
            "geologicae",
            "cuarta",
            "guerra",
            "carlista",
            "rainforests",
            "tropical",
            "hotel",
            "inertia",
            "isla",
        ]
        .into_iter()
        .collect();

        let place_words: HashSet<&str> = [
            "japan",
            "china",
            "india",
            "europe",
            "asia",
            "africa",
            "america",
            "russia",
            "france",
            "germany",
            "england",
            "spain",
            "italy",
            "greece",
            "egypt",
            "persia",
            "arabia",
            "ottoman",
            "byzantine",
            "roman",
            "british",
            "french",
            "german",
            "danish",
            "english",
            "swedish",
            "norwegian",
            "finnish",
            "dutch",
            "polish",
            "czech",
            "hungarian",
            "austrian",
            "swiss",
            "scottish",
            "irish",
            "welsh",
        ]
        .into_iter()
        .collect();

        let mut purged = 0usize;
        for e in &entities {
            if e.entity_type != "person" || connected.contains(&e.id) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue; // Single-word handled elsewhere
            }
            let lower_words: Vec<String> = words.iter().map(|w| w.to_lowercase()).collect();

            // Check if any word is a strong non-person indicator
            let has_concept_word = lower_words
                .iter()
                .any(|w| concept_words.contains(w.as_str()));
            let has_place_word = lower_words.iter().any(|w| place_words.contains(w.as_str()));

            // Names with non-ASCII characters that look like non-English phrases (not person names)
            // Check original-case words (not lower_words!) so "André Weil" passes the uppercase check
            let has_non_english_phrase = e
                .name
                .chars()
                .any(|c| matches!(c, 'ü' | 'ö' | 'ä' | 'é' | 'è' | 'ñ' | 'í'))
                && words.len() >= 3  // 2-word names with diacritics are usually real people
                && !words.iter().all(|w| {
                    w.chars()
                        .next()
                        .is_some_and(|c| c.is_uppercase() || w.len() <= 3)
                });
            // Check for all-lowercase multi-word (sentence fragment, not a name)
            let all_lowercase_words = words
                .iter()
                .all(|w| w.chars().next().is_some_and(|c| c.is_lowercase()));

            // Detect non-name patterns:
            // 1. Name ending with single uppercase letter (citation: "Washbrook D", "Cebon Peter")
            let ends_single_letter = words.len() >= 2
                && words
                    .last()
                    .is_some_and(|w| w.len() == 1 && w.chars().all(|c| c.is_uppercase()));
            // 2. Abbreviation endings ("Madison Univ", "Rear Adm")
            let abbrev_endings = [
                "univ", "adm", "dept", "inst", "corp", "assoc", "intl", "natl", "govt",
            ];
            let ends_abbrev = words
                .last()
                .is_some_and(|w| abbrev_endings.contains(&w.to_lowercase().as_str()));
            // 3. Plural nouns as last word (not person names: "Mass Graves", "Atom Smashers", "B-Trees")
            let plural_last = words.last().is_some_and(|w| {
                let lw = w.to_lowercase();
                lw.len() > 4 && lw.ends_with('s') && !lw.ends_with("ss") && !lw.ends_with("us")
                && !lw.ends_with("is") && !lw.ends_with("as")
                // Check it's not a common surname ending in 's'
                && !["adams", "james", "jones", "williams", "davis", "harris",
                     "lewis", "thomas", "evans", "roberts", "phillips", "edwards",
                     "morris", "hughes", "rogers", "jenkins", "burns", "mills",
                     "matthews", "sanders", "richards", "stevens", "daniels",
                     "reynolds", "chambers", "simmons", "parsons", "watkins",
                     "briggs", "sims", "reeves", "hayes", "rhodes", "sparks",
                     "meadows", "saunders", "hodges", "lyons", "cyrus", "ames",
                     "cummings", "jennings", "hastings", "hawkins", "clemens",
                     "rawlings", "dickens", "collins", "owens", "burroughs",
                     "descartes", "cervantes", "borges", "soares", "fernandes",
                     "martins", "nunes", "gomes", "lopes", "abeles",
                     ].contains(&lw.as_str())
            });
            // 4. Contains hyphen in non-name pattern ("B-Trees", "Cross-Cultural")
            let has_nonname_hyphen = e.name.contains('-')
                && words.iter().any(|w| {
                    let lw = w.to_lowercase();
                    lw.contains('-') && !lw.chars().next().is_some_and(|c| c.is_uppercase())
                });
            // 5. "Elenco Cronologico", "Stationen Dokumente" — multi-word non-English phrases
            //    that aren't person names: no word looks like a first/last name
            let looks_non_name_phrase = words.len() >= 2
                && words.iter().all(|w| w.len() > 3)
                && !words.iter().any(|w| {
                    // Check if word could be a first name (starts uppercase, 3-12 chars, all alpha)
                    let lw = w.to_lowercase();
                    w.len() >= 3
                        && w.len() <= 12
                        && w.chars().next().is_some_and(|c| c.is_uppercase())
                        && w.chars().all(|c| c.is_alphabetic())
                        && !concept_words.contains(lw.as_str())
                        && !place_words.contains(lw.as_str())
                })
                && e.confidence < 0.7;

            // 6. Organization-like suffixes ("Firefly Aerospace", "Circolo Matematico", "Matematica Italiana")
            let org_suffixes: HashSet<&str> = [
                "aerospace",
                "airlines",
                "airways",
                "associates",
                "automotive",
                "bank",
                "brewing",
                "brothers",
                "capital",
                "chemicals",
                "clinic",
                "club",
                "co",
                "college",
                "comics",
                "commission",
                "committee",
                "communications",
                "company",
                "consortium",
                "consulting",
                "corp",
                "corporation",
                "council",
                "dynamics",
                "electronics",
                "energy",
                "engineering",
                "ensemble",
                "entertainment",
                "enterprises",
                "factory",
                "federation",
                "films",
                "foundation",
                "fund",
                "gallery",
                "games",
                "group",
                "healthcare",
                "holdings",
                "hospital",
                "inc",
                "incorporated",
                "industries",
                "initiative",
                "instruments",
                "insurance",
                "international",
                "italiana",
                "italiano",
                "laboratoire",
                "laboratorio",
                "laboratories",
                "laboratory",
                "labs",
                "league",
                "library",
                "limited",
                "llc",
                "ltd",
                "manufacturing",
                "matematica",
                "matematico",
                "media",
                "microsystems",
                "motors",
                "museum",
                "nacional",
                "network",
                "networks",
                "observatory",
                "orchestra",
                "organization",
                "partnership",
                "pharmaceuticals",
                "pharmacy",
                "pictures",
                "plc",
                "press",
                "productions",
                "programs",
                "project",
                "publishing",
                "records",
                "research",
                "restaurant",
                "robotics",
                "school",
                "sciences",
                "semiconductor",
                "seminary",
                "services",
                "shipping",
                "software",
                "solutions",
                "society",
                "squadron",
                "stadium",
                "station",
                "studios",
                "systems",
                "teatro",
                "tech",
                "technologies",
                "technology",
                "telecom",
                "therapeutics",
                "trust",
                "union",
                "university",
                "ventures",
                "werkstatt",
            ]
            .into_iter()
            .collect();
            let ends_org_suffix = words
                .last()
                .is_some_and(|w| org_suffixes.contains(w.to_lowercase().as_str()))
                && words.len() >= 2;

            // 7. Starts with "St" / "San" / "Santa" / "Nea" / "Syr" — likely place names
            let place_prefixes = [
                "st", "san", "santa", "nea", "syr", "fort", "mount", "cape", "port", "lago", "isla",
            ];
            let starts_place_prefix = words
                .first()
                .is_some_and(|w| place_prefixes.contains(&w.to_lowercase().as_str()))
                && words.len() >= 2;

            // 8. Non-name suffixes: "Weather Forecast", "Birth Rate", "Time Zone", etc.
            let nonname_suffixes: HashSet<&str> = [
                "forecast",
                "report",
                "rate",
                "zone",
                "index",
                "ratio",
                "average",
                "estimate",
                "survey",
                "census",
                "registry",
                "catalogue",
                "catalog",
                "inventory",
                "listing",
                "schedule",
                "brief",
                "briefe",
                "bericht",
                "berichte",
                "verzeichnis",
                "übersicht",
                "tabelle",
                "protokoll",
            ]
            .into_iter()
            .collect();
            let ends_nonname_suffix = words
                .last()
                .is_some_and(|w| nonname_suffixes.contains(w.to_lowercase().as_str()));

            // 9. Latin/German document title patterns (multi-word, all capitalized, non-name)
            let latin_doc_words: HashSet<&str> = [
                "liber",
                "libri",
                "opus",
                "opera",
                "acta",
                "annales",
                "historia",
                "rerum",
                "naturalis",
                "gestarum",
                "bellum",
                "civile",
                "de",
                "re",
                "ad",
                "cum",
                "pro",
                "contra",
                "per",
                "sub",
                "commentarii",
                "epistulae",
                "fragmenta",
                "collectanea",
                "unbekannte",
                "gesammelte",
                "sämtliche",
                "ausgewählte",
            ]
            .into_iter()
            .collect();
            let is_latin_doc = words.len() >= 2
                && e.confidence < 0.7
                && lower_words
                    .iter()
                    .filter(|w| latin_doc_words.contains(w.as_str()))
                    .count()
                    >= 2;

            // 10. Foreign-language institutional/article prefixes that indicate non-person entities
            // "Biblioteca Nazionale" (library), "Det Kongelige Bibliotek" (royal library),
            // "Het Brandende Eiland" (the burning island), "Museo Chiaramonti" (museum)
            let institutional_prefixes: &[&str] = &[
                "biblioteca",
                "bibliothèque",
                "bibliothek",
                "museo",
                "musée",
                "société",
                "académie",
                "école",
                "città",
                "palazzo",
                "teatro",
                "basilica",
                "cattedrale",
                "château",
                "abbaye",
                "monastère",
                "hospice",
                "hôpital",
                "lycée",
                "collège",
                "faculté",
                "istituto",
                "accademia",
                "università",
                "circolo",
                // Articles / demonstratives (non-English) — these start noun phrases, not names
                "het",
                "det",
                "das",
                "der",
                "die",
                "les",
                "los",
                "las",
            ];
            let starts_institutional = words.len() >= 2
                && institutional_prefixes.contains(&lower_words[0].as_str())
                && e.confidence < 0.8;

            // 11. Ends with place/building suffixes in non-English languages
            let foreign_place_suffixes: &[&str] = &[
                "eiland",
                "insel",
                "inselsberg",
                "gebirge",
                "wald",
                "fluss",
                "kloster",
                "kirche",
                "schloss",
                "turm",
                "brücke",
                "platz",
                "strasse",
                "straße",
                "gasse",
                "markt",
                "hafen",
                "île",
                "rivière",
                "montagne",
                "forêt",
                "pont",
                "porte",
                "piazza",
                "ponte",
                "torre",
                "porta",
                "isola",
                "lago",
                "mezquita",
                "alcázar",
                "catedral",
                "monasterio",
                "castillo",
            ];
            let ends_foreign_place = words.len() >= 2
                && foreign_place_suffixes.contains(&lower_words.last().unwrap().as_str());

            if has_concept_word
                || has_place_word
                || (has_non_english_phrase && e.confidence < 0.8)
                || (all_lowercase_words && words.len() >= 2)
                || ends_single_letter
                || ends_abbrev
                || (plural_last && !has_place_word && words.len() <= 3)
                || has_nonname_hyphen
                || looks_non_name_phrase
                || ends_org_suffix
                || starts_place_prefix
                || ends_nonname_suffix
                || is_latin_doc
                || starts_institutional
                || ends_foreign_place
            {
                eprintln!(
                    "  [mistyped-person-purge] deleting island '{}' (id={})",
                    e.name, e.id
                );
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    Ok(())
                })?;
                purged += 1;
            }
        }
        Ok(purged)
    }

    /// Fix entities that are concatenated country/place names mistyped as persons.
    /// E.g., "Switzerland Germany Italy" (person) → delete or retype to "place".
    /// Purge low-confidence concept islands: isolated concept entities with
    /// confidence < threshold and no facts are almost certainly NLP extraction noise.
    /// This is more aggressive than suffix-based purging — it uses confidence as signal.
    pub fn purge_low_confidence_concept_islands(&self, max_confidence: f64) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        let mut purged = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            if e.entity_type != "concept" && e.entity_type != "unknown" {
                continue;
            }
            if e.confidence > max_confidence {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let name = e.name.trim();
            let words: Vec<&str> = name.split_whitespace().collect();
            let should_purge = if words.len() >= 3 {
                true
            } else if words.len() == 1 {
                let is_lowercase_start = name.chars().next().is_some_and(|c| c.is_lowercase());
                is_lowercase_start || name.len() <= 4
            } else if words.len() == 2 && e.confidence <= 0.5 {
                words
                    .iter()
                    .all(|w| w.chars().next().is_some_and(|c| c.is_lowercase()))
            } else {
                false
            };
            if should_purge {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    Ok(())
                })?;
                purged += 1;
            }
        }
        if purged > 0 {
            eprintln!("  [low-conf-concept-purge] removed {} low-confidence concept islands (threshold {:.2})", purged, max_confidence);
        }
        Ok(purged)
    }

    /// Purge single-word participle/adjective entities that are not real concepts.
    /// These are NLP extraction noise like "Entangled", "Interacting", "Feathered"
    /// that accumulate relations but contribute no real knowledge.
    /// Deletes the entity and all its relations.
    pub fn purge_participle_concept_entities(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let mut purged = 0usize;

        // Known proper nouns that look like participles but aren't
        let exceptions: HashSet<&str> = [
            "turing",
            "beijing",
            "reading",
            "stirling",
            "darjeeling",
            "nanjing",
            "chongqing",
            "washington",
            "wellington",
            "nottingham",
            "birmingham",
            "buckingham",
            "manning",
            "browning",
            "kipling",
            "lessing",
            "göttingen",
            "united",
            "inspired",
            "advanced",
            "applied",
            "distributed",
            "embedded",
            "integrated",
            "connected",
            "associated",
            "collected",
            "organized",
            "combined",
            "structured",
        ]
        .iter()
        .copied()
        .collect();

        for e in &entities {
            if e.entity_type != "concept" && e.entity_type != "unknown" {
                continue;
            }
            let name = e.name.trim();
            let words: Vec<&str> = name.split_whitespace().collect();
            if words.len() != 1 {
                continue;
            }
            let lower = name.to_lowercase();
            if exceptions.contains(lower.as_str()) {
                continue;
            }
            // Must look like a participle or adjective (not a proper noun concept)
            let is_participle = (lower.ends_with("ed") && lower.len() >= 6)
                || (lower.ends_with("ing") && lower.len() >= 7)
                || (lower.ends_with("ated") && lower.len() >= 7)
                || (lower.ends_with("ized") && lower.len() >= 7)
                || (lower.ends_with("ting") && lower.len() >= 7);
            if !is_participle {
                continue;
            }
            // Only purge low-confidence entities
            if e.confidence > 0.7 {
                continue;
            }
            // Check it has no valuable facts (keyword-only facts don't count)
            let facts = self.brain.get_facts_for(e.id)?;
            let has_real_facts = facts.iter().any(|f| f.key != "keyword");
            if has_real_facts {
                continue;
            }
            // Delete relations, facts, and entity
            self.brain.with_conn(|conn| {
                conn.execute(
                    "DELETE FROM relations WHERE subject_id = ?1 OR object_id = ?1",
                    params![e.id],
                )?;
                conn.execute("DELETE FROM facts WHERE entity_id = ?1", params![e.id])?;
                conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                Ok(())
            })?;
            purged += 1;
        }
        if purged > 0 {
            eprintln!(
                "  [participle-purge] removed {} single-word participle/adjective concept entities",
                purged
            );
        }
        Ok(purged)
    }

    /// Reclassify concept islands that are actually places by checking if their name
    /// exactly matches a connected place entity name.
    pub fn reclassify_concept_islands_as_places(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        let place_names: HashSet<String> = entities
            .iter()
            .filter(|e| connected.contains(&e.id) && e.entity_type == "place")
            .map(|e| e.name.to_lowercase())
            .collect();

        // Geographic suffixes common in European place names (German, Swiss, Nordic, etc.)
        let geo_suffixes: &[&str] = &[
            "brunnen", "berg", "burg", "burg", "stein", "wald", "feld", "dorf", "bach", "bruck",
            "brücke", "hausen", "heim", "hofen", "kirchen", "stadt", "stetten", "weil", "weiler",
            "wil", "ikon", "iken", "ikon", "ach", "au", "ow", "in", "itz", "witz", "grad", "gorod",
            "abad", "pur", "pura", "garh", "fjord", "vik", "ness", "holm", "by", "lund", "polis",
            "chester", "cester", "minster", "bury", "mouth", "haven", "ford", "bridge", "land",
            "dale", "vale", "ville", "bourg", "mont",
        ];

        // Common standalone place-like single words (historical regions, geographic features)
        let standalone_places: HashSet<&str> = [
            "placentia",
            "dertosa",
            "temeswar",
            "khwarezmia",
            "santillana",
            "lauterbrunnen",
            "uetliberg",
        ]
        .into_iter()
        .collect();

        let mut reclassified = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            if e.entity_type != "concept" {
                continue;
            }
            let lower = e.name.to_lowercase();

            // Match 1: exact name match with connected place
            let is_known_place = place_names.contains(&lower);

            // Match 2: geographic suffix detection (single-word, starts uppercase, ≥6 chars)
            let has_geo_suffix = !lower.contains(' ')
                && lower.len() >= 6
                && e.name.chars().next().is_some_and(|c| c.is_uppercase())
                && geo_suffixes.iter().any(|suf| {
                    // Require suffix to be at most half the word to avoid false positives
                    suf.len() <= lower.len() / 2 && lower.ends_with(suf)
                });

            // Match 3: known standalone place names
            let is_standalone = standalone_places.contains(lower.as_str());

            if is_known_place || has_geo_suffix || is_standalone {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE entities SET entity_type = 'place' WHERE id = ?1",
                        params![e.id],
                    )?;
                    Ok(())
                })?;
                reclassified += 1;
            }
        }
        if reclassified > 0 {
            eprintln!(
                "  [concept→place] reclassified {} concept islands (name match + geo suffix)",
                reclassified
            );
        }
        Ok(reclassified)
    }

    /// Also merges connected country-prefixed person entities into their canonical form.
    /// E.g., "Netherlands Oskar Klein" (11 rels) → merge into "Oskar Klein".
    /// Returns count of entities fixed.
    pub fn fix_country_concatenation_entities(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        let country_names: HashSet<&str> = [
            "netherlands",
            "switzerland",
            "germany",
            "france",
            "italy",
            "spain",
            "portugal",
            "belgium",
            "austria",
            "sweden",
            "norway",
            "denmark",
            "finland",
            "poland",
            "hungary",
            "romania",
            "bulgaria",
            "serbia",
            "croatia",
            "greece",
            "turkey",
            "russia",
            "ukraine",
            "japan",
            "korea",
            "taiwan",
            "china",
            "india",
            "brazil",
            "mexico",
            "egypt",
            "israel",
            "iran",
            "iraq",
            "prussia",
            "saxony",
            "bavaria",
            "bohemia",
            "england",
            "scotland",
            "ireland",
            "wales",
            "canada",
            "australia",
            "europe",
            "asia",
            "africa",
            "america",
            "crimea",
            "balkans",
            "caucasus",
            "mongolia",
            "persia",
            "arabia",
            "ottoman",
        ]
        .iter()
        .copied()
        .collect();

        let city_names: HashSet<&str> = [
            "athens",
            "sparta",
            "corinth",
            "thebes",
            "argos",
            "aegina",
            "mantinea",
            "rome",
            "paris",
            "london",
            "berlin",
            "vienna",
            "zurich",
            "moscow",
            "constantinople",
            "antioch",
            "alexandria",
            "trebizond",
            "jerusalem",
            "baghdad",
            "cairo",
            "delhi",
            "beijing",
            "tokyo",
            "florence",
            "venice",
            "naples",
            "milan",
            "geneva",
            "basel",
            "bern",
            "lyon",
            "marseille",
            "madrid",
            "lisbon",
            "amsterdam",
            "brussels",
            "hamburg",
            "munich",
            "prague",
            "budapest",
            "warsaw",
            "stockholm",
            "copenhagen",
            "oslo",
            "baltimore",
            "boston",
            "philadelphia",
            "washington",
            "manhattan",
            "maryland",
            "pennsylvania",
            "delaware",
            "virginia",
            "carolina",
            "corfu",
            "crete",
            "rhodes",
            "cyprus",
            "sicily",
            "sardinia",
            "mycenae",
            "bergen",
            "dresden",
            "leipzig",
            "nuremberg",
        ]
        .iter()
        .copied()
        .collect();

        // Build name → (id, degree) lookup for merge targets
        let mut name_lookup: HashMap<String, (i64, usize)> = HashMap::new();
        for e in &entities {
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            let lower = e.name.to_lowercase();
            let entry = name_lookup.entry(lower).or_insert((e.id, deg));
            if deg > entry.1 {
                *entry = (e.id, deg);
            }
        }

        let mut fixed = 0usize;
        for e in &entities {
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }
            let lower_words: Vec<String> = words.iter().map(|w| w.to_lowercase()).collect();

            // Case 1: ALL words are country/place names → this is not a valid entity
            // E.g., "Switzerland Germany Italy", "Japan India", "Netherlands Switzerland"
            // Also catches city concatenations mistyped as person: "Sparta Corinth Aegina"
            let all_countries = lower_words
                .iter()
                .all(|w| country_names.contains(w.as_str()) || city_names.contains(w.as_str()));
            if all_countries && (e.entity_type == "person" || e.entity_type == "concept") {
                let deg = degree.get(&e.id).copied().unwrap_or(0);
                if deg <= 2 {
                    // Low connectivity — safe to delete
                    eprintln!(
                        "  [country-concat] deleting all-country person '{}' (id={}, deg={})",
                        e.name, e.id, deg
                    );
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "DELETE FROM relations WHERE subject_id = ?1 OR object_id = ?1",
                            params![e.id],
                        )?;
                        conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                        Ok(())
                    })?;
                    fixed += 1;
                } else {
                    // Has many connections — retype to "place" instead of deleting
                    eprintln!(
                        "  [country-concat] retyping all-country person '{}' → place (id={}, deg={})",
                        e.name, e.id, deg
                    );
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "UPDATE entities SET entity_type = 'place' WHERE id = ?1",
                            params![e.id],
                        )?;
                        Ok(())
                    })?;
                    fixed += 1;
                }
                continue;
            }

            // Case 2: First word is a country name, rest looks like a real person name
            // E.g., "Netherlands Oskar Klein" → merge into "Oskar Klein"
            if e.entity_type == "person"
                && words.len() >= 3
                && country_names.contains(lower_words[0].as_str())
            {
                // Check if the non-country remainder is a proper name (starts with uppercase)
                let remainder = words[1..].join(" ");
                let remainder_lower = remainder.to_lowercase();
                if let Some(&(target_id, _)) = name_lookup.get(&remainder_lower) {
                    if target_id != e.id {
                        eprintln!(
                            "  [country-prefix] merging '{}' (id={}) → '{}' (id={})",
                            e.name, e.id, remainder, target_id
                        );
                        self.brain.merge_entities(e.id, target_id)?;
                        fixed += 1;
                    }
                }
            }
        }
        Ok(fixed)
    }

    /// Merge prefix-noise entities: longer entity names that end with a known shorter
    /// entity name, where the prefix is noise (e.g., "Devastated Tim Berners-Lee" → "Tim Berners-Lee",
    /// "Cannes Dev Patel" → "Dev Patel", "Moscow East Berlin" → "East Berlin").
    /// Only merges when the shorter name exists as a separate entity of the same type
    /// and the longer entity has low connectivity (likely an NLP extraction error).
    pub fn merge_prefix_noise_entities(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build name → (id, degree) lookup
        let mut name_lookup: HashMap<String, (i64, usize)> = HashMap::new();
        for e in &entities {
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            let lower = e.name.to_lowercase();
            let entry = name_lookup.entry(lower).or_insert((e.id, deg));
            if deg > entry.1 {
                *entry = (e.id, deg);
            }
        }

        let mut merged = 0usize;
        for e in &entities {
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 3 {
                continue;
            }
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            if deg > 10 {
                continue; // skip highly connected entities
            }

            // Try dropping 1 or 2 prefix words and check if remainder exists
            for skip in 1..=(words.len() - 2).min(2) {
                let remainder = words[skip..].join(" ");
                let remainder_lower = remainder.to_lowercase();
                let remainder_words = words.len() - skip;

                // Remainder must be at least 2 words (to avoid false positives)
                if remainder_words < 2 {
                    continue;
                }

                if let Some(&(target_id, target_deg)) = name_lookup.get(&remainder_lower) {
                    if target_id == e.id {
                        continue;
                    }
                    // Target must have same type or be more connected
                    let target_entity = entities.iter().find(|x| x.id == target_id);
                    if let Some(target) = target_entity {
                        if target.entity_type != e.entity_type {
                            continue;
                        }
                    }
                    // Merge if the prefix word(s) look like noise (place names, adjectives, verbs)
                    // but not if they're genuine name parts (e.g., "Charles" in "Charles James Fox")
                    let prefix_words: Vec<String> =
                        words[..skip].iter().map(|w| w.to_lowercase()).collect();
                    let prefix_looks_noisy = prefix_words.iter().all(|pw| {
                        // Verb forms are always noise prefixes
                        let is_verb_form = (pw.ends_with("ed") && pw.len() > 5)
                            || (pw.ends_with("ing") && pw.len() > 5);
                        let is_demonym = pw.ends_with("man") && pw.len() > 5;
                        // If the prefix word exists as its own entity, it's likely
                        // a place/person name that got concatenated (NLP error)
                        let is_known_entity = name_lookup.contains_key(pw.as_str());
                        is_verb_form || is_demonym || (is_known_entity && e.entity_type == "person")
                    });
                    if prefix_looks_noisy {
                        eprintln!(
                            "  [prefix-noise] merging '{}' (id={}, deg={}) → '{}' (id={}, deg={})",
                            e.name, e.id, deg, remainder, target_id, target_deg
                        );
                        self.brain.merge_entities(e.id, target_id)?;
                        merged += 1;
                        break; // don't try more skips for this entity
                    }
                }
            }
        }
        Ok(merged)
    }

    /// Cross-type token bridge: find pairs of entities of different types that share
    /// significant name tokens and are in different components, suggesting a bridge.
    /// E.g., "Bayesian Learning" (concept) and "Thomas Bayes" (person) share "Bayes/Bayesian".
    /// Returns (entity_a_name, entity_b_name, shared_tokens, bridge_value).
    pub fn find_cross_type_token_bridges(&self) -> Result<Vec<(String, String, Vec<String>, f64)>> {
        let entities = self.brain.all_entities()?;
        let meaningful = meaningful_ids(self.brain)?;
        let relations = self.brain.all_relations()?;

        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Only consider connected entities with decent types
        let high_value: HashSet<&str> = [
            "person",
            "organization",
            "concept",
            "technology",
            "place",
            "company",
        ]
        .into_iter()
        .collect();

        // Build stem → entity mapping (simple: lowercase, strip trailing 's', 'ed', 'ing')
        fn stem(word: &str) -> String {
            let w = word.to_lowercase();
            if w.len() > 4 {
                if let Some(s) = w.strip_suffix("ing") {
                    return s.to_string();
                }
                if let Some(s) = w.strip_suffix("ed") {
                    return s.to_string();
                }
                if let Some(s) = w.strip_suffix("ian") {
                    return s.to_string();
                }
                if let Some(s) = w.strip_suffix("ean") {
                    return s.to_string();
                }
            }
            if w.len() > 3 {
                if let Some(s) = w.strip_suffix('s') {
                    return s.to_string();
                }
            }
            w
        }

        let stopwords: HashSet<&str> = [
            "the", "of", "and", "in", "on", "for", "to", "by", "from", "with",
        ]
        .into_iter()
        .collect();

        let mut stem_entities: HashMap<String, Vec<i64>> = HashMap::new();
        let entity_map: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        for e in &entities {
            if !meaningful.contains(&e.id)
                || !high_value.contains(e.entity_type.as_str())
                || !connected.contains(&e.id)
            {
                continue;
            }
            for word in e.name.split_whitespace() {
                if word.len() < 3 || stopwords.contains(&word.to_lowercase().as_str()) {
                    continue;
                }
                let s = stem(word);
                if s.len() >= 3 {
                    stem_entities.entry(s).or_default().push(e.id);
                }
            }
        }

        // Find cross-type pairs sharing stems
        let mut edges: HashSet<(i64, i64)> = HashSet::new();
        for r in &relations {
            let key = if r.subject_id < r.object_id {
                (r.subject_id, r.object_id)
            } else {
                (r.object_id, r.subject_id)
            };
            edges.insert(key);
        }

        let mut bridges: Vec<(String, String, Vec<String>, f64)> = Vec::new();
        let mut seen_pairs: HashSet<(i64, i64)> = HashSet::new();

        for (stem_word, eids) in &stem_entities {
            if eids.len() < 2 || eids.len() > 50 {
                continue; // Too unique or too common
            }
            for i in 0..eids.len().min(20) {
                for j in (i + 1)..eids.len().min(20) {
                    let a = eids[i];
                    let b = eids[j];
                    let key = if a < b { (a, b) } else { (b, a) };
                    if edges.contains(&key) || !seen_pairs.insert(key) {
                        continue;
                    }
                    let type_a = entity_map
                        .get(&a)
                        .map(|e| e.entity_type.as_str())
                        .unwrap_or("");
                    let type_b = entity_map
                        .get(&b)
                        .map(|e| e.entity_type.as_str())
                        .unwrap_or("");
                    if type_a == type_b {
                        continue; // Same type — not a cross-type bridge
                    }
                    let name_a = entity_map.get(&a).map(|e| e.name.as_str()).unwrap_or("");
                    let name_b = entity_map.get(&b).map(|e| e.name.as_str()).unwrap_or("");
                    bridges.push((
                        name_a.to_string(),
                        name_b.to_string(),
                        vec![stem_word.clone()],
                        1.0,
                    ));
                }
            }
        }

        bridges.truncate(50);
        Ok(bridges)
    }

    /// TF-IDF weighted island reconnection: compute token importance across the entire
    /// entity corpus, then find islands whose name tokens have high TF-IDF overlap with
    /// connected entities. Much smarter than simple token matching.
    pub fn tfidf_island_reconnect(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        let meaningful = meaningful_ids(self.brain)?;

        // Build document frequency: how many entities contain each token
        let stopwords: HashSet<&str> = [
            "the", "of", "and", "in", "on", "for", "to", "by", "from", "with", "at", "an", "or",
            "a", "is", "was", "are", "were", "be", "been", "has", "had", "have",
        ]
        .into_iter()
        .collect();

        let mut doc_freq: HashMap<String, usize> = HashMap::new();
        let total_docs = entities.len().max(1);

        // Tokenize and track
        struct TokenizedEntity {
            id: i64,
            name: String,
            entity_type: String,
            tokens: Vec<String>,
        }

        let mut tokenized: Vec<TokenizedEntity> = Vec::new();
        for e in &entities {
            if !meaningful.contains(&e.id) {
                continue;
            }
            let tokens: Vec<String> = e
                .name
                .split_whitespace()
                .map(|w| w.to_lowercase())
                .filter(|w| w.len() >= 3 && !stopwords.contains(w.as_str()))
                .collect();
            if tokens.is_empty() {
                continue;
            }
            let unique: HashSet<&String> = tokens.iter().collect();
            for t in &unique {
                *doc_freq.entry((*t).clone()).or_insert(0) += 1;
            }
            tokenized.push(TokenizedEntity {
                id: e.id,
                name: e.name.clone(),
                entity_type: e.entity_type.clone(),
                tokens,
            });
        }

        // Compute IDF
        let idf = |token: &str| -> f64 {
            let df = doc_freq.get(token).copied().unwrap_or(1) as f64;
            (total_docs as f64 / df).ln()
        };

        // Build TF-IDF vectors for connected entities (these are our "anchors")
        let mut connected_entities: Vec<&TokenizedEntity> = Vec::new();
        let mut island_entities: Vec<&TokenizedEntity> = Vec::new();

        for te in &tokenized {
            if connected.contains(&te.id) {
                connected_entities.push(te);
            } else {
                island_entities.push(te);
            }
        }

        // Build inverted index: token → connected entity indices
        let mut token_index: HashMap<&str, Vec<usize>> = HashMap::new();
        for (idx, ce) in connected_entities.iter().enumerate() {
            for t in &ce.tokens {
                token_index.entry(t.as_str()).or_default().push(idx);
            }
        }

        let mut reconnected = 0usize;
        let mut seen_pairs: HashSet<(i64, i64)> = HashSet::new();

        for island in &island_entities {
            if island.tokens.is_empty() {
                continue;
            }

            // Compute TF-IDF for island entity
            let island_tfidf: HashMap<&str, f64> =
                island.tokens.iter().map(|t| (t.as_str(), idf(t))).collect();
            let island_norm: f64 = island_tfidf.values().map(|v| v * v).sum::<f64>().sqrt();
            if island_norm < 0.01 {
                continue;
            }

            // Find candidate connected entities via inverted index
            let mut candidate_scores: HashMap<usize, f64> = HashMap::new();
            for t in &island.tokens {
                let t_idf = idf(t);
                if let Some(indices) = token_index.get(t.as_str()) {
                    for &idx in indices {
                        *candidate_scores.entry(idx).or_insert(0.0) += t_idf;
                    }
                }
            }

            // Find best match with cosine similarity
            let mut best: Option<(usize, f64)> = None;
            for (&idx, &_dot_partial) in &candidate_scores {
                let ce = connected_entities[idx];
                let ce_tfidf: HashMap<&str, f64> =
                    ce.tokens.iter().map(|t| (t.as_str(), idf(t))).collect();
                let ce_norm: f64 = ce_tfidf.values().map(|v| v * v).sum::<f64>().sqrt();
                if ce_norm < 0.01 {
                    continue;
                }
                // Full cosine
                let mut dot = 0.0_f64;
                for (t, &w) in &island_tfidf {
                    if let Some(&cw) = ce_tfidf.get(t) {
                        dot += w * cw;
                    }
                }
                let cosine = dot / (island_norm * ce_norm);
                if cosine > 0.4 {
                    if best.is_none() || cosine > best.unwrap().1 {
                        best = Some((idx, cosine));
                    }
                }
            }

            if let Some((idx, score)) = best {
                let target = connected_entities[idx];
                let key = if island.id < target.id {
                    (island.id, target.id)
                } else {
                    (target.id, island.id)
                };
                if seen_pairs.insert(key) && island.name != target.name {
                    // Determine predicate based on types
                    let pred = if island.entity_type == target.entity_type {
                        "associated_with"
                    } else if island.entity_type == "person" || target.entity_type == "person" {
                        "associated_with"
                    } else {
                        "related_concept"
                    };
                    self.brain
                        .upsert_relation(island.id, pred, target.id, "tfidf_reconnect")?;
                    eprintln!(
                        "  [tfidf-reconnect] {} → {} (score: {:.2}, pred: {})",
                        island.name, target.name, score, pred
                    );
                    reconnected += 1;
                    if reconnected >= 100 {
                        break;
                    }
                }
            }
        }

        Ok(reconnected)
    }

    /// Reconnect isolated entities by grouping them with connected entities that
    /// share the same source URL. If an isolated entity was extracted from the same
    /// page as a connected entity, they likely belong in the same topic cluster.
    pub fn reconnect_islands_by_source(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;

        // Build set of connected entity IDs
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build source_url → Vec<entity_id> for connected entities
        let mut source_to_connected: HashMap<String, Vec<i64>> = HashMap::new();
        for r in &relations {
            if let Some(ref url) = Some(&r.source_url) {
                if !url.is_empty() {
                    source_to_connected
                        .entry(url.to_string())
                        .or_default()
                        .push(r.subject_id);
                    source_to_connected
                        .entry(url.to_string())
                        .or_default()
                        .push(r.object_id);
                }
            }
        }
        // Dedup within each source
        for ids in source_to_connected.values_mut() {
            ids.sort_unstable();
            ids.dedup();
        }

        let entities = self.brain.all_entities()?;
        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        // For each isolated entity, check if any source URL in its relations connects
        // it to a connected entity. We look at the entity's name in relation source_urls.
        // Alternative: use the entity's first_seen timestamp to find co-temporal sources.
        //
        // Strategy: find isolated entities that appear in a relation's source_url context
        // by checking if the entity was created from the same source as connected entities.
        // Since isolated entities have NO relations, we need another signal.
        // Use temporal co-discovery: entities first_seen within 1 second of each other
        // were likely extracted from the same page.

        // Group entities by first_seen timestamp (truncated to nearest 2 seconds)
        let mut time_cohorts: HashMap<String, Vec<i64>> = HashMap::new();
        for e in &entities {
            if is_noise_type(&e.entity_type) || is_noise_name(&e.name) {
                continue;
            }
            // Truncate to nearest 10 seconds for fuzzy matching
            let ts_str = e.first_seen.format("%Y-%m-%dT%H:%M:%S").to_string();
            let secs = e.first_seen.second();
            let ts_key = format!("{}{}", &ts_str[..17], secs / 10 * 10);
            time_cohorts.entry(ts_key).or_default().push(e.id);
        }

        let mut reconnected = 0usize;
        for (_ts, cohort) in &time_cohorts {
            if cohort.len() < 2 || cohort.len() > 20 {
                continue; // Skip singletons and overly large batches
            }
            // Find connected entities in this cohort
            let connected_in_cohort: Vec<i64> = cohort
                .iter()
                .filter(|id| connected.contains(id))
                .copied()
                .collect();
            if connected_in_cohort.is_empty() {
                continue;
            }
            // Pick the highest-degree connected entity as the hub
            let hub_id = *connected_in_cohort
                .iter()
                .max_by_key(|&&id| {
                    relations
                        .iter()
                        .filter(|r| r.subject_id == id || r.object_id == id)
                        .count()
                })
                .unwrap();
            let hub_entity = match id_to_entity.get(&hub_id) {
                Some(e) => e,
                None => continue,
            };

            // Connect isolated entities in this cohort to the hub
            for &eid in cohort {
                if connected.contains(&eid) || eid == hub_id {
                    continue;
                }
                let island = match id_to_entity.get(&eid) {
                    Some(e) => e,
                    None => continue,
                };
                // Only connect same high-value types or complementary types
                let type_compatible = matches!(
                    (island.entity_type.as_str(), hub_entity.entity_type.as_str()),
                    ("person", "person")
                        | ("person", "concept")
                        | ("concept", "person")
                        | ("person", "organization")
                        | ("organization", "person")
                        | ("person", "place")
                        | ("place", "person")
                        | ("concept", "concept")
                        | ("organization", "concept")
                        | ("concept", "organization")
                        | ("place", "place")
                        | ("organization", "organization")
                );
                if !type_compatible {
                    continue;
                }
                // Determine predicate based on type pair
                let pred = match (island.entity_type.as_str(), hub_entity.entity_type.as_str()) {
                    ("person", "person") => "contemporary_of",
                    ("person", "place") | ("place", "person") => "active_in",
                    ("person", "organization") | ("organization", "person") => "affiliated_with",
                    ("person", "concept") => "contributed_to",
                    ("concept", "person") => "pioneered_by",
                    ("concept", "concept") => "related_concept",
                    ("organization", "concept") | ("concept", "organization") => "works_on",
                    ("organization", "organization") => "partner_of",
                    ("place", "place") => "located_near",
                    _ => "related_to",
                };
                let ok = self.brain.with_conn(|conn| {
                    conn.execute(
                        "INSERT OR IGNORE INTO relations (subject_id, predicate, object_id, confidence, source_url, learned_at)
                         VALUES (?1, ?2, ?3, 0.4, 'temporal_cohort', ?4)",
                        params![eid, pred, hub_id, Utc::now().to_rfc3339()],
                    )?;
                    Ok(())
                })?;
                let _ = ok;
                connected.insert(eid);
                reconnected += 1;
                if reconnected >= 500 {
                    return Ok(reconnected);
                }
            }
        }
        Ok(reconnected)
    }

    /// Knowledge coherence score: measure how well-integrated each topic cluster is.
    /// Returns clusters sorted by "improvement potential" — clusters that would benefit
    /// most from additional crawling or entity linking.
    pub fn knowledge_coherence_analysis(
        &self,
    ) -> Result<Vec<(String, usize, f64, f64, Vec<String>)>> {
        let communities = crate::graph::louvain_communities(self.brain)?;
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let meaningful = meaningful_ids(self.brain)?;

        let entity_map: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        // Group entities by community
        let mut comm_members: HashMap<usize, Vec<i64>> = HashMap::new();
        for (&eid, &comm) in &communities {
            if meaningful.contains(&eid) {
                comm_members.entry(comm).or_default().push(eid);
            }
        }

        // Build adjacency for degree counting
        let mut adj: HashMap<i64, HashSet<i64>> = HashMap::new();
        for r in &relations {
            adj.entry(r.subject_id).or_default().insert(r.object_id);
            adj.entry(r.object_id).or_default().insert(r.subject_id);
        }

        let mut results: Vec<(String, usize, f64, f64, Vec<String>)> = Vec::new();

        for (comm_id, members) in &comm_members {
            if members.len() < 3 {
                continue;
            }
            let member_set: HashSet<i64> = members.iter().copied().collect();

            // Internal density: edges within community / possible edges
            let mut internal_edges = 0usize;
            let mut external_edges = 0usize;
            let mut total_degree = 0usize;

            for &eid in members {
                if let Some(neighbors) = adj.get(&eid) {
                    for &n in neighbors {
                        if member_set.contains(&n) {
                            internal_edges += 1;
                        } else {
                            external_edges += 1;
                        }
                        total_degree += 1;
                    }
                }
            }
            internal_edges /= 2; // counted twice

            let n = members.len() as f64;
            let possible = n * (n - 1.0) / 2.0;
            let density = if possible > 0.0 {
                internal_edges as f64 / possible
            } else {
                0.0
            };

            // Coherence = density * (1 + external connectivity ratio)
            let _avg_degree = if members.len() > 0 {
                total_degree as f64 / members.len() as f64
            } else {
                0.0
            };
            let ext_ratio = if total_degree > 0 {
                external_edges as f64 / total_degree as f64
            } else {
                0.0
            };
            let coherence = density * (1.0 + ext_ratio);

            // Improvement potential: large clusters with low density
            let improvement_potential = n * (1.0 - density);

            // Find representative name (most connected entity)
            let rep_name = members
                .iter()
                .max_by_key(|&&eid| adj.get(&eid).map_or(0, |n| n.len()))
                .and_then(|&eid| entity_map.get(&eid).map(|e| e.name.clone()))
                .unwrap_or_else(|| format!("community_{}", comm_id));

            // Suggest topics: low-degree members that could use more connections
            let suggestions: Vec<String> = members
                .iter()
                .filter(|&&eid| adj.get(&eid).map_or(0, |n| n.len()) <= 2)
                .take(5)
                .filter_map(|&eid| entity_map.get(&eid).map(|e| e.name.clone()))
                .collect();

            results.push((
                rep_name,
                members.len(),
                coherence,
                improvement_potential,
                suggestions,
            ));
        }

        results.sort_by(|a, b| b.3.partial_cmp(&a.3).unwrap_or(std::cmp::Ordering::Equal));
        results.truncate(20);
        Ok(results)
    }

    /// Strategy effectiveness report: analyze which hypothesis generation strategies
    /// have the best confirmation rates and adjust weights accordingly.
    pub fn strategy_effectiveness_report(&self) -> Result<Vec<(String, usize, usize, f64, f64)>> {
        let hypotheses = self.list_hypotheses(None)?;

        let mut strategy_stats: HashMap<String, (usize, usize, usize)> = HashMap::new(); // total, confirmed, rejected

        for h in &hypotheses {
            let entry = strategy_stats
                .entry(h.pattern_source.clone())
                .or_insert((0, 0, 0));
            entry.0 += 1;
            match h.status {
                HypothesisStatus::Confirmed => entry.1 += 1,
                HypothesisStatus::Rejected => entry.2 += 1,
                _ => {}
            }
        }

        let mut report: Vec<(String, usize, usize, f64, f64)> = strategy_stats
            .iter()
            .map(|(strategy, &(total, confirmed, rejected))| {
                let resolved = confirmed + rejected;
                let confirmation_rate = if resolved > 0 {
                    confirmed as f64 / resolved as f64
                } else {
                    0.5 // no data
                };
                let current_weight = self.get_pattern_weight(strategy).unwrap_or(0.5);
                (
                    strategy.clone(),
                    total,
                    confirmed,
                    confirmation_rate,
                    current_weight,
                )
            })
            .collect();

        report.sort_by(|a, b| b.3.partial_cmp(&a.3).unwrap_or(std::cmp::Ordering::Equal));
        Ok(report)
    }

    /// Demote or remove uniform-predicate hubs: entities with degree ≥ 5 where
    /// >90% of relations use the same predicate. These are often NLP artifacts
    /// (e.g., entity connected to many others only via "contemporary_of").
    /// Removes relations from the dominant predicate if they're low confidence,
    /// keeping the entity but reducing its artificial inflation.
    /// Detect discovery plateau: returns (is_plateau, recent_rate, trend).
    /// A plateau means the last 5 runs show declining or flat confirmation rate.
    pub fn detect_plateau(&self) -> Result<(bool, f64, f64)> {
        let rows: Vec<(i64, i64)> = self.brain.with_conn(|conn| {
            let mut stmt = conn.prepare(
                "SELECT hypotheses_generated, confirmed FROM discovery_velocity ORDER BY id DESC LIMIT 10"
            )?;
            let rows = stmt.query_map([], |row| {
                Ok((row.get::<_, i64>(0)?, row.get::<_, i64>(1)?))
            })?.filter_map(|r| r.ok()).collect::<Vec<_>>();
            Ok(rows)
        })?;

        if rows.len() < 5 {
            return Ok((false, 0.0, 0.0));
        }

        let rates: Vec<f64> = rows
            .iter()
            .map(|(gen, conf)| {
                if *gen == 0 {
                    0.0
                } else {
                    *conf as f64 / *gen as f64
                }
            })
            .collect();

        let recent_avg = rates[..5].iter().sum::<f64>() / 5.0;
        let older_avg = if rates.len() >= 10 {
            rates[5..10].iter().sum::<f64>() / 5.0
        } else {
            rates[5..].iter().sum::<f64>() / rates[5..].len().max(1) as f64
        };

        let trend = recent_avg - older_avg;
        let is_plateau = trend <= 0.0 && recent_avg < 0.5;

        Ok((is_plateau, recent_avg, trend))
    }

    /// Use structural holes to find bridge entities and generate cross-domain hypotheses.
    pub fn generate_hypotheses_from_structural_holes(
        &self,
        limit: usize,
    ) -> Result<Vec<Hypothesis>> {
        let holes = crate::graph::structural_holes(self.brain, 5)?;
        let entities = self.brain.all_entities()?;
        let id_name: HashMap<i64, String> =
            entities.iter().map(|e| (e.id, e.name.clone())).collect();
        let relations = self.brain.all_relations()?;

        // For top structural hole entities, look at their neighborhoods and propose cross-cluster links
        let mut adj: HashMap<i64, Vec<i64>> = HashMap::new();
        for r in &relations {
            adj.entry(r.subject_id).or_default().push(r.object_id);
            adj.entry(r.object_id).or_default().push(r.subject_id);
        }

        let mut hypotheses = Vec::new();
        for (eid, constraint, _deg) in holes.iter().take(limit.min(20)) {
            let bridge_name = match id_name.get(eid) {
                Some(n) => n.clone(),
                None => continue,
            };
            if is_noise_name(&bridge_name) {
                continue;
            }

            // Get neighbors and check if any pair of them might be related
            let neighbors = match adj.get(eid) {
                Some(n) => n,
                None => continue,
            };
            if neighbors.len() < 3 {
                continue;
            }

            // Sample up to 20 neighbor pairs
            let sample: Vec<i64> = neighbors.iter().take(20).copied().collect();
            for i in 0..sample.len().min(10) {
                for j in (i + 1)..sample.len().min(10) {
                    let a = sample[i];
                    let b = sample[j];
                    // Skip if already connected
                    if adj.get(&a).map(|v| v.contains(&b)).unwrap_or(false) {
                        continue;
                    }

                    let name_a = match id_name.get(&a) {
                        Some(n) => n.as_str(),
                        None => continue,
                    };
                    let name_b = match id_name.get(&b) {
                        Some(n) => n.as_str(),
                        None => continue,
                    };
                    if is_noise_name(name_a) || is_noise_name(name_b) {
                        continue;
                    }

                    let conf = (0.6 - constraint * 0.5).max(0.25).min(0.75);
                    hypotheses.push(Hypothesis {
                        id: 0,
                        subject: name_a.to_string(),
                        predicate: "related_to".to_string(),
                        object: name_b.to_string(),
                        confidence: conf,
                        evidence_for: vec![format!("structural_hole_bridge={bridge_name},constraint={constraint:.3}")],
                        evidence_against: vec![],
                        reasoning_chain: vec![format!(
                            "Both {name_a} and {name_b} connect through structural hole bridge \
                             '{bridge_name}' (constraint={constraint:.3}), suggesting potential cross-domain link"
                        )],
                        status: HypothesisStatus::Testing,
                        discovered_at: Utc::now().format("%Y-%m-%d %H:%M:%S").to_string(),
                        pattern_source: "structural_hole".to_string(),
                    });
                    if hypotheses.len() >= limit {
                        break;
                    }
                }
                if hypotheses.len() >= limit {
                    break;
                }
            }
            if hypotheses.len() >= limit {
                break;
            }
        }
        Ok(hypotheses)
    }

    pub fn demote_uniform_hubs(&self, min_degree: usize, max_dominant_frac: f64) -> Result<usize> {
        let entropy_data = crate::graph::per_entity_predicate_entropy(self.brain, min_degree)?;
        let entities = self.brain.all_entities()?;
        let id_name: HashMap<i64, &str> =
            entities.iter().map(|e| (e.id, e.name.as_str())).collect();

        let mut demoted = 0usize;
        for (eid, _entropy, dominant_pred, dominant_frac) in &entropy_data {
            if *dominant_frac < max_dominant_frac {
                continue; // healthy diversity
            }
            // Skip well-known high-value entity types that legitimately have uniform predicates
            let name = id_name.get(eid).copied().unwrap_or("");
            if name.is_empty() || is_noise_name(name) {
                continue;
            }

            // Remove low-confidence relations using the dominant predicate
            let removed: usize = self.brain.with_conn(|conn| {
                let count: usize = conn.query_row(
                    "SELECT COUNT(*) FROM relations WHERE (subject_id = ?1 OR object_id = ?1) AND predicate = ?2 AND confidence < 0.6",
                    params![eid, dominant_pred],
                    |row| row.get(0),
                )?;
                if count > 0 {
                    conn.execute(
                        "DELETE FROM relations WHERE (subject_id = ?1 OR object_id = ?1) AND predicate = ?2 AND confidence < 0.6",
                        params![eid, dominant_pred],
                    )?;
                }
                Ok(count)
            })?;
            demoted += removed;
        }
        Ok(demoted)
    }

    /// Purge isolated entities whose names are clearly non-English extraction noise.
    /// Detects Latin suffixes, non-Latin scripts, and foreign-language fragments
    /// that NLP extractors pull from multilingual Wikipedia articles.
    /// Purge entities that are NLP concatenation artifacts: a descriptive/historical
    /// prefix word + a proper entity name. Examples: "Punic Carthage", "Yuan China",
    /// "Efficiency Carnot", "Illyria Alexander". If the suffix matches a known entity,
    /// merge relations into that entity; otherwise just purge the island.
    pub fn purge_topic_prefix_entities(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build name→id lookup for merging
        let name_to_id: HashMap<String, i64> = entities
            .iter()
            .map(|e| (e.name.to_lowercase(), e.id))
            .collect();

        // Descriptive/historical prefix words that signal NLP concatenation artifacts.
        // These are adjectives, demonyms, or topic words that get concatenated with
        // entity names during extraction from Wikipedia articles.
        let topic_prefixes: HashSet<&str> = [
            "punic",
            "yuan",
            "ming",
            "tang",
            "han",
            "qing",
            "song",
            "sui",
            "byzantine",
            "ottoman",
            "roman",
            "greek",
            "persian",
            "mughal",
            "illyria",
            "lancel",
            "efficiency",
            "medieval",
            "ancient",
            "modern",
            "imperial",
            "royal",
            "colonial",
            "federal",
            "national",
            "central",
            "devastated",
            "celebrated",
            "famous",
            "notable",
            "prominent",
            "legendary",
            "renowned",
            "classical",
            "neolithic",
            "paleolithic",
            "mesolithic",
            "baroque",
            "gothic",
            "renaissance",
            "enlightenment",
            "crusader",
            "viking",
            "celtic",
            "gallic",
            "frankish",
            "visigothic",
            "moorish",
            "mamluk",
            "abbasid",
            "umayyad",
            "safavid",
            "qajar",
            "joseon",
            "tokugawa",
            "meiji",
            "edo",
            "heian",
            "kamakura",
        ]
        .iter()
        .copied()
        .collect();

        let mut purged = 0usize;
        for e in &entities {
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 || words.len() > 4 {
                continue;
            }
            let first_lower = words[0].to_lowercase();
            if !topic_prefixes.contains(first_lower.as_str()) {
                continue;
            }
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            // Only purge low-degree entities (likely noise, not well-established)
            if deg > 10 {
                continue;
            }
            // Check if the suffix (remaining words) matches a known entity
            let suffix = words[1..].join(" ");
            let suffix_lower = suffix.to_lowercase();
            if let Some(&target_id) = name_to_id.get(&suffix_lower) {
                if target_id != e.id {
                    // Merge: reassign relations from this entity to the target
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "UPDATE relations SET subject_id = ?1 WHERE subject_id = ?2",
                            params![target_id, e.id],
                        )?;
                        conn.execute(
                            "UPDATE relations SET object_id = ?1 WHERE object_id = ?2",
                            params![target_id, e.id],
                        )?;
                        conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                        Ok(())
                    })?;
                    purged += 1;
                    continue;
                }
            }
            // No matching target — purge if island or very low degree
            if deg <= 2 {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "DELETE FROM relations WHERE subject_id = ?1 OR object_id = ?1",
                        params![e.id],
                    )?;
                    conn.execute("DELETE FROM facts WHERE entity_id = ?1", params![e.id])?;
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    Ok(())
                })?;
                purged += 1;
            }
        }
        Ok(purged)
    }

    pub fn purge_foreign_language_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        let mut purged = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            let name = e.name.trim();
            let lower = name.to_lowercase();
            let words: Vec<&str> = name.split_whitespace().collect();

            // Skip entities with facts
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }

            // Skip high-confidence entities (but allow non-Latin script purge at any confidence)
            let total_alpha = lower.chars().filter(|c| c.is_alphabetic()).count();
            let non_ascii_alpha = lower
                .chars()
                .filter(|c| c.is_alphabetic() && !c.is_ascii())
                .count();

            let mut is_foreign = false;

            // Non-Latin script check (Cyrillic, Arabic, CJK, Greek, etc.) — any confidence
            if total_alpha > 2 && non_ascii_alpha as f64 / total_alpha as f64 > 0.4 {
                is_foreign = true;
            }

            // Skip remaining checks for high-confidence entities
            if !is_foreign && e.confidence > 0.8 {
                continue;
            }

            // Single-word Latin/foreign endings
            if !is_foreign && words.len() == 1 && lower.len() >= 5 {
                let latin_endings: &[&str] = &["orum", "arum", "ibus", "ius", "iae", "ensis"];
                for suffix in latin_endings {
                    if lower.ends_with(suffix) && !is_common_english_word(&lower) {
                        is_foreign = true;
                        break;
                    }
                }
                // Known German/French single words (confidence <= 0.8)
                if !is_foreign && e.confidence <= 0.8 {
                    let foreign_single_words: &[&str] = &[
                        "verschränkung",
                        "geburtstag",
                        "beitrag",
                        "reisenden",
                        "sonnenallee",
                        "bilanz",
                        "streifzüge",
                        "belgique",
                        "bibliothèque",
                        "musique",
                        "tabulae",
                        "mechanica",
                        "politiques",
                        "économie",
                        "mathématiques",
                        "philosophie",
                        "république",
                        "révolution",
                        "civilisation",
                        "wissenschaft",
                        "gesellschaft",
                        "geschichte",
                        "forschung",
                        "entwicklung",
                        "grundlagen",
                        "zeitschrift",
                        "abhandlungen",
                        "mitteilungen",
                        "verhandlungen",
                        "archäologie",
                        "korrespondenz",
                        "naturwissenschaften",
                        "sitzungsberichte",
                    ];
                    if foreign_single_words.contains(&lower.as_str()) {
                        is_foreign = true;
                    }
                    // German compound suffixes at any confidence
                    let german_suffixes: &[&str] = &[
                        "straße", "platz", "allee", "brücke", "kirche", "schule", "burg", "stadt",
                        "dorf",
                    ];
                    for suffix in german_suffixes {
                        if lower.ends_with(suffix) && lower.len() > suffix.len() + 2 {
                            is_foreign = true;
                            break;
                        }
                    }
                }
            }

            // Multi-word foreign language fragments (confidence <= 0.7)
            if !is_foreign && words.len() >= 2 && e.confidence <= 0.7 {
                let foreign_suffixes: &[&str] = &[
                    "ción", "ção", "heit", "keit", "schaft", "ière", "ismus", "ität", "ość",
                    "stvo", "ung",
                ];
                for suffix in foreign_suffixes {
                    if lower.ends_with(suffix) {
                        is_foreign = true;
                        break;
                    }
                }
                // German/French/Spanish articles as first word
                let foreign_articles = ["der", "die", "das", "les", "des", "une", "del", "los"];
                if !is_foreign && words.len() >= 2 {
                    if let Some(first) = words.first() {
                        if foreign_articles.contains(&first.to_lowercase().as_str())
                            && e.confidence <= 0.6
                        {
                            is_foreign = true;
                        }
                    }
                }
            }

            if is_foreign {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    Ok(())
                })?;
                purged += 1;
            }
        }
        if purged > 0 {
            eprintln!(
                "  [foreign-language-purge] removed {} non-English island entities",
                purged
            );
        }
        Ok(purged)
    }

    /// Purge isolated person entities whose names end with pronouns, adverbs,
    /// determiners, or other trailing NLP fragment words. These are extraction
    /// errors like "Joseph Crespino It", "Match Twice", "Discussion It".
    pub fn purge_trailing_fragment_person_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        let trailing_noise: HashSet<&str> = [
            "it",
            "him",
            "her",
            "them",
            "us",
            "me",
            "its",
            "his",
            "this",
            "that",
            "these",
            "those",
            "which",
            "what",
            "who",
            "twice",
            "once",
            "often",
            "never",
            "always",
            "sometimes",
            "here",
            "there",
            "where",
            "when",
            "then",
            "now",
            "not",
            "nor",
            "yet",
            "but",
            "and",
            "for",
            "the",
            "any",
            "very",
            "much",
            "more",
            "most",
            "less",
            "also",
            "too",
            "been",
            "being",
            "having",
            "doing",
            "going",
            "etc",
            "eg",
            "ie",
            "so",
            "just",
            "only",
            "even",
            "still",
            "each",
            "every",
            "some",
            "all",
            "both",
            "few",
            "many",
            "own",
            "such",
            "no",
            "rather",
            "quite",
            "merely",
        ]
        .iter()
        .copied()
        .collect();

        let mut purged = 0usize;
        for e in &entities {
            if connected.contains(&e.id) || e.entity_type != "person" {
                continue;
            }
            if e.confidence > 0.7 {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }
            let last = words.last().unwrap().to_lowercase();
            if !trailing_noise.contains(last.as_str()) {
                continue;
            }
            self.brain.with_conn(|conn| {
                conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                Ok(())
            })?;
            purged += 1;
        }
        if purged > 0 {
            eprintln!(
                "  [trailing-fragment-purge] removed {} person islands with trailing noise words",
                purged
            );
        }
        Ok(purged)
    }

    /// Purge isolated person entities that look like Latin/foreign phrases rather
    /// than actual person names. Detects patterns like multi-word names where
    /// most words match common Latin vocabulary.
    pub fn purge_latin_phrase_person_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        let latin_words: HashSet<&str> = [
            "ab",
            "ad",
            "ante",
            "ars",
            "bellum",
            "bona",
            "causa",
            "cum",
            "de",
            "dei",
            "deo",
            "deus",
            "domus",
            "duo",
            "ego",
            "ergo",
            "est",
            "et",
            "ex",
            "fide",
            "gratia",
            "homo",
            "idem",
            "ille",
            "in",
            "inter",
            "iure",
            "ius",
            "lege",
            "lex",
            "loco",
            "lux",
            "mala",
            "mens",
            "modus",
            "mors",
            "naevo",
            "nihil",
            "nisi",
            "non",
            "nos",
            "nulla",
            "omni",
            "opus",
            "pax",
            "per",
            "post",
            "prima",
            "pro",
            "quod",
            "rea",
            "res",
            "rex",
            "se",
            "sed",
            "sic",
            "sine",
            "sol",
            "sub",
            "sui",
            "suo",
            "terra",
            "tot",
            "tua",
            "una",
            "unum",
            "via",
            "vir",
            "vita",
            "vox",
            "zelo",
            "vindicatus",
            "operandi",
            "vivendi",
            "juris",
            "facto",
            "jure",
            "ipso",
            "corpus",
            "habeas",
            "magna",
            "carta",
            "cogito",
            "sum",
            "veritas",
            "amor",
            "finis",
            "orbis",
            "mundus",
            "sanctus",
            "sancta",
            "mare",
            "mons",
            "caput",
            "urbs",
        ]
        .iter()
        .copied()
        .collect();

        let mut purged = 0usize;
        for e in &entities {
            if connected.contains(&e.id) || e.entity_type != "person" {
                continue;
            }
            if e.confidence > 0.6 {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 || words.len() > 5 {
                continue;
            }
            let latin_count = words
                .iter()
                .filter(|w| latin_words.contains(w.to_lowercase().as_str()))
                .count();
            // All words must be Latin for it to be a phrase
            if latin_count < words.len() {
                continue;
            }
            self.brain.with_conn(|conn| {
                conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                Ok(())
            })?;
            purged += 1;
        }
        if purged > 0 {
            eprintln!(
                "  [latin-phrase-purge] removed {} person islands that are Latin phrases",
                purged
            );
        }
        Ok(purged)
    }

    /// Reconnect isolated place entities to the graph by matching geographic
    /// name tokens with connected place/organization entities.
    pub fn reconnect_places_by_geographic_context(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        let geographic_suffixes: HashSet<&str> = [
            "river",
            "lake",
            "mountain",
            "sea",
            "ocean",
            "bay",
            "gulf",
            "strait",
            "canal",
            "valley",
            "desert",
            "island",
            "islands",
            "peninsula",
            "cape",
            "plateau",
            "basin",
            "delta",
            "gorge",
            "empire",
            "kingdom",
            "republic",
            "province",
            "prefecture",
            "county",
            "district",
            "territory",
            "region",
            "city",
            "town",
        ]
        .iter()
        .copied()
        .collect();

        // Build token → connected place entity index
        let mut token_to_connected: HashMap<String, Vec<i64>> = HashMap::new();
        for e in &entities {
            if !connected.contains(&e.id) {
                continue;
            }
            if e.entity_type != "place" && e.entity_type != "organization" {
                continue;
            }
            for word in e.name.split_whitespace() {
                let lower = word.to_lowercase();
                if lower.len() < 4 || geographic_suffixes.contains(lower.as_str()) {
                    continue;
                }
                token_to_connected.entry(lower).or_default().push(e.id);
            }
        }

        let mut reconnected = 0usize;
        let mut seen_pairs: HashSet<(i64, i64)> = HashSet::new();
        for e in &entities {
            if connected.contains(&e.id) || e.entity_type != "place" {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }
            let mut best_target: Option<i64> = None;
            let mut best_score = 0usize;
            for word in &words {
                let lower = word.to_lowercase();
                if lower.len() < 4 || geographic_suffixes.contains(lower.as_str()) {
                    continue;
                }
                if let Some(targets) = token_to_connected.get(&lower) {
                    for &tid in targets {
                        if tid == e.id || seen_pairs.contains(&(e.id, tid)) {
                            continue;
                        }
                        if let Some(te) = entities.iter().find(|x| x.id == tid) {
                            let shared = words
                                .iter()
                                .filter(|w| {
                                    let l = w.to_lowercase();
                                    l.len() >= 4
                                        && !geographic_suffixes.contains(l.as_str())
                                        && te.name.to_lowercase().contains(&l)
                                })
                                .count();
                            if shared > best_score {
                                best_score = shared;
                                best_target = Some(tid);
                            }
                        }
                    }
                }
            }
            if let Some(tid) = best_target {
                if best_score >= 1 {
                    seen_pairs.insert((e.id, tid));
                    self.brain
                        .upsert_relation(e.id, "geographically_related_to", tid, "")?;
                    reconnected += 1;
                    if reconnected >= 100 {
                        break;
                    }
                }
            }
        }
        if reconnected > 0 {
            eprintln!(
                "  [geographic-context-reconnect] connected {} isolated places via shared geographic tokens",
                reconnected
            );
        }
        Ok(reconnected)
    }

    /// Reconnect high-value isolated entities to the knowledge graph using
    /// domain keyword matching. Maps persons/concepts to relevant connected
    /// hub entities based on shared meaningful name tokens.
    pub fn reconnect_islands_by_domain(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }
        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        // Build degree map
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Build token → connected entity ids (only well-connected hubs, tokens >= 4 chars)
        let stopwords: HashSet<&str> = [
            "the", "and", "for", "from", "with", "that", "this", "have", "been", "were", "they",
            "their", "which", "about", "into", "over", "also",
        ]
        .iter()
        .copied()
        .collect();
        let mut token_to_connected: HashMap<String, Vec<(i64, usize)>> = HashMap::new();
        for e in &entities {
            if !connected.contains(&e.id) {
                continue;
            }
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            if deg < 3 {
                continue;
            }
            for word in e.name.to_lowercase().split_whitespace() {
                if word.len() < 4 || stopwords.contains(word) {
                    continue;
                }
                token_to_connected
                    .entry(word.to_string())
                    .or_default()
                    .push((e.id, deg));
            }
        }

        let mut reconnected = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            if e.confidence < 0.7 {
                continue;
            }
            if !HIGH_VALUE_TYPES.contains(&e.entity_type.as_str()) {
                continue;
            }
            if is_noise_name(&e.name) || e.name.len() < 4 {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue; // Only multi-word entities (single-word handled elsewhere)
            }

            let tokens: Vec<String> = e
                .name
                .to_lowercase()
                .split_whitespace()
                .filter(|w| w.len() >= 4 && !stopwords.contains(w))
                .map(|s| s.to_string())
                .collect();
            if tokens.is_empty() {
                continue;
            }

            // Score candidates by shared tokens and degree
            let mut candidate_scores: HashMap<i64, (usize, usize)> = HashMap::new();
            for tok in &tokens {
                if let Some(hits) = token_to_connected.get(tok) {
                    for &(cid, deg) in hits {
                        if cid == e.id {
                            continue;
                        }
                        let entry = candidate_scores.entry(cid).or_insert((0, deg));
                        entry.0 += 1;
                    }
                }
            }

            // Pick the best candidate: prefer type-compatible, high degree
            let best = candidate_scores
                .iter()
                .filter(|(cid, (shared, _))| {
                    if *shared < 1 {
                        return false;
                    }
                    if let Some(ce) = id_to_entity.get(cid) {
                        let sl = e.name.to_lowercase();
                        let cl = ce.name.to_lowercase();
                        if sl.contains(&cl) || cl.contains(&sl) {
                            return false;
                        }
                        let sf = e.name.split_whitespace().next().unwrap_or("");
                        let cf = ce.name.split_whitespace().next().unwrap_or("");
                        if !sf.is_empty() && sf == cf {
                            return false;
                        }
                        true
                    } else {
                        false
                    }
                })
                .max_by_key(|(cid, (shared, deg))| {
                    let type_bonus = if let Some(ce) = id_to_entity.get(cid) {
                        if types_compatible(&e.entity_type, &ce.entity_type) {
                            10
                        } else {
                            0
                        }
                    } else {
                        0
                    };
                    (*shared * 100 + type_bonus, *deg)
                });

            if let Some((&cid, _)) = best {
                if let Some(ce) = id_to_entity.get(&cid) {
                    let pred = infer_predicate(&e.entity_type, &ce.entity_type, None);
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "INSERT OR IGNORE INTO relations (subject_id, predicate, object_id, confidence, source_url, learned_at)
                             VALUES (?1, ?2, ?3, 0.45, 'domain_keyword_reconnect', ?4)",
                            params![e.id, pred, cid, Utc::now().to_rfc3339()],
                        )?;
                        Ok(())
                    })?;
                    connected.insert(e.id);
                    reconnected += 1;
                    if reconnected >= 300 {
                        break;
                    }
                }
            }
        }
        if reconnected > 0 {
            eprintln!(
                "  [domain-keyword-reconnect] connected {} island entities to graph",
                reconnected
            );
        }
        Ok(reconnected)
    }

    /// Connect isolated entities to the graph by matching entity names against
    /// fact values of other entities. E.g., if entity "Quantum Computing" is isolated
    /// and entity "Richard Feynman" has a fact {key: "field", value: "quantum computing"},
    /// create a relation "Richard Feynman" → associated_with → "Quantum Computing".
    pub fn reconnect_via_fact_values(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }
        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        // Build a map: lowercase entity name → entity id (for isolated entities only)
        let mut island_name_to_id: HashMap<String, i64> = HashMap::new();
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            if is_noise_name(&e.name) || is_noise_type(&e.entity_type) {
                continue;
            }
            if e.name.len() < 4 || e.confidence < 0.5 {
                continue;
            }
            // Only multi-word or high-value types
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 && !HIGH_VALUE_TYPES.contains(&e.entity_type.as_str()) {
                continue;
            }
            island_name_to_id.insert(e.name.to_lowercase(), e.id);
        }

        if island_name_to_id.is_empty() {
            return Ok(0);
        }

        let mut reconnected = 0usize;
        let now = Utc::now().to_rfc3339();

        // Build keyword sets per connected entity from their facts
        // Then match: if an isolated entity's name tokens overlap significantly
        // with a connected entity's keyword set, bridge them.
        let stopwords: HashSet<&str> = [
            "the",
            "and",
            "for",
            "from",
            "with",
            "that",
            "this",
            "have",
            "been",
            "were",
            "they",
            "their",
            "which",
            "about",
            "into",
            "over",
            "also",
            "keyword",
            "retrieved",
            "text",
            "source",
        ]
        .iter()
        .copied()
        .collect();

        // For each connected entity, collect its fact-value keywords
        let mut entity_keywords: HashMap<i64, HashSet<String>> = HashMap::new();
        for e in &entities {
            if !connected.contains(&e.id) {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if facts.is_empty() {
                continue;
            }
            let kws: HashSet<String> = facts
                .iter()
                .map(|f| f.value.to_lowercase())
                .filter(|v| v.len() >= 4 && !stopwords.contains(v.as_str()))
                .collect();
            if !kws.is_empty() {
                entity_keywords.insert(e.id, kws);
            }
        }

        // For each isolated entity, tokenize its name and find connected entities
        // sharing ≥2 keyword tokens (or ≥1 for rare/distinctive tokens)
        let mut already_bridged: HashSet<i64> = HashSet::new();
        for (iname, &iid) in &island_name_to_id {
            if already_bridged.contains(&iid) {
                continue;
            }
            let tokens: HashSet<String> = iname
                .split_whitespace()
                .map(|w| w.to_lowercase())
                .filter(|w| w.len() >= 4 && !stopwords.contains(w.as_str()))
                .collect();
            if tokens.is_empty() {
                continue;
            }

            // Score candidates by keyword overlap
            let mut best_cid: Option<i64> = None;
            let mut best_overlap = 0usize;
            for (&cid, kws) in &entity_keywords {
                if cid == iid {
                    continue;
                }
                let overlap = tokens.intersection(kws).count();
                if overlap >= 2 && overlap > best_overlap {
                    // Avoid self-reference or substring matches
                    if let Some(ce) = id_to_entity.get(&cid) {
                        let sl = iname.to_lowercase();
                        let cl = ce.name.to_lowercase();
                        if sl.contains(&cl) || cl.contains(&sl) {
                            continue;
                        }
                    }
                    best_overlap = overlap;
                    best_cid = Some(cid);
                }
            }

            if let Some(cid) = best_cid {
                if let (Some(island_ent), Some(ce)) =
                    (id_to_entity.get(&iid), id_to_entity.get(&cid))
                {
                    let pred = infer_predicate(&ce.entity_type, &island_ent.entity_type, None);
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "INSERT OR IGNORE INTO relations (subject_id, predicate, object_id, confidence, source_url, learned_at)
                             VALUES (?1, ?2, ?3, 0.45, 'fact_value_bridge', ?4)",
                            params![cid, pred, iid, now],
                        )?;
                        Ok(())
                    })?;
                    already_bridged.insert(iid);
                    reconnected += 1;
                    if reconnected >= 200 {
                        break;
                    }
                }
            }
        }
        if reconnected > 0 {
            eprintln!(
                "  [fact-value-bridge] connected {} island entities via fact values",
                reconnected
            );
        }
        Ok(reconnected)
    }

    /// Batch-upgrade confidence of isolated entities that are well-known names.
    pub fn boost_known_entity_confidence(&self) -> Result<usize> {
        let famous_names: HashSet<&str> = [
            "albert einstein",
            "isaac newton",
            "charles darwin",
            "nikola tesla",
            "marie curie",
            "galileo galilei",
            "leonardo da vinci",
            "aristotle",
            "plato",
            "socrates",
            "archimedes",
            "pythagoras",
            "euclid",
            "michael faraday",
            "james maxwell",
            "niels bohr",
            "max planck",
            "erwin schrödinger",
            "werner heisenberg",
            "paul dirac",
            "richard feynman",
            "enrico fermi",
            "lise meitner",
            "emmy noether",
            "bernhard riemann",
            "georg cantor",
            "david hilbert",
            "alan turing",
            "john von neumann",
            "kurt gödel",
            "ada lovelace",
            "grace hopper",
            "joseph stalin",
            "napoleon bonaparte",
            "julius caesar",
            "alexander the great",
            "cleopatra",
            "genghis khan",
            "william shakespeare",
            "jane austen",
            "leo tolstoy",
            "ludwig van beethoven",
            "wolfgang amadeus mozart",
            "johann sebastian bach",
            "ernest rutherford",
            "niels henrik abel",
            "évariste galois",
        ]
        .iter()
        .copied()
        .collect();

        let entities = self.brain.all_entities()?;
        let mut boosted = 0usize;
        for e in &entities {
            if e.confidence >= 0.9 {
                continue;
            }
            let lower = e.name.to_lowercase();
            if famous_names.contains(lower.as_str()) {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE entities SET confidence = MAX(confidence, 0.95) WHERE id = ?1",
                        params![e.id],
                    )?;
                    Ok(())
                })?;
                boosted += 1;
            }
        }
        if boosted > 0 {
            eprintln!(
                "  [known-entity-boost] upgraded confidence of {} well-known entities",
                boosted
            );
        }
        Ok(boosted)
    }

    /// Reconnect isolated entities by matching their name tokens against
    /// predicate-object pairs in the connected graph.
    ///
    /// Example: isolated "Quantum Entanglement" can connect to "Albert Einstein"
    /// if Einstein has a relation "pioneered → Quantum Mechanics" and the tokens
    /// overlap sufficiently.
    pub fn reconnect_islands_by_predicate_object_tokens(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build token index: token → Vec<(entity_id, predicate)> from connected entities' relation objects
        let stopwords: HashSet<&str> = [
            "the", "of", "and", "in", "to", "a", "is", "for", "on", "with", "at", "by", "from",
            "or", "an", "be", "as", "was", "are", "it", "its", "has", "had", "not", "but",
        ]
        .into_iter()
        .collect();

        let mut token_to_subjects: HashMap<String, Vec<(i64, String)>> = HashMap::new();
        for r in &relations {
            if r.confidence < 0.5 || is_generic_predicate(&r.predicate) {
                continue;
            }
            if let Some(obj_ent) = id_to_entity.get(&r.object_id) {
                if is_noise_type(&obj_ent.entity_type) || is_noise_name(&obj_ent.name) {
                    continue;
                }
                for word in obj_ent.name.to_lowercase().split_whitespace() {
                    let w = word.trim_matches(|c: char| !c.is_alphanumeric());
                    if w.len() >= 4 && !stopwords.contains(w) {
                        token_to_subjects
                            .entry(w.to_string())
                            .or_default()
                            .push((r.subject_id, r.predicate.clone()));
                    }
                }
            }
        }

        // For each isolated entity, find connected subjects whose relation objects share tokens
        let mut reconnected = 0usize;
        let max_reconnect = 200;
        for e in &entities {
            if reconnected >= max_reconnect {
                break;
            }
            if connected.contains(&e.id) || is_noise_type(&e.entity_type) || is_noise_name(&e.name)
            {
                continue;
            }
            if e.name.split_whitespace().count() < 2 {
                continue; // Skip single-word entities (too ambiguous)
            }

            // Collect matching subjects by counting shared tokens
            let mut subject_scores: HashMap<i64, (usize, String)> = HashMap::new();
            let tokens: Vec<String> = e
                .name
                .to_lowercase()
                .split_whitespace()
                .map(|w| w.trim_matches(|c: char| !c.is_alphanumeric()).to_string())
                .filter(|w| w.len() >= 4 && !stopwords.contains(w.as_str()))
                .collect();

            if tokens.is_empty() {
                continue;
            }

            for tok in &tokens {
                if let Some(subjects) = token_to_subjects.get(tok) {
                    for (sid, pred) in subjects {
                        if let Some(subj_ent) = id_to_entity.get(sid) {
                            // Don't connect to same type if types are too different
                            if subj_ent.entity_type == e.entity_type {
                                continue; // Same-type token matching is handled by other strategies
                            }
                        }
                        let entry = subject_scores.entry(*sid).or_insert((0, pred.clone()));
                        entry.0 += 1;
                    }
                }
            }

            // Require at least 2 shared tokens or 50%+ token overlap
            let min_overlap = if tokens.len() <= 2 {
                2
            } else {
                (tokens.len() + 1) / 2
            };
            let best = subject_scores
                .iter()
                .filter(|(_, (score, _))| *score >= min_overlap)
                .max_by_key(|(_, (score, _))| *score);

            if let Some((&subject_id, (_, _pred))) = best {
                let predicate = if e.entity_type == "concept"
                    && id_to_entity
                        .get(&subject_id)
                        .map(|s| s.entity_type.as_str())
                        == Some("person")
                {
                    "pioneered".to_string()
                } else {
                    format!("related_to")
                };
                let _ = self.brain.upsert_relation(
                    subject_id,
                    &predicate,
                    e.id,
                    "prometheus:predicate_object_token_reconnect",
                );
                reconnected += 1;
            }
        }

        if reconnected > 0 {
            eprintln!(
                "  [predicate-object-token-reconnect] connected {} island entities to graph",
                reconnected
            );
        }
        Ok(reconnected)
    }

    /// Connect isolated high-confidence person entities to the graph by finding
    /// connected persons who share significant name tokens (surname matching).
    /// E.g., isolated "Lise Meitner" can connect to "Otto Hahn" if both appear
    /// in the same source URL, or to "Nuclear Fission" via domain keywords.
    pub fn reconnect_high_value_islands_by_surname(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        // Build: source_url → set of entity ids mentioned in relations from that source
        let mut source_entities: HashMap<String, Vec<i64>> = HashMap::new();
        for r in &relations {
            if !r.source_url.is_empty() && r.source_url.starts_with("http") {
                source_entities
                    .entry(r.source_url.clone())
                    .or_default()
                    .push(r.subject_id);
                source_entities
                    .entry(r.source_url.clone())
                    .or_default()
                    .push(r.object_id);
            }
        }
        // Dedup within each source
        for ids in source_entities.values_mut() {
            ids.sort_unstable();
            ids.dedup();
        }

        // Build connected entity name-token index: token → Vec<entity_id>
        let mut token_to_connected: HashMap<String, Vec<i64>> = HashMap::new();
        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();
        for e in &entities {
            if !connected.contains(&e.id) || is_noise_type(&e.entity_type) {
                continue;
            }
            for token in e.name.split_whitespace() {
                let t = token.to_lowercase();
                if t.len() >= 4 {
                    token_to_connected.entry(t).or_default().push(e.id);
                }
            }
        }

        let mut reconnected = 0usize;
        let mut seen_pairs: HashSet<(i64, i64)> = HashSet::new();

        // Strategy 1: Same-source reconnection for isolated multi-word person/concept entities
        for e in &entities {
            if connected.contains(&e.id)
                || is_noise_type(&e.entity_type)
                || is_noise_name(&e.name)
                || e.confidence < 0.7
                || e.name.split_whitespace().count() < 2
            {
                continue;
            }
            if reconnected >= 200 {
                break;
            }

            // Check if any source mentions a connected entity with a shared surname token
            let island_tokens: Vec<String> = e
                .name
                .split_whitespace()
                .map(|t| t.to_lowercase())
                .filter(|t| t.len() >= 4)
                .collect();

            // Find connected entities sharing a significant token (usually surname)
            let mut candidates: Vec<(i64, usize)> = Vec::new();
            for token in &island_tokens {
                if let Some(cids) = token_to_connected.get(token) {
                    for &cid in cids {
                        if cid == e.id || seen_pairs.contains(&(e.id.min(cid), e.id.max(cid))) {
                            continue;
                        }
                        if let Some(ce) = id_to_entity.get(&cid) {
                            // Must share same entity type or be compatible
                            if ce.entity_type == e.entity_type
                                || (e.entity_type == "person" && ce.entity_type == "person")
                                || (e.entity_type == "concept" && ce.entity_type == "concept")
                            {
                                candidates.push((cid, 1));
                            }
                        }
                    }
                }
            }

            // Pick the best candidate (most shared tokens)
            if !candidates.is_empty() {
                // Count shared tokens per candidate
                let mut token_counts: HashMap<i64, usize> = HashMap::new();
                for (cid, _) in &candidates {
                    *token_counts.entry(*cid).or_insert(0) += 1;
                }
                // Require at least 2 shared tokens for persons (surname + first name or similar)
                let min_shared = if e.entity_type == "person" { 2 } else { 1 };
                if let Some((&best_id, &count)) = token_counts.iter().max_by_key(|(_, &c)| c) {
                    if count >= min_shared {
                        // This is likely a duplicate — merge
                        let pair = (e.id.min(best_id), e.id.max(best_id));
                        if !seen_pairs.contains(&pair) {
                            self.brain.merge_entities(e.id, best_id)?;
                            seen_pairs.insert(pair);
                            reconnected += 1;
                            continue;
                        }
                    }
                }
            }

            // Strategy 2: Connect to contemporary entities from same source URL
            // Find any source_url that mentions tokens from this entity
            for (url, src_ids) in &source_entities {
                let url_lower = url.to_lowercase();
                let matches_url = island_tokens.iter().any(|t| url_lower.contains(t.as_str()));
                if !matches_url {
                    continue;
                }
                // Connect to the highest-degree entity from this source
                let mut best: Option<(i64, usize)> = None;
                for &sid in src_ids {
                    if sid == e.id || !connected.contains(&sid) {
                        continue;
                    }
                    if let Some(ce) = id_to_entity.get(&sid) {
                        if ce.entity_type == e.entity_type
                            || HIGH_VALUE_TYPES.contains(&ce.entity_type.as_str())
                        {
                            let deg = relations
                                .iter()
                                .filter(|r| r.subject_id == sid || r.object_id == sid)
                                .count();
                            if best.is_none() || deg > best.unwrap().1 {
                                best = Some((sid, deg));
                            }
                        }
                    }
                }
                if let Some((hub_id, _)) = best {
                    let pair = (e.id.min(hub_id), e.id.max(hub_id));
                    if !seen_pairs.contains(&pair) {
                        // Create a relation rather than merge (they're different entities)
                        let predicate = if e.entity_type == "person" {
                            "contemporary_of"
                        } else {
                            "related_concept"
                        };
                        self.brain.upsert_relation(
                            e.id,
                            predicate,
                            hub_id,
                            &format!("prometheus:source_island_reconnect:{}", url),
                        )?;
                        seen_pairs.insert(pair);
                        reconnected += 1;
                        break;
                    }
                }
            }
        }

        if reconnected > 0 {
            eprintln!(
                "  [high-value-island-reconnect] reconnected {} islands via surname/source matching",
                reconnected
            );
        }
        Ok(reconnected)
    }

    /// Reconnect isolated person entities to connected persons sharing the same
    /// surname (last name token). Uses a relaxed confidence threshold (≥ 0.5)
    /// and requires only a matching last name plus compatible type. Creates
    /// `contemporary_of` relations rather than merging, since these are distinct
    /// individuals who may be related (same family, same field).
    pub fn reconnect_islands_by_surname_match(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        // Build surname → connected person ids index
        let mut surname_to_connected: HashMap<String, Vec<(i64, usize)>> = HashMap::new();
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }
        for e in &entities {
            if !connected.contains(&e.id) || e.entity_type != "person" {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }
            let surname = words.last().unwrap().to_lowercase();
            // Skip very short or common surnames that produce false positives
            if surname.len() < 4 {
                continue;
            }
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            surname_to_connected
                .entry(surname.clone())
                .or_default()
                .push((e.id, deg));
            // Also index under diacritics-stripped form for fuzzy matching
            let stripped = strip_diacritics(&surname);
            if stripped != surname {
                surname_to_connected
                    .entry(stripped)
                    .or_default()
                    .push((e.id, deg));
            }
        }

        // Common surnames to skip (too many false positives)
        let skip_surnames: HashSet<&str> = [
            "smith",
            "johnson",
            "williams",
            "brown",
            "jones",
            "miller",
            "davis",
            "wilson",
            "moore",
            "taylor",
            "anderson",
            "thomas",
            "jackson",
            "white",
            "harris",
            "martin",
            "thompson",
            "garcia",
            "martinez",
            "robinson",
            "clark",
            "rodriguez",
            "lewis",
            "lee",
            "walker",
            "hall",
            "allen",
            "young",
            "king",
            "wright",
            "scott",
            "green",
            "baker",
            "adams",
            "nelson",
            "hill",
            "campbell",
            "mitchell",
            "roberts",
            "carter",
            "phillips",
            "evans",
            "turner",
            "torres",
            "parker",
            "collins",
            "edwards",
            "stewart",
            "morris",
            "murphy",
            "cook",
            "rogers",
            "morgan",
            "bell",
            "cooper",
            "bailey",
            "reed",
            "ward",
            "cox",
            "howard",
            "ross",
            "wood",
            "james",
            "west",
            "long",
            "ford",
        ]
        .into_iter()
        .collect();

        let mut reconnected = 0usize;
        let mut seen: HashSet<(i64, i64)> = HashSet::new();

        for e in &entities {
            if connected.contains(&e.id)
                || e.entity_type != "person"
                || e.confidence < 0.5
                || is_noise_name(&e.name)
            {
                continue;
            }
            if reconnected >= 300 {
                break;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }
            let surname = words.last().unwrap().to_lowercase();
            if surname.len() < 4 || skip_surnames.contains(surname.as_str()) {
                continue;
            }

            // Try exact surname match first, then diacritics-stripped fallback
            let candidates = surname_to_connected.get(&surname).or_else(|| {
                let stripped = strip_diacritics(&surname);
                if stripped != surname {
                    surname_to_connected.get(&stripped)
                } else {
                    None
                }
            });
            if let Some(candidates) = candidates {
                // Pick the highest-degree connected person with this surname
                if let Some(&(best_id, _deg)) = candidates.iter().max_by_key(|(_, d)| *d) {
                    let pair = (e.id.min(best_id), e.id.max(best_id));
                    if !seen.contains(&pair) {
                        self.brain.upsert_relation(
                            e.id,
                            "contemporary_of",
                            best_id,
                            "prometheus:surname_island_reconnect",
                        )?;
                        seen.insert(pair);
                        reconnected += 1;
                    }
                }
            }
        }

        if reconnected > 0 {
            eprintln!(
                "  [surname-island-reconnect] reconnected {} person islands via surname matching",
                reconnected
            );
        }
        Ok(reconnected)
    }

    /// Reconnect isolated entities by matching their names against source URLs
    /// in existing relations. If "Stephen Hawking" is isolated and a relation has
    /// source_url containing "Stephen_Hawking" or "stephen-hawking", connect them.
    pub fn reconnect_islands_by_source_url_match(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        // Build source_url → set of connected entity IDs
        let mut url_entities: HashMap<String, HashSet<i64>> = HashMap::new();
        for r in &relations {
            if !r.source_url.is_empty() {
                let entry = url_entities.entry(r.source_url.clone()).or_default();
                entry.insert(r.subject_id);
                entry.insert(r.object_id);
            }
        }

        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        let mut reconnected = 0usize;
        for e in &entities {
            if connected.contains(&e.id) || is_noise_type(&e.entity_type) || is_noise_name(&e.name)
            {
                continue;
            }
            if e.name.len() < 5 || !e.name.contains(' ') {
                continue; // Skip single-word entities (too many false matches)
            }

            // Generate URL-friendly variants of the entity name
            let underscore = e.name.replace(' ', "_");
            let hyphen = e.name.to_lowercase().replace(' ', "-");
            let lower_underscore = e.name.to_lowercase().replace(' ', "_");

            // Search for matching source URLs
            let mut best_hub: Option<(i64, usize)> = None; // (entity_id, degree)
            for (url, eids) in &url_entities {
                if url.contains(&underscore)
                    || url.contains(&hyphen)
                    || url.contains(&lower_underscore)
                {
                    // Find highest-degree connected entity from this URL
                    for &eid in eids {
                        if eid == e.id {
                            continue;
                        }
                        let deg = relations
                            .iter()
                            .filter(|r| r.subject_id == eid || r.object_id == eid)
                            .count();
                        if best_hub.map_or(true, |(_, d)| deg > d) {
                            best_hub = Some((eid, deg));
                        }
                    }
                }
            }

            if let Some((hub_id, _)) = best_hub {
                let hub = match id_to_entity.get(&hub_id) {
                    Some(h) => h,
                    None => continue,
                };
                let pred = match (e.entity_type.as_str(), hub.entity_type.as_str()) {
                    ("person", "person") => "contemporary_of",
                    ("person", "place") | ("place", "person") => "active_in",
                    ("person", "organization") | ("organization", "person") => "affiliated_with",
                    ("person", "concept") | ("concept", "person") => "related_to",
                    ("place", "place") => "located_near",
                    _ => "related_to",
                };
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "INSERT OR IGNORE INTO relations (subject_id, predicate, object_id, confidence, source_url, learned_at)
                         VALUES (?1, ?2, ?3, 0.55, 'source_url_match', ?4)",
                        params![e.id, pred, hub_id, Utc::now().to_rfc3339()],
                    )?;
                    Ok(())
                })?;
                reconnected += 1;
                if reconnected >= 300 {
                    break;
                }
            }
        }
        if reconnected > 0 {
            eprintln!(
                "  [source-url-match] reconnected {} islands via source URL name matching",
                reconnected
            );
        }
        Ok(reconnected)
    }

    /// Type-pair predicate affinity scoring: adjusts hypothesis confidence based
    /// on empirical success rates for (subject_type, object_type, predicate)
    /// combinations. E.g., person→person with `contemporary_of` has high
    /// confirmation rates, while concept→place with `related_to` is weaker.
    pub fn apply_type_pair_affinity(&self, h: &mut Hypothesis) -> Result<()> {
        if h.object == "?" {
            return Ok(());
        }
        let s_ent = self.brain.get_entity_by_name(&h.subject)?;
        let o_ent = self.brain.get_entity_by_name(&h.object)?;
        if let (Some(se), Some(oe)) = (s_ent, o_ent) {
            // High-affinity pairings (boost)
            let high_affinity = matches!(
                (
                    se.entity_type.as_str(),
                    oe.entity_type.as_str(),
                    h.predicate.as_str()
                ),
                ("person", "person", "contemporary_of")
                    | ("person", "concept", "pioneered")
                    | ("person", "technology", "pioneered")
                    | ("person", "organization", "affiliated_with")
                    | ("person", "place", "active_in")
                    | ("organization", "place", "based_in")
                    | ("organization", "person", "affiliated_with")
            );
            // Low-affinity pairings (penalize)
            let low_affinity = matches!(
                (
                    se.entity_type.as_str(),
                    oe.entity_type.as_str(),
                    h.predicate.as_str()
                ),
                ("concept", "place", "related_to")
                    | ("place", "concept", "related_to")
                    | ("concept", "concept", "related_concept")
                    | ("place", "place", "contemporary_of")
                    | ("concept", "concept", "contemporary_of")
            );

            if high_affinity {
                h.confidence = (h.confidence * 1.08).min(1.0);
                h.evidence_for
                    .push("High-affinity type-predicate combination".to_string());
            } else if low_affinity {
                h.confidence *= 0.90;
                h.evidence_against
                    .push("Low-affinity type-predicate combination".to_string());
            }
        }
        Ok(())
    }

    /// Hypothesis quality scoring boost: hypotheses that would increase
    /// predicate diversity for their subject get a confidence boost.
    /// This helps avoid "pioneered-everything" hub formation.
    pub fn boost_diversity_increasing_hypotheses(
        &self,
        hypotheses: &mut [Hypothesis],
    ) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;
        let name_to_id: HashMap<&str, i64> =
            entities.iter().map(|e| (e.name.as_str(), e.id)).collect();

        // Build per-entity predicate distribution
        let mut entity_predicates: HashMap<i64, HashMap<String, usize>> = HashMap::new();
        for r in &relations {
            *entity_predicates
                .entry(r.subject_id)
                .or_default()
                .entry(r.predicate.clone())
                .or_insert(0) += 1;
            *entity_predicates
                .entry(r.object_id)
                .or_default()
                .entry(r.predicate.clone())
                .or_insert(0) += 1;
        }

        let mut boosted = 0usize;
        for h in hypotheses.iter_mut() {
            if h.status != HypothesisStatus::Testing && h.status != HypothesisStatus::Proposed {
                continue;
            }
            if let Some(&subj_id) = name_to_id.get(h.subject.as_str()) {
                if let Some(pred_counts) = entity_predicates.get(&subj_id) {
                    let total: usize = pred_counts.values().sum();
                    if total < 3 {
                        continue; // Too few relations to judge diversity
                    }
                    // Check if hypothesis predicate is already dominant
                    let existing_count = pred_counts.get(&h.predicate).copied().unwrap_or(0);
                    let dominance = existing_count as f64 / total as f64;

                    if dominance > 0.6 {
                        // Adding more of the same predicate → penalize
                        h.confidence *= 0.85;
                        h.evidence_against.push(format!(
                            "Predicate '{}' already dominates ({:.0}%) for {}",
                            h.predicate,
                            dominance * 100.0,
                            h.subject
                        ));
                    } else if existing_count == 0 {
                        // New predicate type → boost
                        h.confidence = (h.confidence * 1.10).min(0.95);
                        h.evidence_for.push(format!(
                            "New predicate '{}' would increase diversity for {}",
                            h.predicate, h.subject
                        ));
                        boosted += 1;
                    }
                }
            }
        }
        Ok(boosted)
    }

    /// Reclassify island entities by inferring type from their name tokens
    /// against the most common type for those tokens in the connected graph.
    pub fn reclassify_islands_by_token_type(&self) -> Result<usize> {
        let relations = self.brain.all_relations()?;
        let entities = self.brain.all_entities()?;

        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Build token → type distribution from connected entities
        let mut token_types: HashMap<String, HashMap<String, usize>> = HashMap::new();
        for e in &entities {
            if !connected.contains(&e.id) || is_noise_type(&e.entity_type) || is_noise_name(&e.name)
            {
                continue;
            }
            for word in e.name.to_lowercase().split_whitespace() {
                let w = word.trim_matches(|c: char| !c.is_alphanumeric());
                if w.len() >= 4 {
                    *token_types
                        .entry(w.to_string())
                        .or_default()
                        .entry(e.entity_type.clone())
                        .or_insert(0) += 1;
                }
            }
        }

        let mut reclassified = 0usize;
        for e in &entities {
            if connected.contains(&e.id) || is_noise_type(&e.entity_type) || is_noise_name(&e.name)
            {
                continue;
            }
            // Only reclassify "concept" entities (most likely misclassified)
            if e.entity_type != "concept" {
                continue;
            }
            // Count type votes from name tokens
            let mut type_votes: HashMap<String, usize> = HashMap::new();
            let mut total_votes = 0usize;
            for word in e.name.to_lowercase().split_whitespace() {
                let w = word.trim_matches(|c: char| !c.is_alphanumeric());
                if let Some(types) = token_types.get(w) {
                    for (t, count) in types {
                        if t != "concept" {
                            // Don't vote for concept (that's what we're trying to fix)
                            *type_votes.entry(t.clone()).or_insert(0) += count;
                            total_votes += count;
                        }
                    }
                }
            }
            if total_votes < 3 {
                continue;
            }
            // Require strong consensus (>60% agreement)
            if let Some((best_type, best_count)) = type_votes.iter().max_by_key(|(_, v)| *v) {
                if *best_count as f64 / total_votes as f64 > 0.6 && best_type != "concept" {
                    let _ = self.brain.with_conn(|conn| {
                        conn.execute(
                            "UPDATE entities SET entity_type = ?1 WHERE id = ?2",
                            params![best_type, e.id],
                        )?;
                        Ok(())
                    });
                    reclassified += 1;
                }
            }
        }

        if reclassified > 0 {
            eprintln!(
                "  [token-type-reclassify] reclassified {} island concepts by token voting",
                reclassified
            );
        }
        Ok(reclassified)
    }

    // -----------------------------------------------------------------------
    // Nickname / diminutive expansion merge
    // -----------------------------------------------------------------------

    /// Merge isolated person entities whose first name is a common nickname
    /// of a connected entity's first name (e.g., "Alex Waibel" → "Alexander Waibel").
    pub fn merge_nickname_variants(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut connected: HashSet<i64> = HashSet::new();
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Nickname → full name(s) mapping
        let nickname_map: &[(&[&str], &[&str])] = &[
            (&["alex", "alec"], &["alexander", "alexandra", "alexei"]),
            (&["mike", "mick"], &["michael"]),
            (&["bill", "billy", "will", "willy"], &["william"]),
            (&["bob", "bobby", "rob", "robby"], &["robert"]),
            (&["dick", "rick", "ricky"], &["richard"]),
            (&["jim", "jimmy"], &["james"]),
            (&["joe", "joey"], &["joseph"]),
            (&["jack"], &["john", "jonathan"]),
            (&["johnny"], &["john"]),
            (&["tom", "tommy"], &["thomas"]),
            (
                &["ed", "eddie", "ted", "teddy"],
                &["edward", "theodore", "edmund"],
            ),
            (&["ben", "benny"], &["benjamin"]),
            (&["charlie", "chuck"], &["charles"]),
            (&["dave", "davy"], &["david"]),
            (&["dan", "danny"], &["daniel"]),
            (&["steve"], &["steven", "stephen"]),
            (&["matt"], &["matthew", "matthias"]),
            (&["nick", "nicky"], &["nicholas", "nicolas", "nikolai"]),
            (
                &["chris"],
                &["christopher", "christian", "christine", "christina"],
            ),
            (&["tony"], &["anthony", "antonio"]),
            (&["frank", "frankie"], &["francis", "franklin", "francisco"]),
            (&["harry"], &["henry", "harold", "harrison"]),
            (&["larry"], &["lawrence", "laurence"]),
            (&["jerry"], &["gerald", "jerome", "jeremiah"]),
            (&["sam", "sammy"], &["samuel", "samantha"]),
            (&["pete"], &["peter"]),
            (&["fred", "freddy"], &["frederick", "alfred"]),
            (&["al"], &["albert", "alfred", "alan"]),
            (&["bert"], &["albert", "bertrand", "herbert", "robert"]),
            (&["ken", "kenny"], &["kenneth"]),
            (&["ron", "ronny"], &["ronald"]),
            (&["don", "donny"], &["donald"]),
            (&["pat"], &["patrick", "patricia"]),
            (&["andy"], &["andrew", "andreas"]),
            (&["max"], &["maximilian", "maxwell"]),
            (&["liz", "lizzy", "beth"], &["elizabeth", "elisabeth"]),
            (
                &["kate", "kathy", "katie"],
                &["katherine", "catherine", "kathleen"],
            ),
            (&["sue", "susie"], &["susan", "suzanne"]),
            (&["meg", "maggie"], &["margaret"]),
            (&["jenny", "jen"], &["jennifer", "virginia"]),
            (&["dmitri"], &["dmitry", "dmitrii"]),
            (&["nikolai"], &["nikolay", "nicolas"]),
            (&["johann"], &["johannes"]),
            (&["henri"], &["henry"]),
            (&["georg"], &["george"]),
            (&["friedrich"], &["frederick"]),
            (&["wilhelm"], &["william"]),
            (&["karl"], &["carl", "charles"]),
            (&["josef"], &["joseph"]),
            (&["ludwig"], &["louis"]),
            (&["ernst"], &["ernest"]),
        ];

        // Build: lowercase first name → set of equivalent first names
        let mut equivalences: HashMap<String, HashSet<String>> = HashMap::new();
        for (nicks, fulls) in nickname_map {
            let mut group: HashSet<String> = HashSet::new();
            for n in *nicks {
                group.insert(n.to_string());
            }
            for f in *fulls {
                group.insert(f.to_string());
            }
            let group_clone = group.clone();
            for name in &group_clone {
                equivalences
                    .entry(name.clone())
                    .or_default()
                    .extend(group.iter().cloned());
            }
        }

        // Build last-name → list of (id, first_name_lower, degree) for person entities
        let mut by_last: HashMap<String, Vec<(i64, String, usize)>> = HashMap::new();
        for e in &entities {
            if e.entity_type != "person" {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }
            let last = words[words.len() - 1].to_lowercase();
            let first = words[0].to_lowercase();
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            by_last.entry(last).or_default().push((e.id, first, deg));
        }

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        for (_last, group) in &by_last {
            if group.len() < 2 || group.len() > 10 {
                continue;
            }
            for i in 0..group.len() {
                let (id_a, first_a, deg_a) = &group[i];
                if absorbed.contains(id_a) {
                    continue;
                }
                let equiv_a = match equivalences.get(first_a) {
                    Some(e) => e,
                    None => continue,
                };
                for j in (i + 1)..group.len() {
                    let (id_b, first_b, deg_b) = &group[j];
                    if absorbed.contains(id_b) || id_a == id_b {
                        continue;
                    }
                    if equiv_a.contains(first_b) {
                        // Merge: keep the one with higher degree (or longer name if tied)
                        let (keep, remove) = if deg_a >= deg_b {
                            (*id_a, *id_b)
                        } else {
                            (*id_b, *id_a)
                        };
                        self.brain.merge_entities(remove, keep)?;
                        absorbed.insert(remove);
                        merged += 1;
                    }
                }
            }
        }

        Ok(merged)
    }

    // -----------------------------------------------------------------------
    // Transliteration-variant merge (edit distance)
    // -----------------------------------------------------------------------

    /// Merge person entities with identical last names whose first names differ
    /// by only 1-2 characters (transliteration variants like Mendeleyev/Mendeleev,
    /// Tchaikovsky/Chaikovsky, etc.)
    pub fn merge_transliteration_variants(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        // Group by (entity_type, last_word)
        let mut by_key: HashMap<(String, String), Vec<(i64, String, usize)>> = HashMap::new();
        for e in &entities {
            if !matches!(e.entity_type.as_str(), "person" | "place" | "concept") {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }
            let last = words.last().unwrap().to_lowercase();
            let prefix = words[..words.len() - 1]
                .iter()
                .map(|w| w.to_lowercase())
                .collect::<Vec<_>>()
                .join(" ");
            let deg = degree.get(&e.id).copied().unwrap_or(0);
            by_key
                .entry((e.entity_type.clone(), last))
                .or_default()
                .push((e.id, prefix, deg));
        }

        let mut merged = 0usize;
        let mut absorbed: HashSet<i64> = HashSet::new();

        for group in by_key.values() {
            if group.len() < 2 || group.len() > 20 {
                continue;
            }
            for i in 0..group.len() {
                let (id_a, prefix_a, deg_a) = &group[i];
                if absorbed.contains(id_a) {
                    continue;
                }
                for j in (i + 1)..group.len() {
                    let (id_b, prefix_b, deg_b) = &group[j];
                    if absorbed.contains(id_b) || id_a == id_b {
                        continue;
                    }
                    let dist = levenshtein(prefix_a, prefix_b);
                    let min_len = prefix_a.len().min(prefix_b.len());
                    // Allow edit distance 1 for short prefixes, 2 for longer
                    let max_dist = if min_len <= 4 { 1 } else { 2 };
                    if dist <= max_dist && dist > 0 {
                        let (keep, remove) = if deg_a >= deg_b {
                            (*id_a, *id_b)
                        } else {
                            (*id_b, *id_a)
                        };
                        self.brain.merge_entities(remove, keep)?;
                        absorbed.insert(remove);
                        merged += 1;
                    }
                }
            }
        }

        Ok(merged)
    }

    // -----------------------------------------------------------------------
    // Reclassify well-known isolated concept entities
    // -----------------------------------------------------------------------

    /// Reclassify isolated concept entities that are clearly historical figures
    /// or geographical locations based on keyword patterns in names.
    pub fn reclassify_isolated_concepts_extended(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        let place_keywords: HashSet<&str> = [
            "jerusalem",
            "damascus",
            "babylon",
            "constantinople",
            "carthage",
            "mesopotamia",
            "anatolia",
            "transoxiana",
            "dalmatia",
            "numidia",
            "mauretania",
            "ravenna",
            "westphalia",
            "bohemia",
            "silesia",
            "saxony",
            "prussia",
            "burgundy",
            "normandy",
            "aquitaine",
            "lombardy",
            "catalonia",
            "andalusia",
            "castile",
            "navarre",
            "galicia",
            "sardinia",
            "corsica",
            "sicily",
            "crete",
            "cyprus",
            "rhodes",
            "sumatra",
            "borneo",
            "java",
            "bahrain",
            "balochistan",
            "belgium",
            "britain",
            "china",
            "india",
            "atlantic",
            "caspian",
            "caucasus",
            "chandigarh",
            "bletchley",
        ]
        .iter()
        .copied()
        .collect();

        let tech_keywords: HashSet<&str> = [
            "javascript",
            "python",
            "haskell",
            "fortran",
            "cobol",
            "tensorflow",
            "pytorch",
            "kubernetes",
            "blockchain",
            "youtube",
            "spotify",
            "docker",
            "linux",
            "unix",
            "golang",
            "rust",
            "typescript",
            "react",
            "angular",
            "graphql",
            "redis",
            "mongodb",
            "postgresql",
            "elasticsearch",
            "kafka",
            "hadoop",
            "spark",
            "terraform",
        ]
        .iter()
        .copied()
        .collect();

        // Well-known historical/scientific figures often misclassified as concept
        let person_keywords: HashSet<&str> = [
            "charlemagne",
            "plato",
            "aristotle",
            "socrates",
            "talleyrand",
            "metternich",
            "bismarck",
            "napoleon",
            "hannibal",
            "cicero",
            "seneca",
            "virgil",
            "ovid",
            "horace",
            "plutarch",
            "thucydides",
            "herodotus",
            "xenophon",
            "suetonius",
            "tacitus",
            "livy",
            "polybius",
            "confucius",
            "avicenna",
            "averroes",
            "maimonides",
            "saladin",
            "tamerlane",
            "genghis",
            "kublai",
            "chebyshev",
            "plancherel",
            "hopcroft",
            "diophantus",
            "euclid",
            "archimedes",
            "pythagoras",
            "hippocrates",
            "galen",
            "ptolemy",
            "copernicus",
            "vesalius",
            "paracelsus",
            "linnaeus",
            "lavoisier",
            "fourier",
            "lagrange",
            "laplace",
            "cauchy",
            "galois",
            "abel",
            "riemann",
            "poincaré",
            "hilbert",
            "cantor",
            "dedekind",
            "weierstrass",
            "jacobi",
            "ramanujan",
            "erdős",
            "grothendieck",
            "serre",
            "weil",
            "gauss",
            "fermat",
            "descartes",
            "leibniz",
            "voltaire",
            "rousseau",
            "montesquieu",
            "diderot",
            "locke",
            "hobbes",
            "hume",
            "kant",
            "hegel",
            "nietzsche",
            "schopenhauer",
            "kierkegaard",
            "spinoza",
            "machiavelli",
            "erasmus",
            "aquinas",
            "augustine",
            "boethius",
            "bayezid",
            "suleiman",
            "mehmed",
            "selim",
            "murad",
            "whittaker",
            "newton",
            "gödel",
            "boltzmann",
            "bose",
            "bardeen",
            "bednorz",
            "belousov",
            "besicovitch",
            "champernowne",
            "douady",
            "cooper",
            "cornell",
            "asoka",
            "banerji",
            "allchin",
            "clift",
            "brooke",
            "adams",
            "alexander",
            "barbara",
        ]
        .iter()
        .copied()
        .collect();

        let org_keywords: HashSet<&str> = [
            "unesco", "nato", "opec", "asean", "cern", "nasa", "darpa", "ieee", "acm",
        ]
        .iter()
        .copied()
        .collect();

        let mut reclassified = 0usize;

        for e in &entities {
            if e.entity_type != "concept" || connected.contains(&e.id) {
                continue;
            }
            let lower = e.name.to_lowercase();

            let new_type = if place_keywords.contains(lower.as_str()) {
                Some("place")
            } else if tech_keywords.contains(lower.as_str()) {
                Some("technology")
            } else if person_keywords.contains(lower.as_str()) {
                Some("person")
            } else if org_keywords.contains(lower.as_str()) {
                Some("organization")
            } else {
                None
            };

            if let Some(nt) = new_type {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE entities SET entity_type = ?1 WHERE id = ?2",
                        params![nt, e.id],
                    )?;
                    Ok(())
                })?;
                reclassified += 1;
            }
        }

        Ok(reclassified)
    }

    /// Reclassify connected single-word concepts as persons when they match
    /// a surname of an existing person entity in the graph. Also refines
    /// "pioneered" predicates to "contemporary_of" for person→person pairs.
    pub fn reclassify_connected_surname_concepts(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        // Build surname set from known person entities (2+ word names)
        let mut known_surnames: HashSet<String> = HashSet::new();
        for e in &entities {
            if e.entity_type == "person" {
                let words: Vec<&str> = e.name.split_whitespace().collect();
                if words.len() >= 2 {
                    if let Some(last) = words.last() {
                        let lower = last.to_lowercase();
                        if lower.len() > 3 {
                            known_surnames.insert(lower);
                        }
                    }
                }
            }
        }

        // Find connected single-word concepts that match known surnames
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        let mut reclassified = 0usize;
        for e in &entities {
            if e.entity_type != "concept" {
                continue;
            }
            if !connected.contains(&e.id) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() != 1 {
                continue;
            }
            let lower = e.name.to_lowercase();
            if lower.len() <= 3 {
                continue;
            }
            // Skip words that are clearly not surnames
            if is_common_english_word(&lower) {
                continue;
            }
            if !known_surnames.contains(&lower) {
                continue;
            }

            // Reclassify concept → person
            self.brain.with_conn(|conn| {
                conn.execute(
                    "UPDATE entities SET entity_type = 'person' WHERE id = ?1",
                    params![e.id],
                )?;
                Ok(())
            })?;

            // Refine "pioneered" predicates involving this entity
            for r in &relations {
                if r.object_id == e.id && r.predicate == "pioneered" {
                    let _ = self.brain.with_conn(|conn| {
                        let res = conn.execute(
                            "UPDATE relations SET predicate = 'contemporary_of' WHERE id = ?1",
                            params![r.id],
                        );
                        match res {
                            Ok(_) => Ok(()),
                            Err(rusqlite::Error::SqliteFailure(err, _))
                                if err.code == rusqlite::ErrorCode::ConstraintViolation =>
                            {
                                conn.execute("DELETE FROM relations WHERE id = ?1", params![r.id])?;
                                Ok(())
                            }
                            Err(e) => Err(e),
                        }
                    });
                }
            }

            reclassified += 1;
        }

        if reclassified > 0 {
            eprintln!(
                "  [surname-concept-reclassify] reclassified {} single-word concepts as persons (matched known surnames)",
                reclassified
            );
        }
        Ok(reclassified)
    }

    /// Purge isolated person entities whose names consist entirely of common
    /// English words (not proper names). These are NLP extraction artifacts.
    pub fn purge_common_english_person_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        let mut purged = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            if e.entity_type != "person" {
                continue;
            }
            // Only target low-confidence islands
            if e.confidence > 0.7 {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 || words.len() > 3 {
                continue;
            }
            // Check if ALL words are common English words (not names)
            let all_common = words.iter().all(|w| {
                let lower = w.to_lowercase();
                is_common_english_word(&lower)
            });
            if !all_common {
                continue;
            }
            self.brain.with_conn(|conn| {
                conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                Ok(())
            })?;
            purged += 1;
        }
        if purged > 0 {
            eprintln!(
                "  [common-english-person-purge] removed {} isolated person entities with all-common-English-word names",
                purged
            );
        }
        Ok(purged)
    }

    /// Reclassify person islands that match geographic/event/concept patterns.
    ///
    /// Catches Latin prefixes (Pax, Via, Porta, Terra, etc.), geographic suffixes
    /// (Corridor, Basin, Pass, etc.), and historical event suffixes (Accord, Pact,
    /// Treaty, Schism, etc.) that NLP misclassifies as person entities.
    pub fn reclassify_geographic_person_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        // Latin/geographic prefixes that indicate places, not persons
        let place_prefixes: &[&str] = &[
            "pax ", "via ", "porta ", "pont ", "mons ", "mont ", "cabo ", "isla ", "sierra ",
            "terra ", "lago ", "rio ", "val ", "col ", "fort ", "camp ", "klein ", "gross ",
            "groß ",
        ];

        // Suffixes indicating geographic features or events
        let place_suffixes: &[&str] = &[
            " corridor",
            " basin",
            " pass",
            " gorge",
            " canyon",
            " reef",
            " glacier",
            " plateau",
            " delta",
            " strait",
            " gulf",
            " cape",
            " peninsula",
            " archipelago",
            " lagoon",
            " fjord",
            " atoll",
            " oasis",
            " steppe",
            " tundra",
            " savanna",
            " savannah",
            " noir",
            " noire",
            " blanche",
            " haute",
            " basse",
        ];

        let concept_suffixes: &[&str] = &[
            " accord",
            " pact",
            " treaty",
            " schism",
            " cordiale",
            " knot",
            " manifesto",
            " doctrine",
            " heresy",
            " crusade",
            " armistice",
            " convention",
            " protocol",
            " charter",
            " concordat",
            " edict",
            " decree",
            " fusion",
            " fission",
            " sanitaire",
            " entente",
        ];

        let mut reclassified = 0usize;
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            if e.entity_type != "person" || e.confidence > 0.7 {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }

            let lower = e.name.to_lowercase();

            let is_place = place_prefixes.iter().any(|p| lower.starts_with(p))
                || place_suffixes.iter().any(|s| lower.ends_with(s));
            let is_concept = concept_suffixes.iter().any(|s| lower.ends_with(s));

            if is_place {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE entities SET entity_type = 'place' WHERE id = ?1",
                        params![e.id],
                    )?;
                    Ok(())
                })?;
                reclassified += 1;
            } else if is_concept {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE entities SET entity_type = 'concept' WHERE id = ?1",
                        params![e.id],
                    )?;
                    Ok(())
                })?;
                reclassified += 1;
            }
        }
        if reclassified > 0 {
            eprintln!(
                "  [geo-person-reclassify] reclassified {} isolated person entities to place/concept",
                reclassified
            );
        }
        Ok(reclassified)
    }

    /// Purge ancient low-confidence islands: entities that were first seen >30 days ago,
    /// Reclassify isolated person entities that are actually place+region concatenations
    /// (e.g., "Annapolis Maryland", "Hucknall Nottinghamshire", "Hennigsdorf Havelland").
    /// These are NLP extraction artifacts from geographic references like "Annapolis, Maryland".
    pub fn reclassify_place_region_person_islands(&self) -> Result<usize> {
        // Known US states, regions, and administrative divisions that appear as second words
        const REGIONS: &[&str] = &[
            "alabama",
            "alaska",
            "arizona",
            "arkansas",
            "california",
            "colorado",
            "connecticut",
            "delaware",
            "florida",
            "georgia",
            "hawaii",
            "idaho",
            "illinois",
            "indiana",
            "iowa",
            "kansas",
            "kentucky",
            "louisiana",
            "maine",
            "maryland",
            "massachusetts",
            "michigan",
            "minnesota",
            "mississippi",
            "missouri",
            "montana",
            "nebraska",
            "nevada",
            "hampshire",
            "jersey",
            "mexico",
            "york",
            "carolina",
            "dakota",
            "ohio",
            "oklahoma",
            "oregon",
            "pennsylvania",
            "island",
            "tennessee",
            "texas",
            "utah",
            "vermont",
            "virginia",
            "washington",
            "wisconsin",
            "wyoming",
            "columbia",
            // UK counties / regions
            "nottinghamshire",
            "yorkshire",
            "lancashire",
            "oxfordshire",
            "berkshire",
            "buckinghamshire",
            "cambridgeshire",
            "cheshire",
            "cornwall",
            "cumbria",
            "derbyshire",
            "devon",
            "dorset",
            "durham",
            "essex",
            "gloucestershire",
            "hampshire",
            "herefordshire",
            "hertfordshire",
            "kent",
            "leicestershire",
            "lincolnshire",
            "london",
            "merseyside",
            "norfolk",
            "northamptonshire",
            "northumberland",
            "somerset",
            "staffordshire",
            "suffolk",
            "surrey",
            "sussex",
            "warwickshire",
            "wiltshire",
            "worcestershire",
            // German regions
            "havelland",
            "brandenburg",
            "sachsen",
            "bayern",
            "hessen",
            "thüringen",
            "westfalen",
            "württemberg",
            "pommern",
            "schlesien",
            "preussen",
            "schöneberg",
            "charlottenburg",
            "kreuzberg",
            "neukölln",
            "spandau",
            // French/Italian/Spanish regions
            "provence",
            "normandie",
            "bretagne",
            "aquitaine",
            "picardie",
            "lombardia",
            "toscana",
            "piemonte",
            "veneto",
            "sicilia",
            "andalucía",
            "cataluña",
            "galicia",
            "asturias",
            "navarra",
        ];

        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        let mut reclassified = 0usize;
        for e in &entities {
            if connected.contains(&e.id) || e.entity_type != "person" || e.confidence > 0.7 {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() != 2 {
                continue;
            }
            let second_lower = words[1].to_lowercase();
            if REGIONS.contains(&second_lower.as_str()) {
                // Check first word starts with uppercase (place name)
                if words[0]
                    .chars()
                    .next()
                    .map(|c| c.is_uppercase())
                    .unwrap_or(false)
                {
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "UPDATE entities SET entity_type = 'place' WHERE id = ?1",
                            params![e.id],
                        )?;
                        Ok(())
                    })?;
                    reclassified += 1;
                }
            }
        }
        if reclassified > 0 {
            eprintln!(
                "  [place-region-reclassify] reclassified {} 'CityName RegionName' person islands to place",
                reclassified
            );
        }
        Ok(reclassified)
    }

    /// have confidence ≤0.5, no relations, and no facts. These have had plenty of time
    /// to be connected and haven't been — they're extraction noise.
    pub fn purge_ancient_low_confidence_islands(&self, max_age_days: i64) -> Result<usize> {
        let cutoff = (Utc::now() - chrono::Duration::days(max_age_days))
            .format("%Y-%m-%d")
            .to_string();

        let count: usize = self.brain.with_conn(|conn| {
            let removed = conn.execute(
                "DELETE FROM entities WHERE id IN (
                    SELECT e.id FROM entities e
                    WHERE e.confidence <= 0.5
                    AND e.first_seen < ?1
                    AND e.id NOT IN (SELECT subject_id FROM relations)
                    AND e.id NOT IN (SELECT object_id FROM relations)
                    AND e.id NOT IN (SELECT entity_id FROM facts)
                    AND e.access_count <= 1
                )",
                params![cutoff],
            )?;
            Ok(removed)
        })?;

        if count > 0 {
            eprintln!(
                "  [ancient-island-purge] removed {} low-confidence islands older than {} days",
                count, max_age_days
            );
        }
        Ok(count)
    }

    /// Reclassify isolated entities with en-dash compound names (e.g., "Calabi–Yau",
    /// "Bose–Einstein") from place/person to concept. These are mathematical/physical
    /// theories named after their creators, not geographic locations or people.
    pub fn reclassify_endash_compound_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        let mut reclassified = 0usize;
        for e in &entities {
            if connected.contains(&e.id) || e.entity_type == "concept" {
                continue;
            }
            // Must contain en-dash (–) or em-dash (—) separating two capitalized words
            let has_dash = e.name.contains('–') || e.name.contains('—');
            if !has_dash {
                continue;
            }
            // Split on dash and check both parts look like surnames
            let parts: Vec<&str> = e
                .name
                .split(|c| c == '–' || c == '—')
                .map(|s| s.trim())
                .filter(|s| !s.is_empty())
                .collect();
            if parts.len() != 2 {
                continue;
            }
            // Both parts should start with uppercase (surname-like)
            let both_capitalized = parts
                .iter()
                .all(|p| p.chars().next().map(|c| c.is_uppercase()).unwrap_or(false));
            if !both_capitalized {
                continue;
            }
            // Reclassify to concept
            self.brain.with_conn(|conn| {
                conn.execute(
                    "UPDATE entities SET entity_type = 'concept' WHERE id = ?1",
                    params![e.id],
                )?;
                Ok(())
            })?;
            reclassified += 1;
        }
        if reclassified > 0 {
            eprintln!(
                "  [endash-reclassify] reclassified {} en-dash compound islands to concept",
                reclassified
            );
        }
        Ok(reclassified)
    }

    /// Broader minute-level temporal reconnection: connect isolated high-confidence
    /// entities to the nearest hub entity that was first_seen in the same minute.
    /// This is wider than the 10-second cohort in `reconnect_islands_by_source` and
    /// catches entities from the same crawl page that were extracted seconds apart.
    pub fn reconnect_islands_by_minute_cohort(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut connected: HashSet<i64> = HashSet::new();
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        // Group entities by first_seen minute
        let mut minute_cohorts: HashMap<String, Vec<i64>> = HashMap::new();
        for e in &entities {
            if is_noise_type(&e.entity_type) || is_noise_name(&e.name) {
                continue;
            }
            if e.confidence < 0.7 {
                continue;
            }
            let ts_key = e.first_seen.format("%Y-%m-%dT%H:%M").to_string();
            minute_cohorts.entry(ts_key).or_default().push(e.id);
        }

        let mut reconnected = 0usize;
        for (_ts, cohort) in &minute_cohorts {
            if cohort.len() < 3 || cohort.len() > 50 {
                continue;
            }
            // Find the highest-degree connected entity as hub
            let hub_id = match cohort
                .iter()
                .filter(|id| connected.contains(id))
                .max_by_key(|&&id| degree.get(&id).copied().unwrap_or(0))
            {
                Some(&id) => id,
                None => continue,
            };
            let hub = match id_to_entity.get(&hub_id) {
                Some(e) => e,
                None => continue,
            };
            // Only use hubs with degree >= 2 (real anchor nodes)
            if degree.get(&hub_id).copied().unwrap_or(0) < 2 {
                continue;
            }

            for &eid in cohort {
                if connected.contains(&eid) || eid == hub_id {
                    continue;
                }
                let island = match id_to_entity.get(&eid) {
                    Some(e) => e,
                    None => continue,
                };
                // Must be high-value type
                if !HIGH_VALUE_TYPES.contains(&island.entity_type.as_str()) {
                    continue;
                }
                let pred = match (island.entity_type.as_str(), hub.entity_type.as_str()) {
                    ("person", "person") => "contemporary_of",
                    ("person", "place") | ("place", "person") => "active_in",
                    ("person", "organization") | ("organization", "person") => "affiliated_with",
                    ("person", "concept") => "contributed_to",
                    ("concept", "person") => "pioneered_by",
                    ("concept", "concept") => "related_concept",
                    ("organization", "concept") | ("concept", "organization") => "works_on",
                    ("organization", "organization") => "partner_of",
                    ("place", "place") => "located_near",
                    _ => "co_extracted_with",
                };
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "INSERT OR IGNORE INTO relations (subject_id, predicate, object_id, confidence, source_url, learned_at)
                         VALUES (?1, ?2, ?3, 0.35, 'minute_cohort', ?4)",
                        params![eid, pred, hub_id, Utc::now().to_rfc3339()],
                    )?;
                    Ok(())
                })?;
                connected.insert(eid);
                reconnected += 1;
                if reconnected >= 800 {
                    return Ok(reconnected);
                }
            }
        }
        if reconnected > 0 {
            eprintln!(
                "  [minute-cohort-reconnect] reconnected {} islands via minute-level temporal cohort",
                reconnected
            );
        }
        Ok(reconnected)
    }

    /// Handle large temporal cohorts (>50 entities) that minute-cohort skips.
    /// Sub-partitions by entity_type, then connects each type-sub-group to the
    /// highest-degree connected entity of that type (or any type) within the cohort.
    /// This catches bulk-import batches where hundreds of entities arrive together.
    pub fn reconnect_large_cohorts_by_type(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut connected: HashSet<i64> = HashSet::new();
        let mut degree: HashMap<i64, usize> = HashMap::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
            *degree.entry(r.subject_id).or_insert(0) += 1;
            *degree.entry(r.object_id).or_insert(0) += 1;
        }

        let id_to_entity: HashMap<i64, &crate::db::Entity> =
            entities.iter().map(|e| (e.id, e)).collect();

        // Group by first_seen minute
        let mut minute_cohorts: HashMap<String, Vec<i64>> = HashMap::new();
        for e in &entities {
            if is_noise_type(&e.entity_type) || is_noise_name(&e.name) {
                continue;
            }
            if e.confidence < 0.7 {
                continue;
            }
            let ts_key = e.first_seen.format("%Y-%m-%dT%H:%M").to_string();
            minute_cohorts.entry(ts_key).or_default().push(e.id);
        }

        let mut reconnected = 0usize;
        for (_ts, cohort) in &minute_cohorts {
            // Only handle large cohorts (the ones minute-cohort skips)
            if cohort.len() <= 50 {
                continue;
            }

            // Sub-partition by entity_type
            let mut type_groups: HashMap<&str, Vec<i64>> = HashMap::new();
            for &eid in cohort {
                if let Some(e) = id_to_entity.get(&eid) {
                    if HIGH_VALUE_TYPES.contains(&e.entity_type.as_str()) {
                        type_groups.entry(&e.entity_type).or_default().push(eid);
                    }
                }
            }

            for (_etype, group) in &type_groups {
                if group.len() < 2 {
                    continue;
                }
                // Find highest-degree connected entity as hub (from whole cohort)
                let hub_id = match cohort
                    .iter()
                    .filter(|id| connected.contains(id))
                    .max_by_key(|&&id| degree.get(&id).copied().unwrap_or(0))
                {
                    Some(&id) => id,
                    None => {
                        // No connected hub: pick highest-confidence entity as anchor
                        let anchor = group.iter().max_by(|&&a, &&b| {
                            let ca = id_to_entity.get(&a).map(|e| e.confidence).unwrap_or(0.0);
                            let cb = id_to_entity.get(&b).map(|e| e.confidence).unwrap_or(0.0);
                            ca.partial_cmp(&cb).unwrap_or(std::cmp::Ordering::Equal)
                        });
                        match anchor {
                            Some(&id) => id,
                            None => continue,
                        }
                    }
                };
                let hub = match id_to_entity.get(&hub_id) {
                    Some(e) => e,
                    None => continue,
                };

                // Connect up to 30 isolated entities from this type-group to hub
                let mut group_count = 0usize;
                for &eid in group {
                    if connected.contains(&eid) || eid == hub_id {
                        continue;
                    }
                    let island = match id_to_entity.get(&eid) {
                        Some(e) => e,
                        None => continue,
                    };
                    let pred = match (island.entity_type.as_str(), hub.entity_type.as_str()) {
                        ("person", "person") => "contemporary_of",
                        ("person", "place") | ("place", "person") => "active_in",
                        ("person", "organization") | ("organization", "person") => {
                            "affiliated_with"
                        }
                        ("person", "concept") => "contributed_to",
                        ("concept", "person") => "pioneered_by",
                        ("concept", "concept") => "related_concept",
                        ("organization", "organization") => "partner_of",
                        ("place", "place") => "located_near",
                        _ => "co_extracted_with",
                    };
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "INSERT OR IGNORE INTO relations (subject_id, predicate, object_id, confidence, source_url, learned_at)
                             VALUES (?1, ?2, ?3, 0.30, 'large_cohort_type', ?4)",
                            params![eid, pred, hub_id, Utc::now().to_rfc3339()],
                        )?;
                        Ok(())
                    })?;
                    connected.insert(eid);
                    reconnected += 1;
                    group_count += 1;
                    if group_count >= 80 {
                        break;
                    }
                }
                if reconnected >= 2000 {
                    break;
                }
            }
            if reconnected >= 2000 {
                break;
            }
        }
        if reconnected > 0 {
            eprintln!(
                "  [large-cohort-type-reconnect] reconnected {} islands from large temporal cohorts",
                reconnected
            );
        }
        Ok(reconnected)
    }

    /// Connect high-value isolated entities (access_count ≥ 5) to each other
    /// when they share the same temporal cohort (same minute of first_seen).
    /// Unlike minute-cohort reconnection, this doesn't require an existing hub —
    /// it creates a small cluster among the high-value islands themselves,
    /// picking the highest-access entity as the cluster anchor.
    pub fn reconnect_high_value_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;

        let mut connected: HashSet<i64> = HashSet::new();
        for r in &relations {
            connected.insert(r.subject_id);
            connected.insert(r.object_id);
        }

        // Group high-value isolated entities by minute cohort
        let mut cohorts: HashMap<String, Vec<(i64, &crate::db::Entity)>> = HashMap::new();
        for e in &entities {
            if connected.contains(&e.id) {
                continue;
            }
            if is_noise_type(&e.entity_type) || is_noise_name(&e.name) {
                continue;
            }
            if e.access_count < 5 {
                continue;
            }
            if !HIGH_VALUE_TYPES.contains(&e.entity_type.as_str()) {
                continue;
            }
            let ts_key = e.first_seen.format("%Y-%m-%dT%H:%M").to_string();
            cohorts.entry(ts_key).or_default().push((e.id, e));
        }

        let mut reconnected = 0usize;
        for (_ts, mut cohort) in cohorts {
            if cohort.len() < 2 || cohort.len() > 40 {
                continue;
            }
            // Pick highest-access entity as anchor
            cohort.sort_by(|a, b| b.1.access_count.cmp(&a.1.access_count));
            let anchor_id = cohort[0].0;
            let anchor = cohort[0].1;

            for &(eid, island) in &cohort[1..] {
                if connected.contains(&eid) {
                    continue;
                }
                let pred = match (island.entity_type.as_str(), anchor.entity_type.as_str()) {
                    ("person", "person") => "contemporary_of",
                    ("person", "place") | ("place", "person") => "active_in",
                    ("person", "organization") | ("organization", "person") => "affiliated_with",
                    ("person", "concept") | ("concept", "person") => "related_to",
                    ("concept", "concept") => "related_concept",
                    ("place", "place") => "located_near",
                    _ => "co_extracted_with",
                };
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "INSERT OR IGNORE INTO relations (subject_id, predicate, object_id, confidence, source_url, learned_at)
                         VALUES (?1, ?2, ?3, 0.30, 'hv_island_cohort', ?4)",
                        params![eid, pred, anchor_id, Utc::now().to_rfc3339()],
                    )?;
                    Ok(())
                })?;
                connected.insert(eid);
                reconnected += 1;
                if reconnected >= 500 {
                    return Ok(reconnected);
                }
            }
        }
        if reconnected > 0 {
            eprintln!(
                "  [hv-island-cohort] reconnected {} high-value islands via temporal cohort",
                reconnected
            );
        }
        Ok(reconnected)
    }

    /// Detect and reclassify person entities whose names are actually well-known
    /// two-word compound concepts (e.g., "Aneutronic Fusion", "Diamond Necklace").
    /// Uses a heuristic: if the second word is a common noun (not a name) and the
    /// first word is an adjective/material/descriptor, it's likely a concept.
    pub fn reclassify_compound_concept_persons(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        // Common noun second words that indicate concepts, not persons
        let concept_nouns: HashSet<&str> = [
            "fusion",
            "fission",
            "necklace",
            "paradox",
            "theorem",
            "effect",
            "principle",
            "constant",
            "formula",
            "conjecture",
            "inequality",
            "hypothesis",
            "spectrum",
            "cycle",
            "process",
            "mechanism",
            "reaction",
            "equation",
            "transform",
            "function",
            "series",
            "distribution",
            "integral",
            "matrix",
            "tensor",
            "field",
            "algebra",
            "geometry",
            "topology",
            "manifold",
            "lattice",
            "protocol",
            "algorithm",
            "cipher",
            "code",
            "machine",
            "engine",
            "turbine",
            "reactor",
            "accelerator",
            "collider",
            "telescope",
            "microscope",
            "radiation",
            "emission",
            "absorption",
            "diffraction",
            "interference",
            "resonance",
            "oscillation",
            "entropy",
            "enthalpy",
            "catalysis",
            "synthesis",
            "analysis",
            "plague",
            "epidemic",
            "pandemic",
            "famine",
            "drought",
            "insurgency",
            "rebellion",
            "mutiny",
            "blockade",
            "embargo",
            "boycott",
            "lockout",
            "putsch",
            "coup",
            "restoration",
            // Additional concept indicators
            "walk",
            "ride",
            "trail",
            "path",
            "incident",
            "affair",
            "scandal",
            "controversy",
            "disaster",
            "catastrophe",
            "messenger",
            "triumph",
            "victory",
            "defeat",
            "retreat",
            "advance",
            "offensive",
            "forest",
            "math",
            "mathematics",
            "physics",
            "chemistry",
            "biology",
            "geology",
            "astronomy",
            "economics",
            "philosophy",
            "literature",
            "history",
            "archaeology",
            "linguistics",
            "anthropology",
            "sociology",
            "psychology",
            "despotate",
            "khanate",
            "sultanate",
            "caliphate",
            "emirate",
            "vilayet",
            "eyalet",
            "pashalik",
            "sanjak",
            "prefecture",
            "province",
            "territory",
            "protectorate",
            "dominion",
            "commonwealth",
            "confederation",
            "federation",
            "republic",
            "monarchy",
            "theocracy",
            "mosquito",
            "hypersaline",
            "discovery",
            "exploration",
            "navigation",
            "colonization",
            "industrialization",
            "modernization",
            "urbanization",
            "globalization",
        ]
        .into_iter()
        .collect();

        let mut reclassified = 0usize;
        for e in &entities {
            if connected.contains(&e.id) || e.entity_type != "person" || e.confidence > 0.6 {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() < 2 {
                continue;
            }
            // Check last word of multi-word name (covers 2+ word compounds)
            let last_lower = words.last().unwrap().to_lowercase();
            if concept_nouns.contains(last_lower.as_str()) {
                self.brain.with_conn(|conn| {
                    conn.execute(
                        "UPDATE entities SET entity_type = 'concept' WHERE id = ?1",
                        params![e.id],
                    )?;
                    Ok(())
                })?;
                reclassified += 1;
            }
        }
        if reclassified > 0 {
            eprintln!(
                "  [compound-concept-reclassify] reclassified {} two-word compound concept persons",
                reclassified
            );
        }
        Ok(reclassified)
    }

    /// Reclassify **connected** person entities whose names contain strong geographic
    /// or institutional markers (Region, Area, Heights, University, etc.).
    /// Unlike `reclassify_geographic_person_islands` which only handles isolated entities,
    /// this catches misclassified entities that already have relations, preventing
    /// nonsensical hypotheses like "Schengen Area related_to Person".
    pub fn reclassify_geographic_persons_connected(&self) -> Result<usize> {
        // Suffixes that unambiguously indicate places (not person names)
        const PLACE_SUFFIXES: &[&str] = &[
            " Region",
            " Area",
            " Heights",
            " Oasis",
            " Valley",
            " Island",
            " Islands",
            " Basin",
            " Peninsula",
            " Mountain",
            " Mountains",
            " Desert",
            " Forest",
            " Lake",
            " River",
            " Bay",
            " Gulf",
            " Strait",
            " Channel",
            " Park",
            " Garden",
            " Gardens",
            " Province",
            " County",
            " District",
            " Prefecture",
            " Territory",
            " Coast",
            " Plain",
            " Plains",
            " Plateau",
            " Glacier",
            " Reef",
            " Atoll",
            " Canyon",
            " Gorge",
            " Fjord",
            " Lagoon",
            " Delta",
            " Steppe",
            " Tundra",
            " Savanna",
            " Savannah",
            " Patagonia",
        ];
        // Suffixes that indicate organizations
        const ORG_SUFFIXES: &[&str] = &[
            " University",
            " Institute",
            " College",
            " School",
            " Museum",
            " Academy",
            " Foundation",
            " Laboratory",
            " Hospital",
            " Corporation",
            " Association",
            " Society",
            " Committee",
            " Commission",
            " Agency",
            " Bureau",
            " Ministry",
            " Department",
            " Conclave",
        ];

        let mut reclassified = 0usize;
        self.brain.with_conn(|conn| {
            // Find person entities matching place suffixes
            let mut stmt =
                conn.prepare("SELECT id, name FROM entities WHERE entity_type = 'person'")?;
            let entities: Vec<(i64, String)> = stmt
                .query_map([], |row| Ok((row.get(0)?, row.get(1)?)))?
                .filter_map(|r| r.ok())
                .collect();

            for (id, name) in &entities {
                let is_place = PLACE_SUFFIXES.iter().any(|s| name.ends_with(s));
                let is_org = ORG_SUFFIXES.iter().any(|s| name.ends_with(s));

                if is_place {
                    conn.execute(
                        "UPDATE entities SET entity_type = 'place' WHERE id = ?1",
                        params![id],
                    )?;
                    reclassified += 1;
                } else if is_org {
                    conn.execute(
                        "UPDATE entities SET entity_type = 'organization' WHERE id = ?1",
                        params![id],
                    )?;
                    reclassified += 1;
                }
            }
            Ok(())
        })?;

        if reclassified > 0 {
            eprintln!(
                "  [geo-connected-reclassify] reclassified {} connected person entities to place/org",
                reclassified
            );
        }
        Ok(reclassified)
    }

    /// Reclassify single-word concept islands by matching against surnames of
    /// well-connected person entities in the graph. If "Boltzmann" is isolated as
    /// a concept but "Ludwig Boltzmann" exists as a connected person, reclassify
    /// the island as a person (making it a merge candidate for later passes).
    pub fn reclassify_concept_islands_by_graph_surnames(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        // Build surname → person-type map from connected entities
        let mut surname_types: HashMap<String, &str> = HashMap::new();
        for e in &entities {
            if !connected.contains(&e.id) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() >= 2 {
                let surname = words.last().unwrap().to_lowercase();
                if surname.len() > 3 {
                    surname_types
                        .entry(surname)
                        .or_insert(match e.entity_type.as_str() {
                            "person" => "person",
                            "place" => "place",
                            "organization" => "organization",
                            _ => "person",
                        });
                }
            }
        }

        let mut reclassified = 0usize;
        for e in &entities {
            if e.entity_type != "concept" || connected.contains(&e.id) {
                continue;
            }
            let words: Vec<&str> = e.name.split_whitespace().collect();
            if words.len() != 1 {
                continue;
            }
            let lower = e.name.to_lowercase();
            if lower.len() <= 3 {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }

            if let Some(&new_type) = surname_types.get(&lower) {
                if new_type != e.entity_type.as_str() {
                    self.brain.with_conn(|conn| {
                        conn.execute(
                            "UPDATE entities SET entity_type = ?1 WHERE id = ?2",
                            params![new_type, e.id],
                        )?;
                        Ok(())
                    })?;
                    reclassified += 1;
                }
            }
        }

        Ok(reclassified)
    }

    /// Purge concept islands that are common English words (capitalized due to
    /// sentence-start extraction).
    pub fn purge_common_word_concept_islands(&self) -> Result<usize> {
        let entities = self.brain.all_entities()?;
        let relations = self.brain.all_relations()?;
        let connected: HashSet<i64> = relations
            .iter()
            .flat_map(|r| [r.subject_id, r.object_id])
            .collect();

        let common_words: HashSet<&str> = [
            "abstract",
            "academic",
            "account",
            "achievement",
            "acquired",
            "additional",
            "advanced",
            "affairs",
            "agreement",
            "analysis",
            "ancient",
            "annual",
            "applied",
            "approach",
            "argument",
            "article",
            "aspect",
            "attempt",
            "attention",
            "authority",
            "available",
            "basic",
            "beginning",
            "behind",
            "between",
            "biological",
            "board",
            "brief",
            "called",
            "capable",
            "capital",
            "central",
            "challenge",
            "characteristic",
            "chief",
            "classical",
            "climate",
            "colonial",
            "combined",
            "commission",
            "committee",
            "common",
            "community",
            "comparison",
            "complex",
            "component",
            "concept",
            "conclusion",
            "condition",
            "conference",
            "conflict",
            "congress",
            "consequence",
            "considerable",
            "considered",
            "contemporary",
            "context",
            "contrast",
            "control",
            "conventional",
            "corresponding",
            "council",
            "course",
            "critical",
            "cultural",
            "current",
            "debate",
            "decade",
            "defined",
            "definition",
            "degree",
            "democratic",
            "department",
            "described",
            "description",
            "designed",
            "detail",
            "determined",
            "developed",
            "development",
            "different",
            "digital",
            "dimension",
            "direction",
            "discussion",
            "distinct",
            "division",
            "document",
            "domain",
            "domestic",
            "dominant",
            "during",
            "earlier",
            "economic",
            "edition",
            "education",
            "effective",
            "element",
            "emergence",
            "empire",
            "engaged",
            "engineering",
            "enterprise",
            "environment",
            "episode",
            "equivalent",
            "essential",
            "established",
            "european",
            "evidence",
            "evolution",
            "examination",
            "example",
            "exchange",
            "executive",
            "exercise",
            "exhibition",
            "existence",
            "expanded",
            "experience",
            "experiment",
            "explanation",
            "expression",
            "extended",
            "external",
            "facility",
            "factor",
            "feature",
            "federal",
            "figure",
            "finally",
            "financial",
            "following",
            "foreign",
            "formal",
            "formation",
            "former",
            "foundation",
            "framework",
            "function",
            "fundamental",
            "general",
            "generation",
            "global",
            "golden",
            "government",
            "graduate",
            "greater",
            "heritage",
            "higher",
            "historical",
            "however",
            "identical",
            "impact",
            "imperial",
            "important",
            "incident",
            "included",
            "including",
            "independent",
            "indigenous",
            "individual",
            "industrial",
            "influence",
            "initial",
            "innovation",
            "inspired",
            "institution",
            "instrument",
            "intellectual",
            "intelligence",
            "intended",
            "interaction",
            "interest",
            "internal",
            "international",
            "interpretation",
            "intervention",
            "introduced",
            "invasion",
            "investigation",
            "involved",
            "journal",
            "knowledge",
            "largely",
            "launched",
            "leading",
            "legacy",
            "legislation",
            "length",
            "limited",
            "located",
            "maintained",
            "majority",
            "managed",
            "management",
            "manual",
            "material",
            "mathematical",
            "mechanism",
            "medieval",
            "meeting",
            "member",
            "memorial",
            "method",
            "migration",
            "military",
            "mineral",
            "minister",
            "ministry",
            "mission",
            "mobile",
            "moderate",
            "modern",
            "modified",
            "monarch",
            "movement",
            "multiple",
            "municipal",
            "national",
            "natural",
            "network",
            "normally",
            "northern",
            "notable",
            "nuclear",
            "objective",
            "observation",
            "obtained",
            "occurred",
            "official",
            "operation",
            "opinion",
            "opposition",
            "ordered",
            "original",
            "otherwise",
            "overall",
            "pacific",
            "parallel",
            "parliament",
            "particular",
            "passage",
            "period",
            "permanent",
            "personal",
            "perspective",
            "physical",
            "planned",
            "platform",
            "political",
            "popular",
            "population",
            "position",
            "possible",
            "potential",
            "powerful",
            "practical",
            "presence",
            "presented",
            "previous",
            "primary",
            "principal",
            "principle",
            "produced",
            "production",
            "professional",
            "program",
            "project",
            "prominent",
            "promoted",
            "proportion",
            "proposed",
            "protected",
            "province",
            "published",
            "purpose",
            "radical",
            "reaction",
            "received",
            "recognition",
            "recommended",
            "recorded",
            "recovery",
            "reform",
            "regional",
            "regulation",
            "related",
            "relation",
            "relative",
            "released",
            "religious",
            "remained",
            "remarkable",
            "removed",
            "replaced",
            "reported",
            "represented",
            "republic",
            "required",
            "response",
            "responsible",
            "restored",
            "resulted",
            "returned",
            "revealed",
            "revolution",
            "roman",
            "scientific",
            "section",
            "selected",
            "separate",
            "sequence",
            "served",
            "service",
            "session",
            "settlement",
            "several",
            "significant",
            "similar",
            "situated",
            "situation",
            "social",
            "solution",
            "southern",
            "special",
            "specific",
            "spirit",
            "statement",
            "station",
            "strategic",
            "subsequent",
            "substantial",
            "successful",
            "sufficient",
            "suggested",
            "superior",
            "supplied",
            "supported",
            "surface",
            "survey",
            "survived",
            "technique",
            "territory",
            "theory",
            "traditional",
            "transfer",
            "transition",
            "treatment",
            "tribal",
            "triumph",
            "typically",
            "unified",
            "unique",
            "united",
            "universal",
            "various",
            "version",
            "victory",
            "virtual",
            "visible",
            "vision",
            "wealth",
            "weapon",
            "western",
            "workshop",
        ]
        .iter()
        .copied()
        .collect();

        let mut purged = 0usize;
        for e in &entities {
            if e.entity_type != "concept" || connected.contains(&e.id) {
                continue;
            }
            if e.name.contains(' ') {
                continue;
            }
            let facts = self.brain.get_facts_for(e.id)?;
            if !facts.is_empty() {
                continue;
            }
            let lower = e.name.to_lowercase();
            if common_words.contains(lower.as_str()) {
                self.brain.with_conn(|conn| {
                    conn.execute("DELETE FROM entities WHERE id = ?1", params![e.id])?;
                    Ok(())
                })?;
                purged += 1;
            }
        }

        Ok(purged)
    }
}

// ---------------------------------------------------------------------------
// Free helpers
// ---------------------------------------------------------------------------

/// Simple Levenshtein edit distance between two strings.
fn levenshtein(a: &str, b: &str) -> usize {
    let a: Vec<char> = a.chars().collect();
    let b: Vec<char> = b.chars().collect();
    let (m, n) = (a.len(), b.len());
    if m == 0 {
        return n;
    }
    if n == 0 {
        return m;
    }
    let mut prev: Vec<usize> = (0..=n).collect();
    let mut curr = vec![0usize; n + 1];
    for i in 1..=m {
        curr[0] = i;
        for j in 1..=n {
            let cost = if a[i - 1] == b[j - 1] { 0 } else { 1 };
            curr[j] = (prev[j] + 1).min(curr[j - 1] + 1).min(prev[j - 1] + cost);
        }
        std::mem::swap(&mut prev, &mut curr);
    }
    prev[n]
}

/// Check if an entity name looks like a citation fragment (common in Wikipedia extraction).
fn looks_like_citation(name: &str) -> bool {
    let lower = name.to_lowercase();
    // Contains volume/page indicators
    if lower.contains(" vol ") || lower.contains(" pp ") || lower.contains(" no ") {
        return true;
    }
    // Contains "journal" anywhere
    if lower.contains("journal") {
        return true;
    }
    // Ends with common citation noise
    let citation_suffixes = ["press", "publishers", "publishing", "edition", "eds"];
    let last_word = lower.split_whitespace().last().unwrap_or("");
    if citation_suffixes.contains(&last_word) && lower.split_whitespace().count() >= 3 {
        return true;
    }
    false
}

/// Determine if an isolated entity is likely extraction noise from Wikipedia/encyclopedias.
/// Aggressive filter — only apply to entities with ZERO relations.
fn is_extraction_noise(name: &str, entity_type: &str) -> bool {
    // Already caught by noise filters
    if is_noise_name(name) || is_noise_type(entity_type) {
        return true;
    }
    let lower = name.to_lowercase();
    let word_count = lower.split_whitespace().count();

    // Citation fragments
    if looks_like_citation(name) {
        return true;
    }

    // Names with mixed case patterns suggesting concatenated references
    if word_count >= 3 {
        let words: Vec<&str> = name.split_whitespace().collect();
        let has_trailing_noise = [
            "During",
            "Resting",
            "Commentary",
            "Surveys",
            "Proceedings",
            "Magazine",
            "Review",
            "Bulletin",
            "Like",
        ];
        if words.len() >= 2 && has_trailing_noise.contains(&words[words.len() - 1]) {
            return true;
        }
    }

    // Pure acronyms that are too short to be meaningful when isolated
    if word_count == 1 && name.len() <= 3 && name.chars().all(|c| c.is_uppercase()) {
        return true;
    }

    // Possessive/genitive forms alone (e.g. "Switzerland's", "Ramanujan's")
    if lower.ends_with("'s") || lower.ends_with("\u{2019}s") {
        return true;
    }

    // Starts with "The " followed by a single generic word
    if lower.starts_with("the ") && word_count == 2 {
        return true;
    }

    // Multi-word fragments containing prepositions/articles mid-name suggest sentence chunks
    let filler_words = [
        "of", "the", "and", "in", "on", "for", "to", "by", "from", "with", "at", "an", "or",
    ];
    if word_count >= 3 {
        let words: Vec<&str> = lower.split_whitespace().collect();
        let filler_count = words.iter().filter(|w| filler_words.contains(w)).count();
        // If >40% of words are fillers + total is long, it's a sentence fragment
        if filler_count >= 2 && filler_count as f64 / word_count as f64 > 0.35 {
            return true;
        }
    }

    // Names that look like "Topic1 Topic2 Topic3" — concatenated unrelated words
    // Heuristic: 4+ capitalized words with no filler words = keyword salad
    if word_count >= 4 {
        let words: Vec<&str> = name.split_whitespace().collect();
        let cap_count = words
            .iter()
            .filter(|w| w.starts_with(|c: char| c.is_uppercase()))
            .count();
        let filler = words
            .iter()
            .filter(|w| filler_words.contains(&w.to_lowercase().as_str()))
            .count();
        if cap_count >= 4 && filler == 0 {
            return true;
        }
    }

    // Entity names containing "MacTutor", "Archive", "ISBN" — reference noise
    let ref_noise = ["mactutor", "archive", "isbn", "doi:", "arxiv", "springer"];
    if ref_noise.iter().any(|r| lower.contains(r)) {
        return true;
    }

    // Multi-word entities containing words that indicate concatenation artifacts
    // e.g., "Sicily Result Spartan", "Valium Velcro", "Champollion Lettre"
    if word_count >= 2 {
        let concat_noise_words = [
            "result",
            "results",
            "lettre",
            "lettres",
            "velcro",
            "semiorder",
            "versus",
            "chapter",
            "chapters",
            "volume",
            "volumes",
            "edition",
            "appendix",
            "index",
            "table",
            "tables",
            "figure",
            "figures",
            "section",
            "sections",
            "page",
            "pages",
            "footnote",
            "footnotes",
            "bibliography",
            "glossary",
            "preface",
            "prologue",
            "epilogue",
            "abstract",
            "summary",
            "overview",
            "introduction",
            "conclusion",
        ];
        let words_lower: Vec<String> = lower.split_whitespace().map(|s| s.to_string()).collect();
        if words_lower
            .iter()
            .any(|w| concat_noise_words.contains(&w.as_str()))
        {
            return true;
        }
    }

    // Single generic words that got capitalized by NLP extractors
    let generic_singles = [
        "actor",
        "areas",
        "unity",
        "proof",
        "lieutenant",
        "terminology",
        "calinger",
        "estreicher",
        "improvement",
        "superchip",
        "location",
        "outcome",
        "abolition",
        "pickwick",
        "ruffini",
        "newton",
        "historically",
        "politically",
        "coalition",
    ];
    if word_count == 1 && generic_singles.contains(&lower.as_str()) {
        return true;
    }

    // Type mismatches: places classified as persons, etc.
    // "Crystal Palace" as person, "Middle East" as person, etc.
    let place_indicators = [
        "palace",
        "east",
        "west",
        "north",
        "south",
        "island",
        "ocean",
        "mountain",
        "valley",
        "river",
        "lake",
        "strait",
        "peninsula",
        "gulf",
        "bay",
        "coast",
        "border",
        "frontier",
        "colony",
        "republic",
        "kingdom",
        "empire",
    ];
    if entity_type == "person" && word_count >= 2 {
        let has_place_word = lower
            .split_whitespace()
            .any(|w| place_indicators.contains(&w));
        if has_place_word {
            return true;
        }
    }

    // Names starting with "Source " — extraction artifact
    if lower.starts_with("source ") {
        return true;
    }

    // Names that are clearly descriptions, not entities: contain "Years", "Survey", "Overview"
    let desc_words = [
        "survey",
        "overview",
        "übersicht",
        "origins",
        "recognition",
        "processing",
        "diagnosing",
        "improvement",
        "dependability",
        "embedded",
        "quantum",
        "symposium",
    ];
    if word_count >= 2 && desc_words.iter().any(|d| lower.contains(d)) {
        return true;
    }

    // Entity name == entity type (e.g. entity "Actor" of type "concept")
    if lower == entity_type {
        return true;
    }

    // Non-English words that commonly appear as extracted entity fragments
    // (French, German academic/citation text)
    let non_english_noise = [
        "accompagnées",
        "pensées",
        "chaires",
        "mémoires",
        "études",
        "régime",
        "même",
        "après",
        "année",
        "années",
        "siècle",
        "über",
        "und",
        "eine",
        "eines",
        "junge",
        "neue",
        "neuen",
        "des",
        "der",
        "die",
        "das",
        "dem",
        "den",
        "für",
        "avec",
        "dans",
        "pour",
        "les",
        "aux",
        "sur",
        "une",
        "della",
        "degli",
        "delle",
        "nelle",
        "nella",
        "dello",
    ];
    if word_count >= 2 {
        let words: Vec<&str> = lower.split_whitespace().collect();
        let foreign_count = words
            .iter()
            .filter(|w| non_english_noise.contains(w))
            .count();
        // If >30% of words are non-English noise, it's a citation fragment
        if foreign_count >= 1 && foreign_count as f64 / word_count as f64 > 0.3 {
            return true;
        }
    }

    // Academic journal abbreviation patterns: entities containing "Monthly Notices",
    // "Astrophysical Journal", "Physical Review", etc.
    let journal_patterns = [
        "monthly notices",
        "physical review",
        "astrophysical",
        "letters to",
        "annals of",
        "proceedings of",
        "transactions of",
        "reviews of",
        "reports on",
        "advances in",
        "frontiers in",
        "studies in",
    ];
    if word_count >= 2 && journal_patterns.iter().any(|jp| lower.contains(jp)) {
        return true;
    }

    // Entities where known entity name is prefixed/suffixed with random context words
    // Pattern: "SomeContext KnownEntity MoreContext" where the middle is what matters
    // Detect by checking for ALL-CAPS abbreviations mixed with regular words
    if word_count >= 3 {
        let words: Vec<&str> = name.split_whitespace().collect();
        let has_acronym = words.iter().any(|w| {
            w.len() >= 3
                && w.len() <= 8
                && w.chars().all(|c| c.is_uppercase() || c.is_ascii_digit())
        });
        let has_normal = words.iter().any(|w| {
            w.len() >= 3
                && w.starts_with(|c: char| c.is_uppercase())
                && w.chars().skip(1).any(|c| c.is_lowercase())
        });
        // "CFHTLenS Monthly Notices" pattern: acronym + generic words
        if has_acronym && has_normal && word_count <= 4 {
            let generic_trail: HashSet<&str> = [
                "monthly", "notices", "letters", "review", "reports", "papers", "notes", "studies",
                "focus", "dead", "mass",
            ]
            .iter()
            .copied()
            .collect();
            let generic_count = words
                .iter()
                .filter(|w| generic_trail.contains(&w.to_lowercase().as_str()))
                .count();
            if generic_count >= 1 {
                return true;
            }
        }
    }

    // Mixed-script names (Latin + Cyrillic/Arabic/CJK) — usually translation artifacts
    let has_latin = lower.chars().any(|c| c.is_ascii_alphabetic());
    let has_non_latin = name.chars().any(|c| {
        c.is_alphabetic()
            && !c.is_ascii_alphabetic()
            && c != 'é'
            && c != 'è'
            && c != 'ê'
            && c != 'ë'
            && c != 'à'
            && c != 'â'
            && c != 'ä'
            && c != 'ö'
            && c != 'ü'
            && c != 'ß'
            && c != 'ñ'
            && c != 'ç'
            && c != 'î'
            && c != 'ô'
            && c != 'û'
            && c != 'æ'
            && c != 'ø'
            && c != 'å'
            && c != 'í'
            && c != 'ó'
            && c != 'ú'
    });
    if has_latin && has_non_latin && word_count >= 3 {
        return true;
    }

    // Names ending with "Post-Intelligencer", "Modelling", "Diploma" — org/concept, not entities
    let noise_endings = ["post-intelligencer", "modelling", "diploma", "semantics"];
    if word_count >= 2 {
        let last = lower.split_whitespace().last().unwrap_or("");
        if noise_endings.contains(&last) {
            return true;
        }
    }

    // Sentence fragments disguised as entities: "unknown" type with verb-laden text.
    // Real entities are noun phrases; sentence fragments contain verbs, pronouns, determiners.
    if entity_type == "unknown" && word_count >= 3 {
        let words: Vec<&str> = lower.split_whitespace().collect();
        let sentence_verbs = [
            "is",
            "was",
            "were",
            "are",
            "will",
            "would",
            "could",
            "should",
            "can",
            "may",
            "might",
            "has",
            "had",
            "have",
            "did",
            "does",
            "do",
            "been",
            "being",
            "became",
            "become",
            "came",
            "went",
            "said",
            "made",
            "found",
            "gave",
            "took",
            "got",
            "proved",
            "lost",
            "destroyed",
            "decided",
            "published",
            "continued",
            "controlled",
            "succeeded",
            "predicted",
            "conquered",
            "invaded",
            "defeated",
            "established",
            "discovered",
            "built",
            "wrote",
            "studied",
            "increased",
            "decreased",
            "produced",
            "created",
            "remained",
            "returned",
            "appeared",
            "contained",
            "included",
            "involved",
            "began",
            "started",
            "ended",
            "died",
            "born",
            "lived",
        ];
        let pronouns = [
            "he", "she", "it", "his", "her", "its", "their", "they", "them", "him", "who", "whom",
            "whose", "which", "what", "that", "this", "these", "those",
        ];
        let verb_count = words.iter().filter(|w| sentence_verbs.contains(w)).count();
        let pronoun_count = words.iter().filter(|w| pronouns.contains(w)).count();
        // 2+ verbs/pronouns in a 3+ word "entity" = sentence fragment
        if verb_count + pronoun_count >= 2 {
            return true;
        }
        // Even 1 verb + starting with lowercase = sentence fragment
        if verb_count >= 1 {
            if let Some(first_char) = name.chars().next() {
                if first_char.is_lowercase() {
                    return true;
                }
            }
        }
    }

    // "unknown" type entities over 40 chars are almost always extraction errors
    if entity_type == "unknown" && name.len() > 40 {
        return true;
    }

    false
}

/// Determine if a single-word isolated entity should be purged.
/// Returns true for generic English words, citation surnames, adverbs, adjectives, etc.
/// Preserves entities that look like well-known proper nouns or technical terms.
fn should_purge_single_word(name: &str, entity_type: &str) -> bool {
    let lower = name.to_lowercase();
    let len = lower.len();

    // Very short names are almost always noise
    if len <= 3 {
        return true;
    }

    // Already caught by noise filters
    if is_noise_name(name) || is_noise_type(entity_type) {
        return true;
    }

    // Starts with lowercase → not a proper noun → generic word
    if name.starts_with(|c: char| c.is_lowercase()) {
        return true;
    }

    // For "concept" type: aggressively purge common English word patterns
    if entity_type == "concept" {
        // Adverbs ending in -ly
        if lower.ends_with("ly") && len >= 5 {
            return true;
        }
        // Adjectives: -ous, -ive, -ful, -less, -able, -ible, -ical, -ary, -ory
        let adj_suffixes = [
            "ous", "ive", "ful", "less", "able", "ible", "ical", "ary", "ory", "ish", "ular",
            "inal", "ular", "etic", "atic",
        ];
        if adj_suffixes.iter().any(|s| lower.ends_with(s)) && len >= 6 {
            return true;
        }
        // Past participles (-ed), gerunds (-ing) — often sentence fragments
        if (lower.ends_with("ed") || lower.ends_with("ing")) && len >= 5 {
            return true;
        }
        // Plural abstract concepts (-ies, -isms, -ists, -ments, -tions, -nesses)
        let abstract_suffixes = [
            "isms", "ists", "ments", "tions", "nesses", "ities", "ences", "ances",
        ];
        if abstract_suffixes.iter().any(|s| lower.ends_with(s)) {
            return true;
        }
        // Common generic concept words
        let generic_concepts = [
            "unlike",
            "others",
            "battle",
            "timeline",
            "further",
            "please",
            "learn",
            "multiple",
            "lectures",
            "origins",
            "bulletin",
            "elements",
            "universe",
            "physicists",
            "aside",
            "together",
            "video",
            "consider",
            "attempts",
            "fellows",
            "chamber",
            "user",
            "contexts",
            "humans",
            "lowest",
            "bells",
            "comic",
            "chariot",
            "comet",
            "ceramic",
            "stellar",
            "temperature",
            "falcon",
            "piranha",
            "mitten",
            "treatise",
            "television",
            "particle",
            "stratosphere",
            "antimatter",
            "apply",
            "strongly",
            "axioms",
            "debates",
            "problems",
            "ideas",
            "systems",
            "methods",
            "models",
            "levels",
            "forces",
            "fields",
            "waves",
            "forms",
            "rules",
            "tools",
            "parts",
            "types",
            "modes",
            "roles",
            "units",
            "rates",
            "phases",
            "zones",
            "loops",
            "paths",
            "nodes",
            "links",
            "terms",
            "claims",
            "facts",
            "texts",
            "codes",
            "tests",
            "maps",
            "keys",
            "data",
            "sets",
            "rows",
            "logs",
            "tags",
            "runs",
            "gaps",
            "ends",
            "bits",
            "aims",
        ];
        if generic_concepts.contains(&lower.as_str()) {
            return true;
        }
        // Single-word concept entities that look like surnames (capitalized, 5-12 chars,
        // ending in common surname suffixes) are almost always citation artifacts
        let surname_suffixes = [
            "ier", "iere", "ski", "sky", "ley", "ley", "ner", "ger", "ler", "sen", "son", "man",
            "men", "kov", "ova", "enko", "elli", "ini", "otti", "ardi", "ardy", "burg", "dorf",
            "feld", "stein", "berg", "wald", "rff", "off", "eff", "ych", "vich", "wicz",
        ];
        let chars: Vec<char> = name.chars().collect();
        if chars.len() >= 5
            && chars.len() <= 14
            && chars[0].is_uppercase()
            && chars[1..]
                .iter()
                .all(|c| c.is_lowercase() || *c == '-' || *c == '\'')
            && surname_suffixes.iter().any(|s| lower.ends_with(s))
        {
            return true;
        }
        return false;
    }

    // For "person" type: purge if it looks like a citation surname
    if entity_type == "person" {
        // Single-word "person" entities are almost always citation last names
        // unless they're a very well-known mononymous person
        let known_mononymous = [
            "aristotle",
            "plato",
            "socrates",
            "euclid",
            "archimedes",
            "confucius",
            "avicenna",
            "averroes",
            "fibonacci",
            "michelangelo",
            "raphael",
            "caravaggio",
            "rembrandt",
            "voltaire",
            "napoleon",
            "galileo",
            "copernicus",
            "hypatia",
            "ptolemy",
            "hippocrates",
            "pythagoras",
            "herodotus",
            "homer",
            "thales",
            "democritus",
            "epicurus",
            "seneca",
            "virgil",
            "ovid",
            "tacitus",
            "livy",
            "cicero",
            "nero",
            "caesar",
            "cleopatra",
            "hannibal",
            "xerxes",
            "charlemagne",
            "saladin",
            "tamerlane",
            "maimonides",
            "rumi",
            "hafez",
            "omar",
            "drake",
            "magellan",
            "columbus",
            "vespucci",
            "pizarro",
            "cortez",
            "nostradamus",
            "paracelsus",
            "vesalius",
            "kepler",
            "descartes",
            "pascal",
            "leibniz",
            "euler",
            "gauss",
            "riemann",
            "hilbert",
            "poincaré",
            "noether",
            "ramanujan",
            "turing",
            "gödel",
            "shannon",
            "babbage",
            "lovelace",
            "tesla",
            "edison",
            "faraday",
            "maxwell",
            "boltzmann",
            "heisenberg",
            "schrödinger",
            "dirac",
            "feynman",
            "hawking",
            "einstein",
            "newton",
            "darwin",
            "mendel",
            "pasteur",
            "curie",
            "planck",
            "bohr",
            "rutherford",
            "fermi",
            "oppenheimer",
            "madonna",
            "beyoncé",
            "shakira",
            "adele",
            "rihanna",
            "drake",
            "eminem",
            "bono",
            "cher",
            "prince",
            "moby",
            "björk",
            "sia",
            "pelé",
            "ronaldinho",
            "neymar",
            "ronaldo",
            "messi",
            "madonna",
            "picasso",
            "banksy",
            "kandinsky",
            "monet",
            "renoir",
            "cézanne",
            "matisse",
            "warhol",
            "pollock",
            "dostoevsky",
            "tolstoy",
            "chekhov",
            "pushkin",
            "nabokov",
            "kafka",
            "goethe",
            "nietzsche",
            "hegel",
            "kant",
            "spinoza",
            "hume",
            "locke",
            "hobbes",
            "rousseau",
            "montesquieu",
            "machiavelli",
            "buddha",
            "confucius",
            "laozi",
            "zoroaster",
            "muhammad",
            "moses",
            "jesus",
        ];
        if known_mononymous.contains(&lower.as_str()) {
            return false; // Keep well-known mononymous persons
        }
        // Otherwise, single-word person is almost certainly a citation surname
        return true;
    }

    // For "organization" type: single-word orgs are usually abbreviations or generic
    if entity_type == "organization" && len <= 5 {
        return true;
    }

    false
}

/// Detect if an entity type is wrong based on name patterns.
/// Returns Some(correct_type) or None if current type seems fine.
fn detect_correct_type(lower: &str, words: &[&str], current_type: &str) -> Option<&'static str> {
    let word_count = words.len();

    // Place-like names classified as person/concept
    let place_words = [
        "east",
        "west",
        "north",
        "south",
        "island",
        "islands",
        "ocean",
        "sea",
        "mountain",
        "mountains",
        "valley",
        "river",
        "lake",
        "strait",
        "peninsula",
        "gulf",
        "bay",
        "colony",
        "palace",
        "castle",
        "tower",
        "bridge",
        "airport",
        "harbor",
        "harbour",
        "province",
        "canton",
        "county",
        "district",
        "territory",
        "springs",
        "city",
        "creek",
        "basin",
        "desert",
        "plateau",
        "cape",
        "coast",
        "reef",
        "archipelago",
        "fjord",
        "steppe",
        "tundra",
        "savanna",
        "mesa",
        "gorge",
        "canyon",
        "ridge",
        "hills",
        "plains",
        "marsh",
        "swamp",
        "oasis",
        "inlet",
    ];
    if (current_type == "person" || current_type == "concept")
        && word_count >= 2
        && words.iter().any(|w| place_words.contains(w))
    {
        return Some("place");
    }

    // Place → person: names that look like "Firstname Lastname" classified as place
    // Heuristic: 2-3 words, all capitalized, no place/org indicators, no numbers
    if current_type == "place" && (word_count == 2 || word_count == 3) {
        let has_place_word = words.iter().any(|w| place_words.contains(w));
        let has_org_word = words.iter().any(|w| {
            [
                "university",
                "college",
                "institute",
                "company",
                "corp",
                "inc",
                "street",
                "avenue",
                "road",
                "turnpike",
                "post-intelligencer",
                "semantics",
                "modelling",
                "diploma",
            ]
            .contains(w)
        });
        let has_tech_word = words.iter().any(|w| {
            [
                "spark",
                "hadoop",
                "kafka",
                "kubernetes",
                "docker",
                "tensorflow",
                "pytorch",
                "bayes",
                "naive",
                "artificial",
                "neural",
                "quantum",
            ]
            .contains(w)
        });
        let has_digit = lower.chars().any(|c| c.is_ascii_digit());
        if !has_place_word && !has_org_word && !has_tech_word && !has_digit {
            // Check if words look like proper name components (capitalized, alphabetic)
            let all_name_like = words.iter().all(|w| {
                w.len() >= 2 && w.chars().all(|c| c.is_alphabetic() || c == '-' || c == '.')
            });
            // Person names don't usually contain these
            let non_person_words = [
                "soviet-allied",
                "off",
                "columbia",
                "cuban",
                "brazilian",
                "carolingian",
                "imperial",
                "byzantine",
            ];
            let has_non_person = words.iter().any(|w| non_person_words.contains(w));
            if all_name_like && !has_non_person && word_count <= 3 {
                // Additional check: is the last word a common surname-like word?
                // Avoid reclassifying "West Berlin" etc.
                let first_word = words[0];
                let last_word = words[word_count - 1];
                // If first word is a cardinal direction or common adjective, keep as place
                let adj_first = [
                    "new", "old", "great", "upper", "lower", "central", "western", "eastern",
                    "northern", "southern", "south", "north", "east", "west",
                ];
                if !adj_first.contains(&first_word) {
                    // Likely a person name misclassified as place
                    // But be conservative: only if last word >= 4 chars (likely surname)
                    if last_word.len() >= 4 {
                        return Some("person");
                    }
                }
            }
        }
        // Place → technology: known software/tech names
        if has_tech_word {
            return Some("technology");
        }
    }

    // Compound entities misclassified as person: "X Building", "X Day", "X Award", etc.
    if current_type == "person" && word_count >= 2 {
        if let Some(last) = words.last() {
            // These suffixes mean it's a place, not a person
            let place_suffixes = [
                "building",
                "center",
                "centre",
                "house",
                "suite",
                "hall",
                "park",
                "square",
                "street",
                "avenue",
                "boulevard",
                "museum",
                "library",
                "hospital",
                "station",
                "airport",
            ];
            if place_suffixes.contains(last) {
                return Some("place");
            }
            // These suffixes mean it's an organization
            let org_suffixes = [
                "institute",
                "foundation",
                "association",
                "society",
                "academy",
                "council",
                "committee",
                "commission",
                "agency",
                "bureau",
            ];
            if org_suffixes.contains(last) {
                return Some("organization");
            }
            // These suffixes mean it's an event
            let event_suffixes = ["day", "festival", "ceremony", "conference", "symposium"];
            if event_suffixes.contains(last) {
                return Some("event");
            }
            // These suffixes mean it's a concept/thing
            let concept_suffixes = [
                "award",
                "prize",
                "medal",
                "biography",
                "original",
                "wired",
                "founder",
                "countess",
                "notes",
                "letters",
                "papers",
                "numbers",
                "equations",
                "theorem",
                "theorems",
                "theory",
                "principle",
                "principles",
                "formula",
                "method",
                "methods",
                "law",
                "laws",
                "conjecture",
                "paradox",
                "sequence",
                "series",
                "constant",
                "identity",
                "lemma",
                "axiom",
                "calculus",
                "algebra",
                "geometry",
                "mechanics",
                "dynamics",
                "phenomenon",
                "classification",
                "hypothesis",
                "cycle",
                "process",
                "model",
                "system",
                "discovery",
                "discoveries",
                "invention",
                "inventions",
                "revolution",
                "movement",
                "crisis",
                "massacre",
                "rebellion",
                "revolt",
                "campaign",
                "conquest",
                "expedition",
                "voyage",
                "migration",
                "diaspora",
            ];
            if concept_suffixes.contains(last) {
                return Some("concept");
            }
            // Political entities ending with these are places/polities, not persons
            let polity_suffixes = [
                "empire",
                "kingdom",
                "republic",
                "dynasty",
                "caliphate",
                "sultanate",
                "khanate",
                "duchy",
                "principality",
                "confederation",
                "federation",
                "commonwealth",
            ];
            if polity_suffixes.contains(last) {
                return Some("place");
            }
            // Creative works / titles misclassified as person
            let work_indicators = [
                "bride",
                "saga",
                "chronicle",
                "chronicles",
                "manuscript",
                "codex",
                "testament",
                "gospel",
                "epic",
                "odyssey",
                "trilogy",
                "anthology",
            ];
            if work_indicators.contains(last) {
                return Some("concept");
            }
        }
    }

    // Technology-like names classified as organization
    let tech_words = [
        "network",
        "networks",
        "algorithm",
        "protocol",
        "framework",
        "neural",
        "processor",
        "architecture",
        "compiler",
        "runtime",
        "kernel",
        "driver",
    ];
    if current_type == "organization"
        && word_count >= 2
        && words.iter().any(|w| tech_words.contains(w))
    {
        return Some("technology");
    }

    // Concept-like names classified as person (multi-word abstractions)
    let concept_indicators = [
        "theory",
        "theorem",
        "principle",
        "effect",
        "law",
        "paradox",
        "hypothesis",
        "equation",
        "conjecture",
        "inequality",
        "transform",
        "function",
        "distribution",
        "constant",
        "number",
        "numbers",
        "formula",
        "equations",
        "theorems",
        "principles",
        "laws",
        "functions",
        "constants",
        "series",
        "sequence",
        "sequences",
        "identities",
        "identity",
        "lemma",
        "axiom",
        "axioms",
        "calculus",
        "algebra",
        "geometry",
        "mechanics",
        "dynamics",
        "thermodynamics",
        "relativity",
        "classification",
        "method",
        "methods",
        "analysis",
        "model",
        "system",
        "process",
        "cycle",
        "phenomenon",
    ];
    if current_type == "person"
        && word_count >= 2
        && words.iter().any(|w| concept_indicators.contains(w))
    {
        // But "Euler's theorem" should be concept, "Leonhard Euler" should stay person
        // Check: if the last word is a concept indicator, it's probably a concept
        if let Some(last) = words.last() {
            if concept_indicators.contains(last) {
                return Some("concept");
            }
        }
    }

    // GDP/statistics entities are concepts, not persons
    if current_type == "person"
        && (lower.contains("gdp") || lower.contains("population") || lower.contains("statistics"))
    {
        return Some("concept");
    }

    // Well-known misclassifications: geographic entities classified as person
    let known_places: &[&str] = &[
        "hong kong",
        "new york",
        "new zealand",
        "sri lanka",
        "el salvador",
        "middle east",
        "south america",
        "north america",
        "central asia",
        "south asia",
        "east asia",
        "southeast asia",
        "sub-saharan africa",
        "saharan africa",
        "latin america",
        "central europe",
        "western europe",
        "eastern europe",
        "northern europe",
        "southern europe",
        "spanish netherlands",
        "austrian netherlands",
        "dutch east indies",
        "french indochina",
        "british india",
        "british columbia",
        "british raj",
        "ottoman empire",
        "roman empire",
        "byzantine empire",
        "mughal empire",
        "emi koussi",
        "tibesti",
        "galápagos",
        "galapagos",
        "crimean peninsula",
        "iberian peninsula",
        "korean peninsula",
        "malay peninsula",
        "arabian peninsula",
        "cape of good hope",
        "cape horn",
        "tierra del fuego",
        "papua new guinea",
        "ivory coast",
        "costa rica",
        "puerto rico",
        "sierra leone",
        "burkina faso",
        "south sudan",
        "north korea",
        "south korea",
        "trinidad and tobago",
        "antigua and barbuda",
        "saint kitts and nevis",
        "czech republic",
        "dominican republic",
        "central african republic",
        "equatorial guinea",
        "timor-leste",
        "east timor",
        "west bank",
        "gaza strip",
        "middle kingdom",
        "mongol empire",
        "inca empire",
        "aztec empire",
        "holy land",
        "fertile crescent",
        "silk road",
        "great wall",
        "great barrier reef",
        "dead sea",
        "black sea",
        "red sea",
        "caspian sea",
        "aral sea",
        "baltic sea",
        "adriatic sea",
        "coral sea",
        "north sea",
        "irish sea",
        "machu picchu",
        "huayna picchu",
        "angkor wat",
        "mont blanc",
        "monte carlo",
    ];
    if current_type == "person" && known_places.contains(&lower) {
        return Some("place");
    }
    // Entities ending with geographic suffixes that are definitely places
    if current_type == "person" && word_count >= 2 {
        let geo_suffixes = [
            "springs",
            "city",
            "islands",
            "island",
            "mountains",
            "mountain",
            "valley",
            "river",
            "lake",
            "strait",
            "peninsula",
            "gulf",
            "bay",
            "creek",
            "basin",
            "desert",
            "plateau",
            "cape",
            "coast",
            "reef",
            "archipelago",
            "steppe",
            "canyon",
            "ridge",
            "hills",
            "plains",
            "falls",
            "pass",
            "harbor",
            "harbour",
            "port",
            "inlet",
            "fjord",
            "ocean",
            "oceans",
            "sea",
            "seas",
            "seamount",
            "oceania",
            "atoll",
            "canal",
            "straits",
            "dam",
            "reservoir",
            "lagoon",
            "cove",
            "glacier",
            "volcano",
            "crater",
            "delta",
            "marsh",
            "swamp",
            "oasis",
            "savannah",
            "prairie",
            "tundra",
            "wetlands",
            "range",
            "ranges",
            "highlands",
            "lowlands",
            "foothills",
            "headwaters",
            "estuary",
            "confluence",
            "waterfall",
        ];
        if let Some(last) = words.last() {
            if geo_suffixes.contains(last) {
                return Some("place");
            }
        }
        // Also catch geo features as ANY word (e.g., "Thai Canal Thailand")
        let strong_geo_words = [
            "canal",
            "strait",
            "straits",
            "volcano",
            "glacier",
            "reservoir",
            "archipelago",
            "peninsula",
            "fjord",
            "atoll",
            "lagoon",
        ];
        if words.iter().any(|w| strong_geo_words.contains(w)) {
            return Some("place");
        }
        // Geographic prefixes: "Stadt X", "Wadi X", "Monte X", "Rio X", "Sierra X", etc.
        let geo_prefixes = [
            "stadt", "wadi", "monte", "rio", "sierra", "cerro", "lago", "cabo", "isla", "punta",
            "bahia", "golfo", "valle", "selva", "col", "pic", "massif", "fort", "camp", "san",
            "santa", "santo", "bahr", "jebel", "jabal", "tel", "ain", "ras", "khor", "bir",
            "banteay", "wat", "kampong", "phnom",
        ];
        if words.len() >= 2 && geo_prefixes.contains(&words[0]) {
            return Some("place");
        }
    }

    // Country-adjective + geographic/political noun → place, not person
    // E.g., "Spanish Netherlands", "French Congo", "British Guiana"
    if current_type == "person" && word_count >= 2 {
        let country_adjectives = [
            "spanish",
            "french",
            "british",
            "dutch",
            "portuguese",
            "german",
            "italian",
            "russian",
            "chinese",
            "japanese",
            "ottoman",
            "persian",
            "roman",
            "byzantine",
            "austrian",
            "hungarian",
            "swedish",
            "danish",
            "norwegian",
            "finnish",
            "polish",
            "belgian",
            "swiss",
            "greek",
            "turkish",
            "egyptian",
            "indian",
            "thai",
            "korean",
            "vietnamese",
            "iraqi",
            "iranian",
            "saudi",
            "moroccan",
            "algerian",
            "tunisian",
            "libyan",
            "ethiopian",
            "somali",
            "colonial",
            "imperial",
            "soviet",
            "royal",
            "mongol",
            "inca",
            "aztec",
            "mayan",
            "maya",
            "mughal",
            "carolingian",
            "merovingian",
            "abbasid",
            "umayyad",
            "fatimid",
            "seljuk",
            "safavid",
            "qing",
            "ming",
            "tang",
            "song",
            "han",
            "tokugawa",
            "joseon",
            "khmer",
            "chola",
            "gupta",
            "maurya",
            "zulu",
            "maori",
            "viking",
            "norman",
            "saxon",
            "frankish",
            "visigoth",
            "vandal",
            "lombard",
        ];
        let geo_political_nouns = [
            "empire",
            "kingdom",
            "republic",
            "federation",
            "confederation",
            "union",
            "territory",
            "colony",
            "protectorate",
            "mandate",
            "dominion",
            "prefecture",
            "caliphate",
            "sultanate",
            "khanate",
            "duchy",
            "principality",
            "frontier",
            "lowlands",
            "highlands",
            "midlands",
            "heartland",
            "hinterland",
            "borderlands",
            "congo",
            "guinea",
            "guiana",
            "sahara",
            "indochina",
            "indies",
            "anatolia",
            "mesopotamia",
            "balkans",
            "caucasus",
            "scandinavia",
            "siberia",
            "manchuria",
            "patagonia",
            "amazonia",
            "polynesia",
            "melanesia",
            "micronesia",
            "levant",
            "maghreb",
            "raj",
            "columbia",
        ];
        if let Some(first) = words.first() {
            if country_adjectives.contains(first) {
                if let Some(last) = words.last() {
                    if geo_political_nouns.contains(last) {
                        return Some("place");
                    }
                }
                // Also catch any word in the name being geographic
                if words
                    .iter()
                    .skip(1)
                    .any(|w| geo_political_nouns.contains(w))
                {
                    return Some("place");
                }
            }
        }
    }

    // Well-known concepts misclassified as person
    let known_concepts: &[&str] = &[
        "big bang",
        "dark matter",
        "dark energy",
        "black hole",
        "quantum mechanics",
        "general relativity",
        "special relativity",
        "string theory",
        "machine learning",
        "artificial intelligence",
        "deep learning",
        "natural selection",
        "climate change",
        "global warming",
        "plate tectonics",
        "continental drift",
        "new scientist",
        "scientific american",
    ];
    if current_type == "person" && known_concepts.contains(&lower) {
        return Some("concept");
    }

    // Organizations misclassified as person
    let known_orgs: &[&str] = &["new scientist", "scientific american", "nature"];
    if current_type == "person" && known_orgs.contains(&lower) {
        return Some("organization");
    }

    // "State X" patterns misclassified as person (e.g. "State Anthem", "Crusader States")
    // But keep "State Henry Kissinger" etc. — those are "Secretary of State + name" fragments
    if current_type == "person" && word_count >= 2 {
        let state_concept_suffixes = [
            "anthem",
            "church",
            "churches",
            "estate",
            "estates",
            "states",
            "clause",
            "sicilies",
            "council",
            "militia",
            "department",
        ];
        if let Some(last) = words.last() {
            if state_concept_suffixes.contains(last) {
                return Some("concept");
            }
        }
        // "X Church" → organization
        let org_last = ["church", "churches", "orthodox", "catholic"];
        if let Some(last) = words.last() {
            if org_last.contains(last) {
                return Some("organization");
            }
        }
        // "Federated States", "Crusader States", "First/Second/Third Estate"
        if let Some(first) = words.first() {
            let ordinal_first = ["first", "second", "third", "fourth", "fifth"];
            if ordinal_first.contains(first) {
                let concept_second = ["estate", "estates", "republic", "empire", "crusade"];
                if word_count == 2 && concept_second.contains(&words[1]) {
                    return Some("concept");
                }
            }
        }
    }

    // Single-word entities that are obviously not persons
    if current_type == "person" && word_count == 1 {
        let concept_singles = [
            "philosophy",
            "matrix",
            "supersymmetries",
            "supermembranes",
            "microcontrollers",
            "fundamentals",
            "correspondents",
        ];
        if concept_singles.contains(&lower) {
            return Some("concept");
        }
        // Civilization / ethnic group names misclassified as person
        let civilization_names = [
            "aztec",
            "maya",
            "mayan",
            "inca",
            "viking",
            "norse",
            "celtic",
            "gaul",
            "gaulish",
            "visigoth",
            "vandal",
            "lombard",
            "saxon",
            "frankish",
            "norman",
            "berber",
            "bedouin",
            "phoenician",
            "sumerian",
            "akkadian",
            "assyrian",
            "babylonian",
            "hittite",
            "minoan",
            "mycenaean",
            "etruscan",
            "thracian",
            "scythian",
            "sarmatian",
            "hun",
            "goth",
            "ostrogoth",
            "cossack",
            "tatar",
            "mongol",
            "khmer",
            "maori",
            "zulu",
            "bantu",
            "igbo",
            "yoruba",
            "hausa",
            "swahili",
            "ashanti",
            "fulani",
            "tuareg",
            "inuit",
            "navajo",
            "sioux",
            "apache",
            "comanche",
            "iroquois",
            "cherokee",
            "pueblo",
            "olmec",
            "toltec",
            "zapotec",
            "mixtec",
            "arawak",
            "carib",
            "guarani",
            "quechua",
            "aymara",
            "aboriginal",
            "polynesian",
            "melanesian",
            "micronesian",
            "samurai",
            "shogun",
            "mughal",
            "rajput",
            "sikh",
            "dravidian",
            "aryan",
            "vedic",
            "hellenistic",
            "carolingian",
            "merovingian",
            "abbasid",
            "umayyad",
            "fatimid",
            "seljuk",
            "safavid",
            "ottoman",
            "byzantine",
            "spartan",
            "athenian",
            "roman",
            "persian",
            "parthian",
            "sasanian",
            "sassanid",
        ];
        if civilization_names.contains(&lower) {
            return Some("concept");
        }
    }

    // "Middle X" geographic regions misclassified as person (e.g. "Middle Niger", "Middle East")
    if current_type == "person" && word_count >= 2 {
        let geo_prefixes = [
            "middle", "upper", "lower", "inner", "outer", "greater", "lesser",
        ];
        if let Some(first) = words.first() {
            if geo_prefixes.contains(first) {
                return Some("place");
            }
        }
    }

    // Well-known countries/regions misclassified as concept or person
    let known_countries: &[&str] = &[
        "netherlands",
        "germany",
        "france",
        "spain",
        "portugal",
        "italy",
        "greece",
        "turkey",
        "egypt",
        "india",
        "china",
        "japan",
        "korea",
        "russia",
        "brazil",
        "mexico",
        "canada",
        "australia",
        "argentina",
        "chile",
        "peru",
        "colombia",
        "venezuela",
        "cuba",
        "iran",
        "iraq",
        "syria",
        "afghanistan",
        "pakistan",
        "bangladesh",
        "myanmar",
        "thailand",
        "vietnam",
        "indonesia",
        "malaysia",
        "singapore",
        "taiwan",
        "mongolia",
        "nepal",
        "cambodia",
        "laos",
        "austria",
        "switzerland",
        "belgium",
        "poland",
        "czechia",
        "hungary",
        "romania",
        "bulgaria",
        "serbia",
        "croatia",
        "ukraine",
        "belarus",
        "lithuania",
        "latvia",
        "estonia",
        "finland",
        "sweden",
        "norway",
        "denmark",
        "iceland",
        "ireland",
        "scotland",
        "wales",
        "england",
        "morocco",
        "algeria",
        "tunisia",
        "libya",
        "sudan",
        "ethiopia",
        "kenya",
        "tanzania",
        "uganda",
        "nigeria",
        "ghana",
        "senegal",
        "cameroon",
        "madagascar",
        "mozambique",
        "zimbabwe",
        "zambia",
        "angola",
        "namibia",
        "botswana",
        "somalia",
        "eritrea",
        "djibouti",
        "gabon",
        "congo",
        "shropshire",
        "bradford",
        "clermont",
        "esztergom",
        "lombardy",
        "saxony",
        "bavaria",
        "bohemia",
        "moravia",
        "silesia",
        "alsace",
        "catalonia",
        "andalusia",
        "galicia",
        "brittany",
        "normandy",
        "flanders",
        "wallonia",
        "tyrol",
        "transylvania",
        "thrace",
        "mesopotamia",
        "anatolia",
        "persia",
        "judea",
        "canaan",
        "baltic",
        "siberia",
        "sahara",
        "patagonia",
        "scandinavia",
        "balkans",
        "caucasus",
        "crimea",
        "cyprus",
        "crete",
        "sardinia",
        "sicily",
        "corsica",
        "sumatra",
        "borneo",
        "java",
        "ceylon",
        "formosa",
        "kashmir",
        "tibet",
        "manchuria",
        "xinjiang",
    ];
    if (current_type == "concept" || current_type == "person")
        && word_count == 1
        && known_countries.contains(&lower)
    {
        return Some("place");
    }

    // Multi-word place names misclassified as concept or person
    // Generic suffix-based detection for empires, kingdoms, republics, etc.
    if (current_type == "concept" || current_type == "person") && word_count >= 2 {
        let geopolitical_suffixes: &[&str] = &[
            "empire",
            "kingdom",
            "republic",
            "dynasty",
            "caliphate",
            "khanate",
            "sultanate",
            "principality",
            "confederation",
            "netherlands",
            "union",
        ];
        if let Some(last) = words.last() {
            if geopolitical_suffixes.contains(last) {
                // Exception: "Soviet Union" is place, "Trade Union" is organization
                let org_prefixes = ["trade", "labor", "labour", "workers", "european"];
                if *last == "union" && org_prefixes.iter().any(|p| lower.starts_with(p)) {
                    return Some("organization");
                }
                return Some("place");
            }
        }
        let place_compounds: &[&str] = &[
            "soviet union",
            "west germany",
            "east germany",
            "west berlin",
            "east berlin",
            "north korea",
            "south korea",
            "north vietnam",
            "south vietnam",
            "saudi arabia",
            "south africa",
            "costa rica",
            "puerto rico",
            "new guinea",
            "new caledonia",
            "new hebrides",
            "rhine valley",
            "nile delta",
            "ganges plain",
            "fertile crescent",
        ];
        if place_compounds.contains(&lower) {
            return Some("place");
        }
    }

    None
}

/// Heuristic: is this likely a real, notable entity worth keeping even if isolated?
fn is_likely_real_entity(name: &str, entity_type: &str) -> bool {
    let lower = name.to_lowercase();
    let words: Vec<&str> = lower.split_whitespace().collect();
    let word_count = words.len();

    // Already filtered by noise checks
    if is_noise_name(name) {
        return false;
    }

    // Person names: 2-3 words, each capitalized, no noise words
    if entity_type == "person" && (word_count == 2 || word_count == 3) {
        let all_cap = name
            .split_whitespace()
            .all(|w| w.starts_with(|c: char| c.is_uppercase()));
        if all_cap {
            return true;
        }
    }

    // Well-known organization patterns
    if entity_type == "organization" && word_count <= 4 {
        let org_suffixes = [
            "inc",
            "corp",
            "ltd",
            "gmbh",
            "ag",
            "llc",
            "foundation",
            "institute",
            "university",
            "college",
            "party",
            "association",
        ];
        if let Some(last) = words.last() {
            if org_suffixes.iter().any(|s| last.ends_with(s)) {
                return true;
            }
        }
    }

    // Places: 1-3 words, capitalized
    if entity_type == "place" && word_count <= 3 {
        let all_cap = name
            .split_whitespace()
            .all(|w| w.starts_with(|c: char| c.is_uppercase()));
        if all_cap {
            return true;
        }
    }

    // Single well-capitalized words that are proper nouns
    if word_count == 1 && name.starts_with(|c: char| c.is_uppercase()) && name.len() >= 4 {
        // Filter out common English words that sneak through as entities
        // These are NOT proper nouns even when capitalized
        if is_common_english_word(&lower) {
            return false;
        }
        // Words ending in common verb/adjective suffixes are likely not proper nouns
        if lower.ends_with("ing")
            || lower.ends_with("tion")
            || lower.ends_with("ment")
            || lower.ends_with("ness")
            || lower.ends_with("ists")
            || lower.ends_with("isms")
            || lower.ends_with("ally")
            || lower.ends_with("edly")
            || lower.ends_with("ious")
            || lower.ends_with("eous")
            || lower.ends_with("ible")
            || lower.ends_with("able")
        {
            // But allow known proper nouns with these endings (e.g. "Beijing", "Turing")
            let known_exceptions = [
                "beijing",
                "turing",
                "reading",
                "stirling",
                "darjeeling",
                "nanjing",
                "chongqing",
                "washington",
                "wellington",
                "nottingham",
                "birmingham",
                "buckingham",
                "manning",
                "browning",
                "kipling",
                "lessing",
                "göttingen",
            ];
            if !known_exceptions.contains(&lower.as_str()) {
                return false;
            }
        }
        return true;
    }

    false
}

/// Common English words that are NOT proper nouns even when capitalized.
/// Used to filter island entities that are clearly generic terms.
fn is_common_english_word(lower: &str) -> bool {
    const COMMON_WORDS: &[&str] = &[
        // Verbs / verb forms
        "defeat",
        "subtract",
        "encode",
        "assuming",
        "conclude",
        "declare",
        "emerge",
        "evolve",
        "expand",
        "explore",
        "impose",
        "improve",
        "indicate",
        "interpret",
        "introduce",
        "invoke",
        "lapse",
        "mandate",
        "negotiate",
        "observe",
        "oppose",
        "organize",
        "overcome",
        "persist",
        "pledge",
        "possess",
        "precede",
        "preserve",
        "prevail",
        "proceed",
        "produce",
        "propose",
        "pursue",
        "reckon",
        "reform",
        "reign",
        "resolve",
        "restore",
        "retain",
        "retrieve",
        "settle",
        "stimulate",
        "succeed",
        "suppress",
        "sustain",
        "transform",
        "undermine",
        "withdrew",
        "yield",
        "abolish",
        "accomplish",
        // Nouns (generic)
        "beings",
        "championships",
        "director",
        "formula",
        "integers",
        "passages",
        "defeats",
        "pledges",
        "marshalls",
        "theorists",
        "adventists",
        "caliphs",
        "streifzüge",
        "bilanz",
        // Adjectives / adverbs
        "postwar",
        "naively",
        "paleolithic",
        "prehistoric",
        "medieval",
        "contemporary",
        "predominantly",
        "approximately",
        "consequently",
        "furthermore",
        "nevertheless",
        "subsequently",
        "alternatively",
        // German/French noise
        "jahren",
        "während",
        "zwischen",
        "bereits",
        "allerdings",
        "bibliothèque",
        "musique",
        "conseil",
        // Common nouns that appear as capitalized island entities
        "structure",
        "formation",
        "electric",
        "graph",
        "problem",
        "changes",
        "names",
        "upper",
        "studies",
        "addresses",
        "accuracy",
        "acoustics",
        "adulthood",
        "advancements",
        "afterwards",
        "lab",
        "bay",
        "base",
        "stem",
        "pass",
        "quart",
        "manus",
        "court",
        "office",
        "sea",
        "matter",
        "engine",
        "climate",
        "difference",
        "federal",
        "church",
        "states",
        "catholic",
        "reformed",
        "american",
        "scientific",
        "monthly",
        "notices",
        "colloquium",
        "academic",
        "accepted",
        "adiabatic",
        "chapters",
        "principles",
        "applications",
        "communications",
        "mechanics",
        "dynamics",
        "thermodynamics",
        "optics",
        "physics",
        "chemistry",
        "biology",
        "geometry",
        "algebra",
        "calculus",
        "statistics",
        "probability",
        "topology",
        "engineering",
        "architecture",
        "philosophy",
        "psychology",
        "sociology",
        "anthropology",
        "economics",
        "politics",
        "diplomacy",
        "agriculture",
        "industry",
        "commerce",
        "infrastructure",
        "parliament",
        "congress",
        "senate",
        "democracy",
        "monarchy",
        "aristocracy",
        "bureaucracy",
        "independence",
        "sovereignty",
        "territory",
        "population",
        "immigration",
        "emigration",
        "colonization",
        "modernization",
        "industrialization",
        "urbanization",
        "globalization",
        "reformation",
        "enlightenment",
        "renaissance",
        "conquest",
        "invasion",
        "rebellion",
        "uprising",
        "coup",
        "siege",
        "battle",
        "campaign",
        "alliance",
        "treaty",
        "armistice",
        "ceasefire",
        "occupation",
        "liberation",
        "resistance",
        "propaganda",
        "censorship",
        "persecution",
        "genocide",
        "massacre",
        "famine",
        "plague",
        "epidemic",
        "pandemic",
        "drought",
        "flood",
        "earthquake",
        "volcano",
        "tsunami",
        "catastrophe",
        "disaster",
        "crisis",
        "recession",
        "depression",
        "inflation",
        "prosperity",
    ];
    if COMMON_WORDS.contains(&lower) {
        return true;
    }
    // Additional noise: English words that appear as isolated concept entities
    const EXTRA_NOISE: &[&str] = &[
        "coincidence",
        "performance",
        "intermediate",
        "scarlet",
        "publisher",
        "footnote",
        "overwork",
        "welcome",
        "fundamentals",
        "struggle",
        "telegraph",
        "precision",
        "regime",
        "ancient",
        "formaliser",
        "radixsort",
        "multi-pivot",
        "co-developer",
        "prerequisite",
        "supplement",
        "appendix",
        "bibliography",
        "compilation",
        "commentary",
        "correspondence",
        "equivalent",
        "establishment",
        "expedition",
        "interpretation",
        "introduction",
        "investigation",
        "laboratory",
        "manuscript",
        "observation",
        "publication",
        "proceedings",
        "references",
        "reproduction",
        "supplement",
        "translation",
        "collection",
        "contribution",
        "description",
        "development",
        "discovery",
        "distribution",
        "examination",
        "experiment",
        "expression",
        "generation",
        "illustration",
        "instrument",
        "measurement",
        "mechanism",
        "phenomenon",
        "preparation",
        "production",
        "projection",
        "proposition",
        "recognition",
        "regulation",
        "representation",
        "resolution",
        "specification",
        "transformation",
        "verification",
        "hypothesis",
        "suggestion",
        "conclusion",
        "consideration",
        "demonstration",
        "distinction",
        "explanation",
        "identification",
        "implementation",
        "implication",
        "modification",
        "notification",
        "organization",
        "participation",
        "recommendation",
        "registration",
        "celebration",
        "classification",
        "communication",
        "determination",
        "documentation",
        "acknowledgement",
        "abbreviation",
        "acceleration",
        "accommodation",
    ];
    EXTRA_NOISE.contains(&lower)
}

/// Normalize common abbreviations in entity names for better fuzzy matching.
/// Maps: St→Saint, Mt→Mount, Ft→Fort, Dr→Doctor, Prof→Professor, etc.
/// Strip diacritics/accents from a string for fuzzy matching.
/// Maps common accented characters to their ASCII equivalents.
fn strip_diacritics(s: &str) -> String {
    s.chars()
        .map(|c| match c {
            'á' | 'à' | 'â' | 'ä' | 'ã' | 'å' | 'ą' => 'a',
            'Á' | 'À' | 'Â' | 'Ä' | 'Ã' | 'Å' | 'Ą' => 'A',
            'ć' | 'č' | 'ç' => 'c',
            'Ć' | 'Č' | 'Ç' => 'C',
            'ð' | 'đ' => 'd',
            'Ð' | 'Đ' => 'D',
            'é' | 'è' | 'ê' | 'ë' | 'ę' | 'ě' => 'e',
            'É' | 'È' | 'Ê' | 'Ë' | 'Ę' | 'Ě' => 'E',
            'ğ' => 'g',
            'Ğ' => 'G',
            'í' | 'ì' | 'î' | 'ï' | 'ı' => 'i',
            'Í' | 'Ì' | 'Î' | 'Ï' | 'İ' => 'I',
            'ł' => 'l',
            'Ł' => 'L',
            'ñ' | 'ń' | 'ň' => 'n',
            'Ñ' | 'Ń' | 'Ň' => 'N',
            'ó' | 'ò' | 'ô' | 'ö' | 'õ' | 'ø' | 'ő' => 'o',
            'Ó' | 'Ò' | 'Ô' | 'Ö' | 'Õ' | 'Ø' | 'Ő' => 'O',
            'ř' => 'r',
            'Ř' => 'R',
            'ś' | 'š' | 'ş' => 's',
            'Ś' | 'Š' | 'Ş' => 'S',
            'ß' => 's',
            'ť' => 't',
            'Ť' => 'T',
            'ú' | 'ù' | 'û' | 'ü' | 'ů' | 'ű' => 'u',
            'Ú' | 'Ù' | 'Û' | 'Ü' | 'Ů' | 'Ű' => 'U',
            'ý' | 'ÿ' => 'y',
            'Ý' | 'Ÿ' => 'Y',
            'ź' | 'ž' | 'ż' => 'z',
            'Ź' | 'Ž' | 'Ż' => 'Z',
            'æ' => 'a', // simplified
            'Æ' => 'A',
            'œ' => 'o', // simplified
            'Œ' => 'O',
            'þ' => 't',
            'Þ' => 'T',
            _ => c,
        })
        .collect()
}

fn normalize_abbreviations(name: &str) -> String {
    let mappings: &[(&str, &str)] = &[
        ("St ", "Saint "),
        ("St. ", "Saint "),
        ("Mt ", "Mount "),
        ("Mt. ", "Mount "),
        ("Ft ", "Fort "),
        ("Ft. ", "Fort "),
        ("Dr ", "Doctor "),
        ("Dr. ", "Doctor "),
        ("Prof ", "Professor "),
        ("Prof. ", "Professor "),
        ("Univ ", "University "),
        ("Univ. ", "University "),
        ("Inst ", "Institute "),
        ("Inst. ", "Institute "),
    ];
    let mut result = name.to_string();
    for (abbr, full) in mappings {
        // Match at start of name or after space
        if result.starts_with(abbr) {
            result = format!("{}{}", full, &result[abbr.len()..]);
        }
        // Match after space
        let space_abbr = format!(" {}", abbr);
        let space_full = format!(" {}", full);
        while result.contains(&space_abbr) {
            result = result.replacen(&space_abbr, &space_full, 1);
        }
    }
    result
}

fn now_str() -> String {
    Utc::now()
        .naive_utc()
        .format("%Y-%m-%d %H:%M:%S")
        .to_string()
}

fn parse_hypothesis_row(row: &rusqlite::Row) -> Hypothesis {
    let ef: String = row.get::<_, String>(5).unwrap_or_default();
    let ea: String = row.get::<_, String>(6).unwrap_or_default();
    let rc: String = row.get::<_, String>(7).unwrap_or_default();
    Hypothesis {
        id: row.get(0).unwrap_or(0),
        subject: row.get(1).unwrap_or_default(),
        predicate: row.get(2).unwrap_or_default(),
        object: row.get(3).unwrap_or_default(),
        confidence: row.get(4).unwrap_or(0.5),
        evidence_for: serde_json::from_str(&ef).unwrap_or_default(),
        evidence_against: serde_json::from_str(&ea).unwrap_or_default(),
        reasoning_chain: serde_json::from_str(&rc).unwrap_or_default(),
        status: HypothesisStatus::from_str(&row.get::<_, String>(8).unwrap_or_default()),
        discovered_at: row.get(9).unwrap_or_default(),
        pattern_source: row.get(10).unwrap_or_default(),
    }
}

/// Extract domain from a URL (best-effort, no external crate needed).
fn extract_domain(url: &str) -> String {
    let stripped = url
        .strip_prefix("https://")
        .or_else(|| url.strip_prefix("http://"))
        .unwrap_or(url);
    stripped
        .split('/')
        .next()
        .unwrap_or(stripped)
        .to_lowercase()
}

fn is_contradicting_predicate(a: &str, b: &str) -> bool {
    let pairs = [
        ("is", "is_not"),
        ("has", "lacks"),
        ("contains", "excludes"),
        ("member_of", "not_member_of"),
        ("created_by", "not_created_by"),
    ];
    for (p, q) in &pairs {
        if (a == *p && b == *q) || (a == *q && b == *p) {
            return true;
        }
    }
    false
}

/// Check if an entity-type pair is incompatible with a given predicate.
/// Returns true when the triple (subject_type, object_type, predicate) is semantically
/// nonsensical — e.g. a place cannot be "contemporary_of" a concept.
fn is_type_incompatible(subject_type: &str, object_type: &str, predicate: &str) -> bool {
    // Predicates that require both entities to be the same broad category
    let same_type_predicates = [
        "contemporary_of",
        "partner_of",
        "collaborated_with",
        "rival_of",
        "successor_of",
        "predecessor_of",
    ];
    if same_type_predicates.contains(&predicate) {
        // contemporary_of etc. only makes sense between entities of similar kinds
        let compatible_groups: &[&[&str]] = &[
            &["person"],
            &["place", "organization"],
            &["concept", "technology"],
        ];
        let s_group = compatible_groups
            .iter()
            .position(|g| g.contains(&subject_type));
        let o_group = compatible_groups
            .iter()
            .position(|g| g.contains(&object_type));
        if let (Some(sg), Some(og)) = (s_group, o_group) {
            if sg != og {
                return true;
            }
        }
    }

    // located_near requires at least one place
    if predicate == "located_near" {
        if subject_type != "place" && object_type != "place" {
            // concept-concept or person-person for located_near is fine if one is a place
            if subject_type == "concept" || object_type == "concept" {
                return true;
            }
        }
    }

    // affiliated_with requires person→organization or person→concept
    if predicate == "affiliated_with" && subject_type == "place" {
        return true;
    }

    // pioneered requires person or organization as subject, and concept/technology/event as object
    if predicate == "pioneered" {
        if !["person", "organization"].contains(&subject_type) {
            return true;
        }
        if !["concept", "technology", "event"].contains(&object_type) {
            return true;
        }
    }

    // pioneered_by is the inverse: concept/technology → person/organization
    if predicate == "pioneered_by" {
        if !["concept", "technology", "event"].contains(&subject_type) {
            return true;
        }
        if !["person", "organization"].contains(&object_type) {
            return true;
        }
    }

    false
}

/// Check whether a predicate makes semantic sense given the entity types of
/// subject and object.  Returns `false` for combinations that are almost
/// certainly NLP/structural noise (e.g. `contemporary_of` between a person
/// and a concept, or `partner_of` between a book and an organisation).
fn predicate_type_compatible(predicate: &str, subj_type: &str, obj_type: &str) -> bool {
    // Predicates that require both entities to be of the same broad category
    const SAME_TYPE_PREDICATES: &[&str] = &[
        "contemporary_of",
        "rival_of",
        "sibling_of",
        "spouse_of",
        "colleague_of",
        "ally_of",
        "opponent_of",
        "predecessor_of",
        "successor_of",
    ];

    // Predicates that require at least one entity to be a person
    const PERSON_REQUIRED: &[&str] = &[
        "born_in",
        "died_in",
        "studied_at",
        "worked_at",
        "educated_at",
        "served_in",
        "married",
    ];

    // Predicates that make no sense between concepts/works and organisations
    const ORG_CONCEPT_BLOCKLIST: &[&str] = &["partner_of", "affiliated_with", "has_member"];

    let st = subj_type.to_lowercase();
    let ot = obj_type.to_lowercase();

    // Normalise to broad categories
    let broad = |t: &str| -> &str {
        match t {
            t if t.contains("person")
                || t == "scientist"
                || t == "leader"
                || t == "philosopher"
                || t == "author"
                || t == "artist"
                || t == "ruler"
                || t == "military"
                || t == "politician"
                || t == "explorer"
                || t == "mathematician"
                || t == "historian" =>
            {
                "person"
            }
            t if t.contains("org")
                || t.contains("institution")
                || t.contains("company")
                || t.contains("university")
                || t.contains("school")
                || t.contains("church")
                || t.contains("empire") =>
            {
                "org"
            }
            t if t.contains("place")
                || t.contains("location")
                || t.contains("city")
                || t.contains("country")
                || t.contains("region") =>
            {
                "place"
            }
            t if t.contains("concept")
                || t.contains("theory")
                || t.contains("field")
                || t.contains("event")
                || t.contains("work")
                || t.contains("book")
                || t.contains("journal")
                || t.contains("publication") =>
            {
                "concept"
            }
            _ => "unknown",
        }
    };

    let sb = broad(&st);
    let ob = broad(&ot);

    // If either type is unknown, allow (can't validate)
    if sb == "unknown" || ob == "unknown" {
        return true;
    }

    // Same-type predicates: reject if broad categories differ
    if SAME_TYPE_PREDICATES.contains(&predicate) && sb != ob {
        return false;
    }

    // Person-required predicates: reject if neither entity is a person
    if PERSON_REQUIRED.contains(&predicate) && sb != "person" && ob != "person" {
        return false;
    }

    // Block nonsensical concept-org pairings
    if ORG_CONCEPT_BLOCKLIST.contains(&predicate) {
        if (sb == "concept" && ob == "org") || (sb == "org" && ob == "concept") {
            return false;
        }
        if sb == "concept" && ob == "concept" {
            return false;
        }
    }

    true
}

fn predicates_similar(a: &str, b: &str) -> bool {
    if a == b {
        return true;
    }
    let synonyms = [
        &[
            "created",
            "created_by",
            "made",
            "built",
            "developed",
            "invented",
            "designed",
            "authored",
        ][..],
        &["is", "is_a", "type_of", "instance_of"],
        &["has", "contains", "includes", "possesses"],
        &["part_of", "belongs_to", "member_of", "component_of"],
        &["located_in", "based_in", "in", "situated_in", "resides_in"],
        &[
            "related_to",
            "associated_with",
            "connected_to",
            "knows",
            "linked_to",
        ],
        &["born_in", "birthplace", "native_of", "origin"],
        &["died_in", "death_place", "buried_in"],
        &["studied_at", "educated_at", "attended", "alumnus_of"],
        &["worked_at", "employed_by", "affiliated_with", "served_at"],
        &["influenced", "inspired", "influenced_by", "inspired_by"],
        &[
            "preceded",
            "preceded_by",
            "succeeded",
            "succeeded_by",
            "followed_by",
        ],
        &["discovered", "discovered_by", "found", "found_by"],
        &["named_after", "eponym_of", "namesake"],
        &["contemporary_of", "peer_of", "co_practitioner"],
        &["capital_of", "capital", "seat_of"],
        &["founded", "founded_by", "established", "established_by"],
    ];
    for group in &synonyms {
        if group.contains(&a) && group.contains(&b) {
            return true;
        }
    }
    false
}

// ---------------------------------------------------------------------------
// Tests
// ---------------------------------------------------------------------------

#[cfg(test)]
mod tests {
    use super::*;
    use crate::db::Brain;

    fn test_brain() -> Brain {
        Brain::open_in_memory().unwrap()
    }

    fn setup_graph() -> Brain {
        let brain = test_brain();
        let a = brain.upsert_entity("Alice", "person").unwrap();
        let b = brain.upsert_entity("Bob", "person").unwrap();
        let c = brain.upsert_entity("Charlie", "person").unwrap();
        let d = brain.upsert_entity("Diana", "person").unwrap();
        brain.upsert_relation(a, "knows", b, "test").unwrap();
        brain.upsert_relation(a, "knows", c, "test").unwrap();
        brain.upsert_relation(b, "knows", d, "test").unwrap();
        brain.upsert_relation(c, "knows", d, "test").unwrap();
        brain
    }

    #[test]
    fn test_init_schema() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        // Should be able to list hypotheses on empty DB
        let hyps = p.list_hypotheses(None).unwrap();
        assert!(hyps.is_empty());
    }

    #[test]
    fn test_structural_holes() {
        let brain = setup_graph();
        let p = Prometheus::new(&brain).unwrap();
        let holes = p.find_structural_holes().unwrap();
        // B and C both connect to A and D but not to each other
        assert!(
            holes
                .iter()
                .any(|(a, b)| { (a == "Bob" && b == "Charlie") || (a == "Charlie" && b == "Bob") }),
            "Expected B-C hole, got: {:?}",
            holes
        );
    }

    #[test]
    fn test_co_occurrences() {
        let brain = setup_graph();
        let p = Prometheus::new(&brain).unwrap();
        let patterns = p.find_co_occurrences(1).unwrap();
        assert!(!patterns.is_empty());
    }

    #[test]
    fn test_frequent_subgraphs() {
        let brain = setup_graph();
        let p = Prometheus::new(&brain).unwrap();
        let patterns = p.find_frequent_subgraphs(1).unwrap();
        // Should find "knows, knows" motif at least twice (Alice and Diana both have 2 knows edges... wait, only outgoing)
        // Actually Alice has outgoing: knows->Bob, knows->Charlie. That's 1 entity with 2 outgoing of same pred.
        assert!(
            patterns
                .iter()
                .any(|p| p.pattern_type == PatternType::FrequentSubgraph),
            "patterns: {:?}",
            patterns
        );
    }

    #[test]
    fn test_type_gaps() {
        let brain = test_brain();
        // Create 4 persons, 3 of which have "works_at"
        let a = brain.upsert_entity("P1", "person").unwrap();
        let b = brain.upsert_entity("P2", "person").unwrap();
        let c = brain.upsert_entity("P3", "person").unwrap();
        let _d = brain.upsert_entity("P4", "person").unwrap();
        let co = brain.upsert_entity("Corp", "company").unwrap();
        brain.upsert_relation(a, "works_at", co, "test").unwrap();
        brain.upsert_relation(b, "works_at", co, "test").unwrap();
        brain.upsert_relation(c, "works_at", co, "test").unwrap();
        let p = Prometheus::new(&brain).unwrap();
        let gaps = p.find_type_gaps().unwrap();
        assert!(
            gaps.iter()
                .any(|(e, pred, _)| e == "P4" && pred == "works_at"),
            "Expected P4 missing works_at, got: {:?}",
            gaps
        );
    }

    #[test]
    fn test_anomalies() {
        let brain = test_brain();
        let a = brain.upsert_entity("X1", "widget").unwrap();
        let b = brain.upsert_entity("X2", "widget").unwrap();
        let c = brain.upsert_entity("X3", "widget").unwrap();
        let t = brain.upsert_entity("Target", "thing").unwrap();
        brain.upsert_relation(a, "has_feature", t, "test").unwrap();
        brain.upsert_relation(b, "has_feature", t, "test").unwrap();
        // X3 does NOT have has_feature
        let p = Prometheus::new(&brain).unwrap();
        let anomalies = p.find_anomalies().unwrap();
        assert!(
            anomalies
                .iter()
                .any(|p| p.entities_involved.contains(&"X3".to_string())),
            "anomalies: {:?}",
            anomalies
        );
    }

    #[test]
    fn test_generate_hypotheses_from_holes() {
        let brain = setup_graph();
        let p = Prometheus::new(&brain).unwrap();
        let hyps = p.generate_hypotheses_from_holes().unwrap();
        assert!(!hyps.is_empty());
        assert!(hyps.iter().any(|h| h.pattern_source == "structural_hole"));
    }

    #[test]
    fn test_generate_hypotheses_from_type_gaps() {
        let brain = test_brain();
        let a = brain.upsert_entity("Cat Alpha", "animal").unwrap();
        let b = brain.upsert_entity("Dog Beta", "animal").unwrap();
        let c = brain.upsert_entity("Fox Gamma", "animal").unwrap();
        let _d = brain.upsert_entity("Elk Delta", "animal").unwrap();
        let f = brain.upsert_entity("Grass Food", "thing").unwrap();
        brain.upsert_relation(a, "eats", f, "test").unwrap();
        brain.upsert_relation(b, "eats", f, "test").unwrap();
        brain.upsert_relation(c, "eats", f, "test").unwrap();
        let p = Prometheus::new(&brain).unwrap();
        let hyps = p.generate_hypotheses_from_type_gaps().unwrap();
        assert!(
            hyps.iter().any(|h| h.subject == "Elk Delta"
                && h.predicate == "eats"
                && h.object == "Grass Food"),
            "hyps: {:?}",
            hyps.iter()
                .map(|h| format!("{} {} {}", h.subject, h.predicate, h.object))
                .collect::<Vec<_>>()
        );
    }

    #[test]
    fn test_check_contradiction_none() {
        let brain = setup_graph();
        let p = Prometheus::new(&brain).unwrap();
        let h = Hypothesis {
            id: 0,
            subject: "Alice".into(),
            predicate: "knows".into(),
            object: "Diana".into(),
            confidence: 0.5,
            evidence_for: vec![],
            evidence_against: vec![],
            reasoning_chain: vec![],
            status: HypothesisStatus::Proposed,
            discovered_at: now_str(),
            pattern_source: "test".into(),
        };
        assert!(!p.check_contradiction(&h).unwrap());
    }

    #[test]
    fn test_score_hypothesis() {
        let brain = setup_graph();
        let p = Prometheus::new(&brain).unwrap();
        let mut h = Hypothesis {
            id: 0,
            subject: "Bob".into(),
            predicate: "related_to".into(),
            object: "Charlie".into(),
            confidence: 0.5,
            evidence_for: vec!["shared neighbours".into()],
            evidence_against: vec![],
            reasoning_chain: vec![],
            status: HypothesisStatus::Proposed,
            discovered_at: now_str(),
            pattern_source: "structural_hole".into(),
        };
        let score = p.score_hypothesis(&mut h).unwrap();
        assert!(score > 0.0 && score <= 1.0);
    }

    #[test]
    fn test_save_and_get_hypothesis() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        let h = Hypothesis {
            id: 0,
            subject: "X".into(),
            predicate: "is".into(),
            object: "Y".into(),
            confidence: 0.7,
            evidence_for: vec!["reason 1".into()],
            evidence_against: vec![],
            reasoning_chain: vec!["step 1".into(), "step 2".into()],
            status: HypothesisStatus::Proposed,
            discovered_at: now_str(),
            pattern_source: "test".into(),
        };
        let id = p.save_hypothesis(&h).unwrap();
        let loaded = p.get_hypothesis(id).unwrap().unwrap();
        assert_eq!(loaded.subject, "X");
        assert_eq!(loaded.predicate, "is");
        assert_eq!(loaded.object, "Y");
        assert_eq!(loaded.reasoning_chain.len(), 2);
    }

    #[test]
    fn test_list_hypotheses_filter() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        let h1 = Hypothesis {
            id: 0,
            subject: "A".into(),
            predicate: "is".into(),
            object: "B".into(),
            confidence: 0.5,
            evidence_for: vec![],
            evidence_against: vec![],
            reasoning_chain: vec![],
            status: HypothesisStatus::Proposed,
            discovered_at: now_str(),
            pattern_source: "test".into(),
        };
        let h2 = Hypothesis {
            status: HypothesisStatus::Confirmed,
            subject: "C".into(),
            object: "D".into(),
            ..h1.clone()
        };
        p.save_hypothesis(&h1).unwrap();
        p.save_hypothesis(&h2).unwrap();
        let all = p.list_hypotheses(None).unwrap();
        assert_eq!(all.len(), 2);
        let confirmed = p
            .list_hypotheses(Some(HypothesisStatus::Confirmed))
            .unwrap();
        assert_eq!(confirmed.len(), 1);
        assert_eq!(confirmed[0].subject, "C");
    }

    #[test]
    fn test_update_hypothesis_status() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        let h = Hypothesis {
            id: 0,
            subject: "A".into(),
            predicate: "is".into(),
            object: "B".into(),
            confidence: 0.5,
            evidence_for: vec![],
            evidence_against: vec![],
            reasoning_chain: vec![],
            status: HypothesisStatus::Proposed,
            discovered_at: now_str(),
            pattern_source: "test".into(),
        };
        let id = p.save_hypothesis(&h).unwrap();
        p.update_hypothesis_status(id, HypothesisStatus::Confirmed)
            .unwrap();
        let loaded = p.get_hypothesis(id).unwrap().unwrap();
        assert_eq!(loaded.status, HypothesisStatus::Confirmed);
    }

    #[test]
    fn test_save_pattern() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        let pat = Pattern {
            id: 0,
            pattern_type: PatternType::CoOccurrence,
            entities_involved: vec!["A".into(), "B".into()],
            frequency: 5,
            last_seen: now_str(),
            description: "test pattern".into(),
        };
        let id = p.save_pattern(&pat).unwrap();
        assert!(id > 0);
    }

    #[test]
    fn test_save_discovery() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        let h = Hypothesis {
            id: 0,
            subject: "A".into(),
            predicate: "is".into(),
            object: "B".into(),
            confidence: 0.9,
            evidence_for: vec![],
            evidence_against: vec![],
            reasoning_chain: vec![],
            status: HypothesisStatus::Confirmed,
            discovered_at: now_str(),
            pattern_source: "test".into(),
        };
        let hid = p.save_hypothesis(&h).unwrap();
        let did = p
            .save_discovery(hid, &["source1".into(), "source2".into()])
            .unwrap();
        assert!(did > 0);
    }

    #[test]
    fn test_record_outcome_and_weight() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        p.record_outcome("structural_hole", true).unwrap();
        p.record_outcome("structural_hole", true).unwrap();
        p.record_outcome("structural_hole", false).unwrap();
        let w = p.get_pattern_weight("structural_hole").unwrap();
        // EMA-blended weight: historical ratio 2/3, last outcome=false, recency_factor capped at 0.15
        // weight ≈ 0.667 * 0.85 + 0.0 * 0.15 ≈ 0.567
        assert!(w > 0.4 && w < 0.75, "weight: {}", w);
    }

    #[test]
    fn test_get_pattern_weights() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        p.record_outcome("type_gap", true).unwrap();
        p.record_outcome("co_occurrence", false).unwrap();
        let weights = p.get_pattern_weights().unwrap();
        assert_eq!(weights.len(), 2);
    }

    #[test]
    fn test_discovery_scores_empty() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        let scores = p.discovery_scores().unwrap();
        assert!(scores.is_empty());
    }

    #[test]
    fn test_discovery_scores() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        let h = Hypothesis {
            id: 0,
            subject: "A".into(),
            predicate: "is".into(),
            object: "B".into(),
            confidence: 0.9,
            evidence_for: vec![],
            evidence_against: vec![],
            reasoning_chain: vec![],
            status: HypothesisStatus::Confirmed,
            discovered_at: now_str(),
            pattern_source: "test".into(),
        };
        p.save_hypothesis(&h).unwrap();
        let scores = p.discovery_scores().unwrap();
        assert_eq!(*scores.get("A").unwrap(), 1);
        assert_eq!(*scores.get("B").unwrap(), 1);
    }

    #[test]
    fn test_explain_hypothesis() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        let h = Hypothesis {
            id: 0,
            subject: "X".into(),
            predicate: "related_to".into(),
            object: "Y".into(),
            confidence: 0.6,
            evidence_for: vec!["ev1".into()],
            evidence_against: vec![],
            reasoning_chain: vec!["step1".into(), "step2".into()],
            status: HypothesisStatus::Testing,
            discovered_at: now_str(),
            pattern_source: "structural_hole".into(),
        };
        let id = p.save_hypothesis(&h).unwrap();
        let explanation = p.explain(id).unwrap().unwrap();
        assert!(explanation.contains("X"));
        assert!(explanation.contains("step1"));
        assert!(explanation.contains("Evidence For"));
    }

    #[test]
    fn test_explain_nonexistent() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        assert!(p.explain(999).unwrap().is_none());
    }

    #[test]
    fn test_full_discover_pipeline() {
        let brain = setup_graph();
        let p = Prometheus::new(&brain).unwrap();
        let report = p.discover().unwrap();
        assert!(!report.summary.is_empty());
        // Should find at least the B-C structural hole hypothesis
        assert!(
            report.hypotheses_generated.len() > 0 || report.patterns_found.len() > 0,
            "report: {:?}",
            report.summary
        );
    }

    #[test]
    fn test_report_json() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        let report = DiscoveryReport {
            patterns_found: vec![],
            hypotheses_generated: vec![],
            gaps_detected: 0,
            summary: "test".into(),
        };
        let json = p.report_json(&report);
        assert!(json.contains("test"));
    }

    #[test]
    fn test_report_markdown() {
        let brain = test_brain();
        let p = Prometheus::new(&brain).unwrap();
        let report = DiscoveryReport {
            patterns_found: vec![Pattern {
                id: 1,
                pattern_type: PatternType::CoOccurrence,
                entities_involved: vec!["A".into()],
                frequency: 3,
                last_seen: now_str(),
                description: "test pattern".into(),
            }],
            hypotheses_generated: vec![],
            gaps_detected: 0,
            summary: "test summary".into(),
        };
        let md = p.report_markdown(&report);
        assert!(md.contains("PROMETHEUS"));
        assert!(md.contains("test pattern"));
    }

    #[test]
    fn test_validate_hypothesis_with_evidence() {
        let brain = test_brain();
        let a = brain.upsert_entity("Rust", "language").unwrap();
        let b = brain.upsert_entity("Mozilla", "org").unwrap();
        brain.upsert_relation(a, "created_by", b, "test").unwrap();
        let p = Prometheus::new(&brain).unwrap();
        let mut h = Hypothesis {
            id: 0,
            subject: "Rust".into(),
            predicate: "created_by".into(),
            object: "Mozilla".into(),
            confidence: 0.5,
            evidence_for: vec![],
            evidence_against: vec![],
            reasoning_chain: vec![],
            status: HypothesisStatus::Proposed,
            discovered_at: now_str(),
            pattern_source: "test".into(),
        };
        p.validate_hypothesis(&mut h).unwrap();
        assert!(h.confidence > 0.5);
        assert!(!h.evidence_for.is_empty());
    }

    #[test]
    fn test_temporal_patterns() {
        let brain = test_brain();
        let a = brain.upsert_entity("E1", "thing").unwrap();
        let b = brain.upsert_entity("E2", "thing").unwrap();
        let c = brain.upsert_entity("E3", "thing").unwrap();
        let d = brain.upsert_entity("E4", "thing").unwrap();
        // Same subject, different predicates learned sequentially
        brain.upsert_relation(a, "creates", b, "test").unwrap();
        brain.upsert_relation(a, "publishes", c, "test").unwrap();
        brain.upsert_relation(d, "creates", b, "test").unwrap();
        brain.upsert_relation(d, "publishes", c, "test").unwrap();
        let p = Prometheus::new(&brain).unwrap();
        let temporal = p.find_temporal_patterns(2).unwrap();
        assert!(
            temporal
                .iter()
                .any(|p| p.pattern_type == PatternType::TemporalSequence),
            "temporal: {:?}",
            temporal
        );
    }

    #[test]
    fn test_hypothesis_status_roundtrip() {
        assert_eq!(
            HypothesisStatus::from_str(HypothesisStatus::Proposed.as_str()),
            HypothesisStatus::Proposed
        );
        assert_eq!(
            HypothesisStatus::from_str(HypothesisStatus::Testing.as_str()),
            HypothesisStatus::Testing
        );
        assert_eq!(
            HypothesisStatus::from_str(HypothesisStatus::Confirmed.as_str()),
            HypothesisStatus::Confirmed
        );
        assert_eq!(
            HypothesisStatus::from_str(HypothesisStatus::Rejected.as_str()),
            HypothesisStatus::Rejected
        );
    }

    #[test]
    fn test_pattern_type_roundtrip() {
        let types = [
            PatternType::CoOccurrence,
            PatternType::StructuralHole,
            PatternType::TypeGap,
            PatternType::Analogy,
            PatternType::TemporalSequence,
            PatternType::FrequentSubgraph,
        ];
        for t in &types {
            assert_eq!(PatternType::from_str(t.as_str()), *t);
        }
    }

    #[test]
    fn test_predicate_chains() {
        let brain = test_brain();
        let a = brain.upsert_entity("Einstein", "person").unwrap();
        let b = brain.upsert_entity("Germany", "place").unwrap();
        let c = brain.upsert_entity("Europe", "place").unwrap();
        let d = brain.upsert_entity("Curie", "person").unwrap();
        brain.upsert_relation(a, "born_in", b, "test").unwrap();
        brain.upsert_relation(b, "located_in", c, "test").unwrap();
        brain.upsert_relation(d, "born_in", b, "test").unwrap();
        let p = Prometheus::new(&brain).unwrap();
        let chains = p.find_predicate_chains(2).unwrap();
        assert!(
            chains
                .iter()
                .any(|p| p.description.contains("born_in") && p.description.contains("located_in")),
            "chains: {:?}",
            chains
        );
        let hyps = p.generate_hypotheses_from_chains().unwrap();
        assert!(
            hyps.iter().any(|h| h.pattern_source == "predicate_chain"),
            "hyps: {:?}",
            hyps
        );
    }

    #[test]
    fn test_predicates_similar() {
        assert!(predicates_similar("created", "built"));
        assert!(predicates_similar("is", "is_a"));
        assert!(!predicates_similar("created", "eats"));
    }

    #[test]
    fn test_is_contradicting_predicate() {
        assert!(is_contradicting_predicate("is", "is_not"));
        assert!(is_contradicting_predicate("has", "lacks"));
        assert!(!is_contradicting_predicate("is", "has"));
    }

    #[test]
    fn test_detect_correct_type_geopolitical() {
        let cases = vec![
            ("spanish netherlands", "person", Some("place")),
            ("austrian netherlands", "person", Some("place")),
            ("ottoman empire", "person", Some("place")),
            ("roman republic", "concept", Some("place")),
            ("ming dynasty", "person", Some("place")),
            ("soviet union", "person", Some("place")),
            ("trade union", "person", Some("organization")),
            ("french indochina", "person", Some("place")),
            ("british guiana", "person", Some("place")),
            ("dutch east indies", "person", Some("place")),
            ("emi koussi", "person", Some("place")),
            ("galápagos", "person", Some("place")),
        ];
        for (name, current_type, expected) in cases {
            let words: Vec<&str> = name.split_whitespace().collect();
            let result = detect_correct_type(name, &words, current_type);
            assert_eq!(
                result, expected,
                "detect_correct_type({:?}, {:?}) = {:?}, expected {:?}",
                name, current_type, result, expected
            );
        }
    }

    #[test]
    fn test_infer_predicate_basics() {
        assert_eq!(infer_predicate("person", "person", None), "contemporary_of");
        assert_eq!(
            infer_predicate("person", "organization", None),
            "affiliated_with"
        );
        assert_eq!(infer_predicate("person", "place", None), "active_in");
        assert_eq!(
            infer_predicate("concept", "concept", None),
            "related_concept"
        );

        // With shared neighbor context
        let mut snt = HashMap::new();
        snt.insert("organization".to_string(), 1);
        assert_eq!(
            infer_predicate("person", "person", Some(&snt)),
            "colleagues_at"
        );
    }

    #[test]
    fn test_type_incompatibility() {
        // contemporary_of requires same-group types
        assert!(is_type_incompatible("person", "place", "contemporary_of"));
        assert!(is_type_incompatible("place", "person", "contemporary_of"));
        assert!(is_type_incompatible(
            "technology",
            "place",
            "contemporary_of"
        ));
        assert!(!is_type_incompatible("person", "person", "contemporary_of"));
        assert!(!is_type_incompatible(
            "place",
            "organization",
            "contemporary_of"
        ));
        assert!(!is_type_incompatible(
            "concept",
            "technology",
            "contemporary_of"
        ));

        // located_near with concept is bad
        assert!(is_type_incompatible("concept", "person", "located_near"));
        assert!(!is_type_incompatible("person", "place", "located_near"));

        // pioneered needs person/org subject
        assert!(is_type_incompatible("place", "concept", "pioneered"));
        assert!(!is_type_incompatible("person", "concept", "pioneered"));

        // unrelated predicates should be fine
        assert!(!is_type_incompatible("person", "place", "active_in"));
        assert!(!is_type_incompatible("person", "concept", "works_on"));
    }
}
